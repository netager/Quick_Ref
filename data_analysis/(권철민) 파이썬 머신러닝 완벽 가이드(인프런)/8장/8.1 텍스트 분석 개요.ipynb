{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분석(Text Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 이해 - NLP와 텍스트 분석\n",
    "\n",
    "- NLP(Natural Language Processing)\n",
    "    - 인간의 언어를 이해하고 해석하는데 더 중점을 두고 기술이 발전해 옴\n",
    "    - NLP 기술의 발전으로 텍스트 분석도 더욱 정교하게 발전\n",
    "\n",
    "- 텍스트 분석\n",
    "    - 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활요해 모델을 수립하고 정보를 추출해 비즈니즈 인텔리전스(Business Intelligence)나 예측 분석 등의 분석 장법을 주로 수행함\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 주요 영역\n",
    "\n",
    "- 텍스트 분류(Text Classification)\n",
    "    - 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법을 통칭함.\n",
    "    - 예를 들어 특정 신문 기사 내용이 연애/정치/사회/문화 중 어떤 카테고리에 속하는지 자동으로 분류하거나 스팸 메일 검출 같은 프로그램이 이에 속함\n",
    "    - 지도학습을 적용함.\n",
    "\n",
    "- 감성 분석(Sentiment Analysis)\n",
    "    - 텍스트에서 나타나는 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법을 총칭함\n",
    "    - 소셜 미디어 감정 분석, 영화나 제품에 대한 긍정 또는 리뷰, 여론조사 의견 분석 등의 다양한 영역에서 활용됨\n",
    "    - 지도학습 방법뿐만 아니라 비지도학습을 이용해 적용할 수 있음\n",
    "\n",
    "- 텍스트 요약(Summarization)\n",
    "    - 텍스트 내에서 중요한 주제나 중심 사상을추출하는 기법을 말함. \n",
    "    - 대표적으로 토픽 모델링(Topic Modeling)이 있음\n",
    "\n",
    "- 텍스트 군집화와 유사도 측정\n",
    "    - 비슷한 유형의 문서에 대해 군집화를 수행하는 기법을 말함. \n",
    "    - 텍스트 분류를 비지도학습으로 수행하는 방법의 일환으로 사용될 수 있음\n",
    "    - 유사도 측정 역시 문서들간의 유사도를 측정해 비슷한 문서끼리 모을 수 있는 방법임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 분석 머신러닝 수행 프로세스\n",
    "\n",
    "- <img src=\"../images/text_mining.png \" width=\"1200px\" height=\"800px\" title=\"텍스트 분석 머신러닝 수행 프로세스\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬 기반의 NLP, 텍스트 분석 패키지\n",
    "\n",
    "- NLTK(National Language Toolkit for Python)\n",
    "    - 파이썬의 가장 대표적인 NLP 패키지. \n",
    "    - 방대한 데이터 세트와 서브 모듈을 가지고 있으며 NLP의 거의 모든 영역을 커버하고 있음.\n",
    "    - 많은 NLP 패키지가 NLTK의 영향을 받아 작성되고 있음\n",
    "    - 수행 속도 측면에서 아쉬운 부분이 있어서 실제 대량의 데이터 기반에서는 제대로 활용되지 못하고 있음\n",
    "\n",
    "- Gensim\n",
    "    - 토픽 모델링 분야에서 가장 두각을 나타내는 패키지\n",
    "    - 오래전부터 토픽 모델링을 쉽게 구현할 수 있는 기능을 제공해 왔으며, Word2Vec 구현 등의 다양한 신기능도 제공함\n",
    "    - SpaCy와 함께 가장 많이 사용되는 NLP 패키지 임\n",
    "\n",
    "- SpaCy\n",
    "    - 뛰어난 수행 성능으로 최근 가장 주목을 받는 NLP 패키지임.\n",
    "    - 많은 NLP 애플리케이션에서 SpaCy를 사용하는 사례가 늘고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 - 텍스트 정규화\n",
    "\n",
    "- 클렌징(Cleansing)\n",
    "    - 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업\n",
    "    - 예를 들어 HTML, XML 태그나 특정 기호 등을 사전에 제거함\n",
    "\n",
    "- 토큰화(Tokenization)\n",
    "    - 문장 토큰화, 단어 토큰화, n-gram\n",
    "\n",
    "- 필터링/스톱워드 제거/ 철자 수정\n",
    "    - 불필요한 단어나 분석에 큰 의미가 없는 단어(a, the, is, will 등) 그리고 잘못된 철자 수정\n",
    "\n",
    "- Stemming/Lemmatization\n",
    "    - 어근(단어 원형) 추출, Lemmatization이 Stemming보다 정교하고 의미론적 기반에서 단어 원형을 찾아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram\n",
    "- 문장을 개별 단어 별로 하나씩 토큰화 할 경우 문맥적인 의미는 무시될 수 밖에 없음. 이러한 문제를 조금이라도 해결해 보고자 도입된 것이 n-gram임\n",
    "\n",
    "- n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것. n개 단어 크기 윈도우를 만들어 문장의 처음부터 오른쪽으로 움직이면서 토큰화를 수행\n",
    "\n",
    "- 예를 들어 \"Agent Smith Knocks the door\"를 2-gram(bigram)으로 만들면 (Agent, Smith), (Smith, knocks), (knocks, the), (the, door)와 같이 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토근화 함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트의 피처 벡터화 유형\n",
    "- BOW(Bag of Words)\n",
    "    - Document Term Matrix : 개별 문서(또는 문장)을 단어들의 횟수나 정규화 변환된 횟수로 표현\n",
    "  \n",
    "- Word Embedding(Word2Vec)\n",
    "    - 개별 단어를 문맥을 가지는 N차원 공간에 벡터로 표현\n",
    "    - <img src=\"../images/word2vec1.png \" width=\"1000px\" height=\"400px\" title=\"Word2Vector\"></img>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW\n",
    "\n",
    "- Bag of Words 모델은 문서가 가지는 모든 단어(Words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델임\n",
    "\n",
    "- 문서 내 모든 단어를 한꺼번에 봉투(Bag) 안에 넣은 뒤에 흔들어서 섞는다는 의미로 Bag of Words(BOW) 모델이라고 함\n",
    "    - <img src=\"../images/bow.png \" width=\"800px\" height=\"400px\" title=\"BOW\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words 구조\n",
    "\n",
    "- 문장1 : 'My wife likes to watch baseball games and my daughter likes to watch baseball games too'\n",
    "- 문장2 : 'My wife likes to play baseball'\n",
    "    1. 문장1 과 문자2 에 있는 모든 단어에서 중복을 제거하고 각 단어(feature 또는 term)를 컬럼 형태로 나열함. 그리고 나서 각 단어에 고유의 인덱스를 다음과 같이 부여함.\n",
    "        - 'and':0, 'baseball':1, 'daughter':2, 'games':3, 'likes':4, 'my':5, 'play':6, 'to':7, 'too':8, 'watch':9, 'wife':10\n",
    "\n",
    "    2. 문장에서 해당 단어가 나타나는 횟수(Occurrence)를 각 단어(단어 인덱스)에 기재함. 예를 들어 baseball은 문장 1, 2에서 총 2번 나타나며, daughter는 문장 1에서만 1번 나타남\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW 장단점\n",
    "\n",
    "- 장점\n",
    "    - 쉽고 빠른 구축\n",
    "    - 예상보다 문서의 특징을 잘 나타내어 전통적으로 여러분야에서 활용도가 높음\n",
    "\n",
    "- 단점\n",
    "    - 문맥 의미(Semantic Context) 반영 문제\n",
    "    - 희소 행렬 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW 피처 벡터화 유형\n",
    "\n",
    "- 단순 카운트 기반의 벡터화\n",
    "    - 단어 피처에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수, 즉 Count를 부여하는 경우를 카운트 벡터화라고 함.\n",
    "    - 카운트 벡터화에서는 카운트 값이 높을수록 중요한 단어로 인식됨\n",
    "\n",
    "- TF-IDF 벡터화\n",
    "    - 카운트만 부여할 경우 그 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수 밖에 없는 단어까지 높은 값을 부여하게 됨.\n",
    "    - 이러한 문제를 보완하기 위해 TF-IDF(Term Frequency Inverse Document Frequency) 벡터화를 사용함\n",
    "    - TF-IDF는 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 주는 방식으로 값을 부여함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency Inverse Document Frequency)\n",
    "\n",
    "- 특정 단어가 다른 문서에는 나타나지 않고 특정 문서에서만 자주 사용된다면 해당 단어는 해당 문서를 잘 특징짓는 중요 단어일 가능성이 높음\n",
    "- 특정 단어가 매우 많은 여러 문서에서 빈번히 나타난다면 해당 단어는 개별 문서를 특징짓는 정보로서의 의미를 상실\n",
    "\n",
    "- TF(Term Frequency)\n",
    "    - 문서에서 해당 단어가 얼마나 나왔는지를 나타내는 지표\n",
    "\n",
    "- DF(Document Frequency)\n",
    "    - 해당 단어가 몇 개의 문서에서 나타났는지를 나타내는 지표\n",
    "\n",
    "- IDF(Inverse Document Frequency)\n",
    "    - DF의 역수로서 전체 \"문서수 / DF\"\n",
    "\n",
    "- <img src=\"../images/tf_idf.png \" width=\"1200px\" height=\"600px\" title=\"TF-IDF\"></img>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 CountVectorizer 초기화 파라미터\n",
    "\n",
    "- max_df\n",
    "    - 전체 문서에 걸쳐서 너무높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
    "    - 너무 높은 빈도수를 가지는 단어는 스톱 워드와 비슷한 문법적인 특성으로 반복적인 단어일 가능성이 높기에 이를 제거하기 위해 사용됨\n",
    "    - max_df = 100과 같이 정수 값을 가지면 전체 문서에 걸쳐 100개 이하로 나타나는 단어만 피처로 추출한다\n",
    "    - max_df = 0.95 와 같이 부동소수점 값(0.0 ~ 1.0)을 가지면 전체 문서에 걸쳐 빈도수 0~95%까지의 단어만 피처로 추출하고 나머지 상위 5%는 피처로 추출하지 않음\n",
    "\n",
    "- min_df\n",
    "    - 전체 문서에 걸쳐서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터임\n",
    "    - 수백 ~ 수천 개의 전체 문서에서 특정 단어가 min_df에 설정된 값보다 적은 빈도수를 가진다면 이 단어는 크게 중요하지 않거나 가비지(garbage)성 단어일 확률이 높음\n",
    "    - min_df = 2와 같이 정수 값을 가지면 전체 문서에 걸쳐서 2번 이하로 나타나는 단어는 피처로 추출하지 않음\n",
    "    - min_df = 0.02와 같이 부동소수점(0.0 ~ 1.0)을 가지면 전체 문서에 걸쳐서 하위 2% 이하의 빈도수를 가지는 단어는 피처로 추출하지 않음\n",
    "  \n",
    "- max_features\n",
    "    - 피처로 추출하는 피처의 개수를 제한하며 정수로 값을 지정함\n",
    "    - 가령 max_features = 2000으로 지정할 경우 가장 높은 비도를 가지는 단어 순으로 정렬해 2000개 까지만 피처로 추출함\n",
    "\n",
    "- stop_words\n",
    "    - 'english'로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외함\n",
    "\n",
    "- ngram_range\n",
    "    - Bag of Words 모델의 단어 순서를 어느 정도 보강하기 위한 n-gram 범위를 설정함\n",
    "    - 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정함. 예를 들어 (1, 1)로 지정하면 토큰화된 단어를 1개씩 피처로 추출함\n",
    "    - (1, 2)로 지정하면 토큰화된 단어를 1개씩(minimum 1), 그리고 순서대로 2개씩(maximum 2) 묶어서 피처로 추출함\n",
    "\n",
    "- alalyzer\n",
    "    - 피처 추출을 수행한 단위를 지정함. 당연히 디폴트는 'word'임\n",
    "    - Word가 아니라 character의 특정 범위를 피처로 만드는 특정한 경우 등을 적용할 때 사용됨\n",
    "\n",
    "- token_patten\n",
    "    - 토큰화를 수행하는 정규 표현식 패턴을 지정함. 디폴트 값은 '\\b\\w\\w+\\b로, 공백 또는 개행 문자 등으로 구분된 단어 분리자(\\b) 사이의 2문자(문자 또는 숫자, 즉 영숫자) 이상의 단어(word)를 토큰으로 분리합.\n",
    "    - alalyzer='word'로 설정했을 때만 변경 가능하나 디폴트 값을 변경할 경우는 거의 발생하지 않음. 어근 추출시 외부 함수를 사용할 경우 해당 외부 함수를 token_pattern의 인자로 사용함\n",
    "\n",
    "- lower_case\n",
    "    - 모든 문자를 소문자로 변경할 것인지를 설정. 기본은 True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer를 이용한 피처 벡터화\n",
    "\n",
    "- <img src=\"../images/countvectorizer.png \" width=\"1000px\" height=\"600px\" title=\"CountVectorizer를 이용한 피처 벡터화\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희소 행렬\n",
    "\n",
    "- <img src=\"../images/희소행렬.png \" width=\"800px\" height=\"400px\" title=\"희소행렬\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 희소행렬의 저장 변환 형식\n",
    "\n",
    "- COO 형식 : Coordinate(좌표) 방식을 의미하며 0이 아닌 데이터만 별도의 배열(Array)에 저장하고 그 데이터를 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식\n",
    "- CSR 형식 : COO 형식이 위치 배열값을 중복적으로 가지는 문제를 해결한 방식. 일반적으로 CSR 형식이 COO 보다 많이 사용됨.\n",
    "\n",
    "- 파이썬에서는 희소 행렬을 COO, CSR 형식으로 변환하기 위해서 Scipy의 coo_matrix(), csr_matrix() 함수를 이용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COO 형식\n",
    "\n",
    "- <img src=\"../images/coo.png \" width=\"800px\" height=\"400px\" title=\"coo 형식\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSR 형식\n",
    "\n",
    "- <img src=\"../images/csr.png \" width=\"800px\" height=\"400px\" title=\"CSR 형식\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
