{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5장 회귀(Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 소개\n",
    "- 회귀는 현대 통계학을 이루는 큰 축\n",
    "- 회귀분석은 유전적 특성을 연구하던 영국의 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다는 것이 일반론\n",
    "    - 부모의 키가 크더라도 자식의 키가 대를 이어 무한정 커지지 않으며, 부모의 키가 작더라도 대를 이어 자식의 키가\n",
    "      무한정 작아지지 않는다.\n",
    "- 회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법임.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀(Regression) 개요\n",
    "- 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭함.\n",
    "\n",
    "- 아파트 가격은?\n",
    "    - 방 개수, 아파트 크기, 주변 학군, 근처 지하철 역 갯수\n",
    "    - $$Y = W_1X_1 + W_2X_2 + W_3X_3 + ... + W_nX_n$$\n",
    "        - Y 는 종속변수, 즉 아파트 가격\n",
    "        - X1, X2, X3, ... , Xn은 방 개수, 아파트 크기, 주변 학군 등의 독립변수\n",
    "        - W1, W2, W3, ... , Wn은 이 독립변수의 값에 영향을 미치는 회귀 계수(Regression coefficients)\n",
    "\n",
    "- 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 **최적의 회귀 계수**를 찾아내는 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀의 유형\n",
    "- 회귀는 회귀계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌수 있음\n",
    "- 회귀에서 가장 중요한 것은 바로 회귀 계수임. \n",
    "- 이 회귀 계수가 '선형이냐 아니냐'에 따라 선형 회귀와 비선형 회귀로 나눌 수 있음\n",
    "- 독립변수 개수가 한개인지 여러개인지에 따라 단일회귀, 다중회귀로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류(Classification)와 회귀(Regression)\n",
    "\n",
    "- Classification --> 결과값(Category 값, 이산값) : 0 또는 1\n",
    "- Regression     --> 결과값(숫자값, 연속값)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀의 종류\n",
    "\n",
    "- 일반 선형 회귀: 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며,\n",
    "                 규제(Regularization)를 적용하지 않은 모델\n",
    "\n",
    "- 릿지(Ridge): 릿지 회귀는 선형 회귀에 L2 규제를 추가한 회귀 모델\n",
    "\n",
    "- 라쏘(Lasso): 라쏘 회귀는 선형 회귀에 L1 규제를 적용한 방식\n",
    "\n",
    "- 엘라스틱넷(ElasticNet): L2, L1 규제를 함께 결합한 모델\n",
    "\n",
    "- 로지스틱 회귀(Logistic Regression): 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단순 선형 회귀(Simple Regression)를 통한 회귀의 이해\n",
    "- 주택 가격이 주택의 크기로만 결정되는 단순 선형 회귀로 가정하면 다음과 같이 주택 가격은 주택 크기에 대해\n",
    "  선형(직선 형태)의 관계로 표현할 수 있음.\n",
    "\n",
    "    - <img src=\"../images/regression.jpg\" width=\"500px\" height=\"300px\" title=\"단순회귀\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS 기반의 회귀 오류 측정\n",
    "\n",
    "- RSS(Residual Sum of Squares) : 오류 값의 제곱을 구해서 더하는 방식임. 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS 방식으로 오류 합을 구함.\n",
    "        즉, ERROR = RSS 임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS의 이해\n",
    "- RSS는 이제 변수가 W0, W1 인 식으로 표현할 수 있으며, 이 RSS를 최소로 하는 W0, W1. 즉 회귀계수를 학습을 통해서\n",
    "  찾는 것이 머신러닝 기반 회귀의 핵심 사항임.\n",
    "\n",
    "- RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 W 변수(회귀계수)가 중심 변수임을 인지하는 것이 매우\n",
    "  중요함.(학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주함)\n",
    "\n",
    "- 일반적으로 RSS는 학습 데이터 건수로 나누어서 다음과 같이 정규화된 식으로 표현됨\n",
    "\n",
    "    - $$RSS(W_0, W_1) = \\frac{1}{N}\\sum_{i=1}^N(y_i - (W_0 + W_1 * x_i))^2$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS - 회귀의 비용 함수\n",
    "- 회귀의 비용 함수(Cost function)\n",
    "    - $$RSS(W_0, W_1) = \\frac{1}{N}\\sum_{i=1}^N(y_i - (W_0 + W_1 * x_i))^2$$\n",
    "\n",
    "- 회귀에서 이 RSSS는 비용(Cost)이며 W 변수(회귀 계수)로 구성되는 RSS를 비용함수라 함.\n",
    "- 머신러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류값)을 지속해서 감소시키고\n",
    "  최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것임. 비용함수를 손실함수(loss function)라고도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비용 최소화 하기 - 경사 하강법(Gradient Descent) 소개\n",
    "- W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변수값을 도출할 수 있겠지만, W 파라미터가 많으면 고차원 방정식을\n",
    "  동원하더라도 해결이 어려움. \n",
    "\n",
    "- 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식.\n",
    "\n",
    "- 많은 W 파라미터가 있는 경우에 경사 하강법은 보다 간단하고 직관적인 비용함수 최소화 솔루션을 제공\n",
    "\n",
    "- 경사 하강법의 사전적 의미인 '점진적인 하강'이라는 뜻에서도 알 수 있듯이, '점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트 하면서\n",
    "\n",
    "  오류 값이 최소가 되는 W 파라미터를 구하는 방식임.\n",
    "\n",
    "- 경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측갑과 실제 갑의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나감.\n",
    "\n",
    "- 최초 오류 값이 100이었다면 두번째 오류 값은 100보다 작은 90, 세번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속\n",
    "\n",
    "  업데이트해 나감\n",
    "\n",
    "- 그리고, 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W 값을 최적 파라미터로 반환함.\n",
    "\n",
    "- 경사 하강법의 핵심은 \"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?\"임.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분을 통해 비용 함수의 최소값 찾기\n",
    "- 어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?\n",
    "\n",
    "- 비용 함수가 다음 그림과 같은 포물선 형태의 2차 함수라면 경사 하강법은 최초 W에서부터 미분을 적용한 뒤 이 미분값이 계속 감소하는 방향으로\n",
    "  \n",
    "  순차적으로 W를 업데이트 함.\n",
    "\n",
    "- 마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용함수가 최소인 지점으로 간주하고 그때의 W를 반환함. \n",
    "\n",
    "- <img src=\"../images/gradient.png\" width=\"500px\" height=\"300px\" title=\"경사 하강법\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSS의 편미분\n",
    "- R(w)는 변수가 W 파라미터로 이뤄진 함수이며, \n",
    "$$R(w) = \\sum_{i=1}^n(y_i - (w_0 + w_1 * x_i))^2$$\n",
    "\n",
    "- R(w)를 미분해 미분 함수의 최솟값을 구해야 하는데, R(w)는 두 개의 w 파라미터인 w0와 w1을 각각 가지고 있기 때문에 일반적인\n",
    "  미분을 적용할 수가 없고, w0, w1 각 변수에 편미분을 적용해야 함.\n",
    "\n",
    "- R(w)를 최소화하는 w0와 w1의 값은 각각 r(w)를 w0, w1으로 순차적으로 미분을 수행해 얻을 수 있음.\n",
    "\n",
    "$$R(w) = \\sum_{i=1}^n(y_i - (w_0 + w_1 * x_i))^2$$\n",
    "\n",
    "$$\\frac{\\partial{R(w)}}{\\partial{w_1}} = \\frac{2}{N}\\sum_{i=1}^N -x_i * (y_i - (w_0 + w_1x_i) = -\\frac{2}{N}\\sum_{i=1}^N x_i * (실제값_i - 예측값_i)$$\n",
    "\n",
    "$$\\frac{\\partial{R(w)}}{\\partial{w_0}} = \\frac{2}{N}\\sum_{i=1}^N -(y_i - (w_0 + w_1x_i) = -\\frac{2}{N}\\sum_{i=1}^N (실제값_i - 예측값_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분의 연쇄법칙(Chain Rule) - 합성 함수의 미분\n",
    "$$z = f(g(x))$$\n",
    "와 같이 합성 함수로 표현 가능할 때 합성 함수의 미분은\n",
    "\n",
    "$$\\frac{dz}{dx} = f'(g(x)) * g'(x)$$\n",
    "와 같이 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음\n",
    "\n",
    "\n",
    "$$z = (5x + 2)^2 = 25x^2 + 20x + 4$$\n",
    "\n",
    "$$\\frac{dz}{dx} = 50x + 20$$\n",
    "\n",
    "$$ y = 5x + 2 로 \\;표현하면 \\; z = y^2$$\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} * \\frac{dy}{dx}$$\n",
    "\n",
    "$$\\frac{dy}{dx} = 5 \\;\\;\\;\\;\\;     \\frac{dz}{dy} = 2y$$\n",
    "\n",
    "$$\\frac{dz}{dx} = 5 * 2(5x + 2) = 50x + 20$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분의 연쇄 법칙으로 편미분 수행\n",
    "\n",
    "$$R(w) = \\frac{1}{N}\\sum_{i=1}^n(y_i - (w_0 + w_1 * x_i))^2$$\n",
    "$$z = y_i - (w_0 + w_1 * x_i) 라고 \\;하면 \\; R(w) = \\frac{1}{N}\\sum_{i=1}^n z^2$$\n",
    "\n",
    "\n",
    "$$R(w)를 \\; w_1으로 \\; 편미분 \\; \\to \\frac{\\partial R(w)}{\\partial w_1} = \\frac{\\partial R(w)}{\\partial z} * \\frac{\\partial z}{\\partial w_1} = \\frac{2}{N}\\sum_{i=1}^n z * (-x_i) = -\\frac{2}{N}\\sum_{i=1}^n x_i * (y_i - (w_0 + w_1 * x_i))$$\n",
    "$$ = -\\frac{2}{N}\\sum_{i=1}^n x_i * (실제값_i - 예측값_i)$$\n",
    "\n",
    "$$R(w)를 \\; w_0으로 \\; 편미분 \\; \\to \\frac{\\partial R(w)}{\\partial w_0} = \\frac{\\partial R(w)}{\\partial z} * \\frac{\\partial z}{\\partial w_0} = \\frac{2}{N}\\sum_{i=1}^n z * (-1) = -\\frac{2}{N}\\sum_{i=1}^n (y_i - (w_0 + w_1 * x_i))$$\n",
    "$$ = -\\frac{2}{N}\\sum_{i=1}^n (실제값_i - 예측값_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
