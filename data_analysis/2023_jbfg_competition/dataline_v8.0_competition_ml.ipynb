{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JBFG Data Analysis Competition\n",
    "- Null 처리방식 : 2번"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Environment and Library Version\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!pip install watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'DataLine' -nmv --packages numpy,pandas,sklearn,imblearn,tensorflow,plotly,matplotlib,seaborn,missingno,lightgbm\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색적 데이터 분석\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library and Data Loading, Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl  \n",
    "import missingno as msno\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "# import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rc('font', family='Malgun Gothic')  # 한글 폰트 설정\n",
    "                                        # 윈도우 폰트 위치 - C:\\Windows\\Fonts\n",
    "plt.figure(figsize=(10,6))              # 그래프 사이즈 설정\n",
    "sns.set(font='Malgun Gothic', rc={'axes.unicode_minus':False}, style='darkgrid') # 마이너스 처리\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속형 데이터 그래프 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_continuous_graphs(df, column, column_desc):\n",
    "\n",
    "       counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "       exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "       churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "       churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "\n",
    "       fig = make_subplots(rows=3, \n",
    "                     cols=2, \n",
    "                     subplot_titles=('전체 건수 분포', '유지/이탈별 사분위', '유지/이탈별 분포'), \n",
    "                     # shared_xaxes=True,\n",
    "                     horizontal_spacing=0.1,\n",
    "                     vertical_spacing=0.1,\n",
    "                     specs=[[{\"secondary_y\": True}, {}],\n",
    "                            [{}, {\"secondary_y\": True}],\n",
    "                            [{\"secondary_y\": True},{}],\n",
    "                            ]\n",
    "                     )\n",
    "\n",
    "       # 전체\n",
    "       # ----\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column],  marker_color=\"red\", name='이탈'), row=1, col=1, secondary_y=False)\n",
    "       # fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], texttemplate=\"%{x}\", marker_color=\"red\"), row=1, col=1, secondary_y=False)\n",
    "       \n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\", name='유지'), row=1, col=1, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=1, col=1, secondary_y=True)\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "\n",
    "\n",
    "       \n",
    "       # Box Graph\n",
    "       # ---------\n",
    "       # fig.add_trace(go.Box(x=exist_counts, \n",
    "       #               name='유지'), row=2, col=1)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']==0][column].sort_values(), name='유지', marker_color=\"blue\"), row=1, col=2)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']!=0][column].sort_values(), name='이탈', marker_color=\"red\"), row=1, col=2)\n",
    "\n",
    "\n",
    "       # Histogram Graph\n",
    "       # ---------------\n",
    "\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\"), row=2, col=1)\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], marker_color=\"red\"), row=2, col=1)\n",
    "\n",
    "       # Scatter Graph\n",
    "       # -------------\n",
    "       fig.add_trace(go.Scatter(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), mode='lines+markers', marker_color=\"red\", name='이탈'), row=2, col=2, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), mode='lines+markers', marker_color='blue', name='유지'), row=2, col=2, secondary_y=False)\n",
    "\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=2, col=2, secondary_y=True)\n",
    "\n",
    "\n",
    "       # 이탈률\n",
    "       # ------\n",
    "       # churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "       fig.add_trace(go.Histogram(x=churn_rates.sort_index()), row=3, col=1)\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=3, col=1)\n",
    "\n",
    "\n",
    "       fig.update_layout(width=1200, \n",
    "                     height=1200, \n",
    "                     showlegend=False,\n",
    "                     barmode='stack',\n",
    "                     hovermode=\"x\",\n",
    "                     template='plotly_dark',\n",
    "                     )\n",
    "       fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_func(df, column, start_value, units):\n",
    "    bins = np.arange(start_value, df[column].max()+units, units)\n",
    "    bins_label = [str(round(x,2)) for x in bins]\n",
    "    df[f\"{column}_category\"] = pd.cut(df[column], bins, right=True, include_lowest=True, labels=bins_label[:-1])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 범주형 데이터 그래프 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_graphs(df, column, column_desc):\n",
    "    \n",
    "    counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "    exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "    churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "    churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "    \n",
    "    \n",
    "    fig = make_subplots(rows=3, \n",
    "                    cols=2, \n",
    "                    subplot_titles=('【 전체 현황 】', '【 이탈율 】', '【 사분위 】', f'【 {column_desc} 중 전체 현황 】', f'【 {column_desc} 중 유지 현황 】', f'【 {column_desc} 중 이탈 현황 】'), \n",
    "                    # shared_xaxes=True,\n",
    "                    horizontal_spacing=0.1,\n",
    "                    vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}, {}],\n",
    "                           [{}, {'type':'domain'}],\n",
    "                           [{'type':'domain'}, {'type':'domain'}]]\n",
    "                   )\n",
    "\n",
    "\n",
    "    # 전체 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Bar(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), marker_color=\"red\", offsetgroup=0, name='이탈', \n",
    "                         text=churn_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto'), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Bar(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), marker_color=\"blue\", offsetgroup=0, name='유지', \n",
    "                         texttemplate='%{value:,}', \n",
    "                        #  text=exist_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto', base=churn_counts.sort_index()), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', \n",
    "                             line_shape='linear'), row=1, col=1, secondary_y=True)\n",
    "    \n",
    "    fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "    #fig.update_traces(texttemplate='%{value:,}', hovertemplate = '%{label}, %{value}', row=1, col=1)\n",
    "    # fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "    \n",
    "\n",
    "    # 이탈율\n",
    "    # ------\n",
    "    fig.add_trace(go.Bar(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"red\", name='이탈율'),\n",
    "                  row=1, col=2)\n",
    "\n",
    "    \n",
    "    # 사분위\n",
    "    # ------\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']!=0][column].sort_values(), marker_color=\"red\", name='이탈'), row=2, col=1)\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']==0][column].sort_values(), marker_color=\"blue\", name='유지'), row=2, col=1)\n",
    "\n",
    "\n",
    "    # 유지/이탈 현황\n",
    "    # -------------\n",
    "    fig.add_trace(go.Pie(labels=counts.sort_index().index, values=counts.sort_index(), name=f'{column_desc} 분표 현황', texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=2, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=2, col=2)\n",
    "\n",
    "  \n",
    "    # 유지 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=exist_counts.sort_index().index, values=exist_counts.sort_index(), name=\"유지\", texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=1)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=1)\n",
    "\n",
    "\n",
    "    # 이탈 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=churn_counts.sort_index().index, values=churn_counts.sort_index(), name=\"이탈\", texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=2)\n",
    "\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.5, ax=0, ay=0,\n",
    "                    xref = \"paper\", yref = \"paper\", \n",
    "                    text= \"<b>전체</b>\", \n",
    "                    font_size=20,\n",
    "                  ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.21, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>유지</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>이탈</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    \n",
    "    fig.update_layout(width=1200, \n",
    "                  height=1200, \n",
    "                  showlegend=False,\n",
    "                  title_text=f'『 {column_desc} 』에 따른 분석 그래프',\n",
    "                # barmode='stack'\n",
    "                  hovermode=\"x\",\n",
    "                  template='plotly_dark',\n",
    "                 )\n",
    "    \n",
    "\n",
    "    fig.show()    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 데이터 및 Null 건수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def change_format(df, column, format):\n",
    "    if format == 'comma':\n",
    "        df[column] = df[column].apply(lambda x: f\"{x:,}\")\n",
    "    elif format == 'percent':\n",
    "        df[column] = df[column].apply(lambda x: f\"{x:.2%}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_column_na_count(df, column):\n",
    "    '''\n",
    "    '''\n",
    "    column_na_counts = df[column].size, df[column].count(), df[column].isnull().sum()\n",
    "    column_na_counts_df = pd.Series(column_na_counts).to_frame().T\n",
    "    column_na_counts_df.columns = ['tot_counts', 'data_counts', 'null_counts']\n",
    "    column_na_counts_df['data_percents'] = column_na_counts_df['data_counts'].values/column_na_counts_df['tot_counts'].values\n",
    "    column_na_counts_df['null_percents'] = column_na_counts_df['null_counts'].values/column_na_counts_df['tot_counts'].values\n",
    "\n",
    "\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'tot_counts', 'comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'data_counts','comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'null_counts','comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'data_percents', 'percent')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'null_percents', 'percent')\n",
    "\n",
    "    print(column_na_counts_df.to_string(index=False))\n",
    "    print('-'*70)\n",
    "\n",
    "\n",
    "def count_column_data_count(df, column):\n",
    "    ''' \n",
    "    '''\n",
    "    # column_data_countcounts = df.groupby(column)['is_churned'].value_counts().unstack()\n",
    "\n",
    "\n",
    "    column_counts = df.groupby(column)['is_churned'].value_counts().unstack()\n",
    "    column_counts = column_counts.rename(columns={0: 'exist_counts', 1: 'churned_counts'})\n",
    "    column_counts['total_counts'] =  column_counts['exist_counts'] + column_counts['churned_counts']\n",
    "    column_counts = column_counts.fillna(0)\n",
    "\n",
    "    column_percents = df.groupby(column)['is_churned'].value_counts(normalize=True).unstack()\n",
    "    column_percents = column_percents.rename(columns={0: 'exist_percents', 1: 'churned_percents'})\n",
    "    column_percents = column_percents.fillna(0)\n",
    "\n",
    "\n",
    "    column_count_percent = pd.concat([column_counts, column_percents], axis=1)\n",
    "    column_count_percent = column_count_percent.reset_index()\n",
    "    column_count_percent = column_count_percent.sort_values(by='churned_percents', ascending=False)\n",
    "\n",
    "    \n",
    "    column_count_percent = change_format(column_count_percent, 'exist_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'churned_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'total_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'exist_percents', 'percent')\n",
    "    column_count_percent = change_format(column_count_percent, 'churned_percents', 'percent')\n",
    "    \n",
    "\n",
    "    print(column_count_percent.to_string(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library and Data Loading, Function Definition for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "\n",
    "# import scikitplot as skplt\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 출력\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    f11 = f1_score(y_test, pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "          F1: {3:.4f}, F11: {4:.4f}, AUC:{5:.4f}'.format(accuracy, precision, recall, f1, f11, roc_auc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve_plot(y_test=None, pred_proba=None):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve_plot(y_test=None, pred_proba=None):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_plot(y_test , pred_proba):\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음. \n",
    "    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba)\n",
    "\n",
    "    # ROC Curve를 plot 곡선으로 그림. \n",
    "    plt.plot(fprs , tprs, label='ROC')\n",
    "    # 가운데 대각선 직선을 그림. \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   \n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel('FPR( 1 - Sensitivity )'); plt.ylabel('TPR( Recall )')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리(Pre-Processing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요한 컬럼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_churner_df = ml_churner_df.drop('cstno', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 함수 정의"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측(Null) 데이터 확인 및 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tot_column_counts(df):\n",
    "    ''' \n",
    "    '''\n",
    "    data_counts = df.count()\n",
    "    null_counts = df.isnull().sum()\n",
    "    tot_counts_df = pd.concat([data_counts, null_counts], axis=1)\n",
    "    tot_counts_df = tot_counts_df.rename(columns={0: 'data_counts', 1: 'null_counts'})\n",
    "    tot_counts_df.insert(0,'tot_counts', tot_counts_df['data_counts'] + tot_counts_df['null_counts'])\n",
    "    tot_counts_df['data_percents'] = tot_counts_df['data_counts'].values / tot_counts_df['tot_counts'].values\n",
    "    tot_counts_df['null_percents'] = tot_counts_df['null_counts'].values / tot_counts_df['tot_counts'].values\n",
    "    tot_counts_df = tot_counts_df.sort_values(by='null_percents', ascending=False)\n",
    "\n",
    "    tot_counts_df = change_format(tot_counts_df, 'tot_counts', 'comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'data_counts','comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'null_counts','comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'data_percents', 'percent')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'null_percents', 'percent')\n",
    "\n",
    "    tot_counts_df = tot_counts_df.reset_index()\n",
    "\n",
    "    print(tot_counts_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tot_trans_cnt_for_12m      8,101       4,851       3,250        59.88%        40.12% : 중요도 : 1순위\n",
    "#         mean_util_pct      8,101       5,575       2,526        68.82%        31.18% : 중요도 : 6순위\n",
    "#   tot_amt_ratio_q4_q1      8,101       5,666       2,435        69.94%        30.06% : 중요도 : 8순위   Drop - 3순위\n",
    "# tot_trans_amt_for_12m      8,101       6,432       1,669        79.40%        20.60% : 중요도 : 3순위\n",
    "#   tot_cnt_ratio_q4_q1      8,101       6,472       1,629        79.89%        20.11% : 중요도 : 5순위\n",
    "#            imcome_cat      8,101       6,482       1,619        80.01%        19.99% : 중요도 : X,     Drop - 2순위\n",
    "#     tot_revol_balance      8,101       6,580       1,521        81.22%        18.78% : 중요도 : 2순위\n",
    "#                   sex      8,101       7,293         808        90.03%         9.97%  : 중요도 : X,    Drop - 1순위\n",
    "\n",
    "# 공선성\n",
    "# ------ \n",
    "# age와 mon_on_book, \n",
    "# credit_line과 mean_open_to_buy, \n",
    "# tot_trans_cnt_for_12m와 tot_trans_amt_for_12m \n",
    "\n",
    "# Feature 중요도\n",
    "# --------------\n",
    "# tot_trans_cnt_for_12m - tot_revol_balance - tot_trans_amt_for_12m - tot_product_count - tot_cnt_ratio_q4_q1 - \n",
    "# mean_util_pct - contact_cnt_for_12m\n",
    "# tot_amt_ratio_q4_q1 - months_inact_for_12m - credi_line - mean_open_to_buy - mon_on_book - age\n",
    "\n",
    "\n",
    "\n",
    "def drop_null_column(df, drop_list):\n",
    "    # Null 소유 필드 Drop\n",
    "    # ------------------\n",
    "    df = df.drop('sex', axis=1)\n",
    "    df = df.drop('imcome_cat', axis=1)\n",
    "    df = df.drop('tot_amt_ratio_q4_q1', axis=1)\n",
    "    # df = df.drop('mean_util_pct', axis=1)\n",
    "    # df = df.drop('tot_trans_cnt_for_12m', axis=1)\n",
    "    \n",
    "    # 다중공선성 처리\n",
    "    # ----------\n",
    "    df = df.drop('age', axis=1)\n",
    "    df = df.drop('mean_open_to_buy', axis=1)\n",
    "    # df = df.drop('tot_trans_amt_for_12m', axis=1)\n",
    "\n",
    "    df.dropna(axis=0, inplace=True)        \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 원-핫(One-Hot) 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_onehot(df):\n",
    "    # sex                    7293 non-null   object \n",
    "    # education              8101 non-null   object \n",
    "    # marital_stat           8101 non-null   object \n",
    "    # imcome_cat             6482 non-null   object \n",
    "    # card_type              8101 non-null   object \n",
    "\n",
    "    # # ml_churner_df = pd.concat([ml_churner_df, pd.get_dummies(ml_churner_df['sex'])], axis=1)\n",
    "    # df = pd.concat([df, pd.get_dummies(df['education']).drop(columns=['Unknown'])], axis=1)\n",
    "    # df = pd.concat([df, pd.get_dummies(df['marital_stat']).drop(columns=['Unknown'])], axis=1)\n",
    "    # # ml_churner_df = pd.concat([ml_churner_df, pd.get_dummies(ml_churner_df['imcome_cat']).drop(columns=['Unknown'])], axis=1)\n",
    "    # df = pd.concat([df, pd.get_dummies(df['card_type'])], axis=1)\n",
    "    # # ml_churner_df.drop(columns = ['sex', 'education', 'marital_stat', 'imcome_cat', 'card_type'], inplace=True)\n",
    "    # df.drop(columns = ['education', 'marital_stat', 'card_type'], inplace=True)\n",
    "    \n",
    "    catcols = df.select_dtypes(exclude = ['int64','float64']).columns\n",
    "    df = pd.get_dummies(df, columns = catcols)\n",
    "    \n",
    "    return df, catcols\n",
    "\n",
    "# ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "# ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "# ddf, catcols = encode_onehot(ml_churner_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습에 사용될 중요 Feature 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(df, model):\n",
    "\n",
    "    #Set the random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    #Define a list of available models for selection\n",
    "    available_models = {\n",
    "        'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "        'SVM': SVC(kernel='linear'),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'LASSO': Lasso(alpha=0.01),  # Agrega LASSO aquí\n",
    "        'RFE': RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=13)\n",
    "    }\n",
    "\n",
    "    # Choose the desired model for feature selection\n",
    "    chosen_model = model\n",
    "\n",
    "    # Create the selected model\n",
    "    clf = available_models[chosen_model]\n",
    "\n",
    "    #Train the model with the data\n",
    "    clf = clf.fit(df.values, y)\n",
    "\n",
    "    # Obtain feature importances from the model\n",
    "    feature_importances = clf.feature_importances_\n",
    "\n",
    "    # Create a SelectFromModel object with the trained classifier\n",
    "    model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "    #Transform the original features to obtain the selected ones\n",
    "\n",
    "    X_df = model.transform(df.values)\n",
    "\n",
    "    selected_feature_indices = model.get_support(indices=True)\n",
    "\n",
    "    #Get the indices of the selected features\n",
    "    selected_columns = df.columns[selected_feature_indices]\n",
    "    \n",
    "    return X_df, selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_smote(X_new, y):\n",
    "    #Model Training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "    sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train, y_train=sm.fit_resample(X_train,y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_normalization(X_train, X_test):\n",
    "    #Normalization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc=StandardScaler()\n",
    "    X_train=sc.fit_transform(X_train)\n",
    "    X_test=sc.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델별 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(proc_type, drop_no, model_comparison, X_train, y_train, X_test, y_test):\n",
    "    #Training with different models\n",
    "    #entrenamiento con distintos modelos\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    #Create a list of tuples with the model name and the classifier instance\n",
    "    # Crear una lista de tuplas con el nombre del modelo y la instancia del clasificador\n",
    "    models = [\n",
    "        # ('Logistic Regression', LogisticRegression()),\n",
    "        # ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "        # ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "        # ('Naive Bayes', GaussianNB()),\n",
    "        # ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "        ('LightGBM', LGBMClassifier(n_estimators=500, random_state=42, boosting_type='GOSS')),\n",
    "        ('Xg Boost', XGBClassifier())\n",
    "    ]\n",
    "\n",
    "\n",
    "    for model_name, classifier in models:\n",
    "        #Fit the model using the training set\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        #Make predictions on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        \n",
    "        #Calculate model metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        #f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        ## 확인 필요 ??? \n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "        cv_accuracy = accuracies.mean()\n",
    "        cv_std = accuracies.std()\n",
    "        accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "        accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1], )\n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        #Print model metrics\n",
    "        # print(\"-\" * 30)\n",
    "        # print(f\"Model: {model_name}\")\n",
    "        # print(\"-\" * 30)\n",
    "        # print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "        # print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "        # print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "        # print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "\n",
    "        #Add metrics to the models comparison dictionary\n",
    "        model_comparison[f'{model_name}_{proc_type}_{drop_no}'] = [drop_no, accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std, roc_auc]\n",
    "        # print(classification_report(y_pred, y_test, zero_division=1))\n",
    "        # print(\"-\" * 60)\n",
    "\n",
    "        \n",
    "        # get_clf_eval(y_test, y_pred, pred_proba)\n",
    "        \n",
    "        # precision_recall_curve_plot(y_test, pred_proba)\n",
    "        # roc_curve_plot(y_test , pred_proba)\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 및 예측 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_result(model_comparison):\n",
    "    import pandas as pd\n",
    "\n",
    "    # MODEL COMPARISSON\n",
    "\n",
    "    Model_com_df=pd.DataFrame(model_comparison).T\n",
    "    Model_com_df.columns=['Drop No', 'Model Accuracy','Model Accuracy-0','Model Accuracy-1','Model F1-Score','CV Accuracy','CV std', 'AUC']\n",
    "    Model_com_df=Model_com_df.sort_values(by='AUC',ascending=False)\n",
    "    # display(Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma'))\n",
    "\n",
    "\n",
    "    Model_com_df = pd.DataFrame(model_comparison).T\n",
    "    Model_com_df.columns = ['Drop No', 'Model Accuracy', 'Model Accuracy-No', 'Model Accuracy-Yes', 'Model F1-Score', 'CV Accuracy', 'CV std', 'AUC']\n",
    "    Model_com_df = Model_com_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "    def highlight_below_75(s):\n",
    "        if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "            return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "        else:\n",
    "            return ['color: black'] * len(s)\n",
    "\n",
    "    styled_df = Model_com_df.style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV Accuracy']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV Accuracy'])\n",
    "    display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인 처리 for 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_churner_df['credit_line']\n",
    "bb = ['credit_line']\n",
    "for col in bb:\n",
    "    cc = col\n",
    "\n",
    "aa = 'credit_line'\n",
    "ml_churner_df[cc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_null_column(df, drop_list):\n",
    "    \n",
    "    for col_name in drop_list:\n",
    "        # print(col_name, type(col_name))\n",
    "        df = df.drop(col_name, axis=1)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "result_list = []\n",
    "drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "for j in range(1, len(drop_target_columns)+1):\n",
    "    for i in combinations(drop_target_columns, j):\n",
    "        result_list.append(list(i))\n",
    "\n",
    "for no, drop_column in enumerate(result_list):\n",
    "    ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "    ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "    ml_churner_df = drop_null_column(ml_churner_df, drop_column)\n",
    "    print(f'구분 : {no}, 남은 갯수: {len(ml_churner_df)}, Drop Col:{drop_column}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_column(df, drop_list):\n",
    "    \n",
    "    for col_name in drop_list:\n",
    "        print(col_name, type(col_name))\n",
    "        \n",
    "        df = df.drop(col_name, axis=1)\n",
    "        \n",
    "    # df.dropna(axis=0, inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "result_list = []\n",
    "drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "for j in range(1, len(drop_target_columns)+1):\n",
    "    for i in combinations(drop_target_columns, j):\n",
    "        result_list.append(list(i))\n",
    "\n",
    "for drop_column in result_list:\n",
    "    ml_churner_df = drop_null_column(ml_churner_df, drop_column)\n",
    "    print(f'남은 갯수: {len(ml_churner_df)}, Drop Col:{drop_column}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_target_columns.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('sex', axis=1)\n",
    "# df = df.drop('imcome_cat', axis=1)\n",
    "# df = df.drop('tot_amt_ratio_q4_q1', axis=1)\n",
    "# # df = df.drop('mean_util_pct', axis=1)\n",
    "# # df = df.drop('tot_trans_cnt_for_12m', axis=1)\n",
    "\n",
    "# # 다중공선성 처리\n",
    "# # ----------\n",
    "# df = df.drop('age', axis=1)\n",
    "# df = df.drop('mean_open_to_buy', axis=1)\n",
    "# # df = df.drop('tot_trans_amt_for_12m', axis=1)\n",
    "drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "\n",
    "for i in drop_target_columns:\n",
    "    print(i)\n",
    "    \n",
    "# drop_target_columns    \n",
    "# ml_churner_df = drop_null_column(ml_churner_df, ['tot_trans_cnt_for_12m'])\n",
    "\n",
    "\n",
    "test_list = ['one', 'two', 'three'] \n",
    "\n",
    "for i in test_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_column(df, drop_list):\n",
    "    \n",
    "    for col_name in drop_list:\n",
    "        # print(col_name, type(col_name))\n",
    "        df = df.drop(col_name, axis=1)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# result_list = []\n",
    "# drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "# for j in range(1, len(drop_target_columns)+1):\n",
    "#     for i in combinations(drop_target_columns, j):\n",
    "#         result_list.append(list(i))\n",
    "\n",
    "# for no, drop_column in enumerate(result_list):\n",
    "#     ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "#     ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "#     ml_churner_df = drop_null_column(ml_churner_df, drop_column)\n",
    "#     print(f'구분 : {no}, 남은 갯수: {len(ml_churner_df)}, Drop Col:{drop_column}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "model_eval_comparison = {}                        \n",
    "    \n",
    "# -----------\n",
    "# 예측\n",
    "# -----------\n",
    "\n",
    "# 데이터 로드 및 고객번호 삭제\n",
    "ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "# Null 처리\n",
    "result_list = []\n",
    "drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "#drop_target_columns = ['sex','imcome_cat']\n",
    "for j in range(1, len(drop_target_columns)+1):\n",
    "    for i in combinations(drop_target_columns, j):\n",
    "        result_list.append(list(i))\n",
    "\n",
    "\n",
    "for drop_no, drop_column in enumerate(result_list):\n",
    "    ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "    ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "    ml_churner_df = drop_null_column(ml_churner_df, drop_column)\n",
    "    after_null_drop_cnt = len(ml_churner_df)\n",
    "       \n",
    "\n",
    "    ml_churner_df, catcols = encode_onehot(ml_churner_df)  \n",
    "\n",
    "\n",
    "    #We create our feature matrix and our target variable vector.\n",
    "    X=ml_churner_df.drop(['is_churned'],axis=1)\n",
    "    y=ml_churner_df['is_churned']\n",
    "\n",
    "    X_new, selected_columns = select_feature(X, 'ExtraTrees')\n",
    "    # display(selected_columns)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = proc_smote(X_new, y)\n",
    "    X_train_org = X_train.copy()\n",
    "\n",
    "    X_train, X_test = proc_normalization(X_train, X_test)    \n",
    "\n",
    "    print(f'구분: {drop_no}, X_train 건수: {len(X_new)}, X_train_SMOTE 건수: {len(X_train)}, After Drop 건수: {after_null_drop_cnt}, Drop Col:{drop_column}')\n",
    "    \n",
    "    # display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    proc_type='P'\n",
    "    fit_predict(proc_type, drop_no, model_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print_eval_result(model_comparison)\n",
    "\n",
    "\n",
    "\n",
    "    # -----------\n",
    "    # 평가\n",
    "    # -----------\n",
    "    test_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "    test_df = test_df.drop('cstno', axis=1)\n",
    "\n",
    "\n",
    "    test_df, catcols = encode_onehot(test_df)  \n",
    "    print(f'Test df: {type(test_df)}')\n",
    "    \n",
    "    #We create our feature matrix and our target variable vector.\n",
    "    X=test_df.drop(['is_churned'],axis=1)\n",
    "    y=test_df['is_churned']\n",
    "    y_test = y\n",
    "\n",
    "    # display(selected_columns)\n",
    "    X_new = X[selected_columns]\n",
    "\n",
    "\n",
    "    X_train_temp, X_test = proc_normalization(X_train_org, X_new.values)   \n",
    "\n",
    "\n",
    "    # display(X_test.shape)\n",
    "\n",
    "    # display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    proc_type='E'\n",
    "    fit_predict(proc_type, drop_no, model_eval_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print_eval_result(model_eval_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_null_column(df, drop_list):\n",
    "    \n",
    "    for col_name in drop_list:\n",
    "        # print(col_name, type(col_name))\n",
    "        df = df.drop(col_name, axis=1)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "result_list = []\n",
    "drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "for j in range(1, len(drop_target_columns)+1):\n",
    "    for i in combinations(drop_target_columns, j):\n",
    "        result_list.append(list(i))\n",
    "\n",
    "for no, drop_column in enumerate(result_list):\n",
    "    ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "    ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "    ml_churner_df = drop_null_column(ml_churner_df, drop_column)\n",
    "    print(f'구분 : {no}, 남은 갯수: {len(ml_churner_df)}, Drop Col:{drop_column}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# -----------\n",
    "# 예측\n",
    "# -----------\n",
    "\n",
    "# 데이터 로드 및 고객번호 삭제\n",
    "ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "ml_churner_df = ml_churner_df.drop('cstno', axis=1)\n",
    "\n",
    "# Null 처리\n",
    "\n",
    "ml_churner_df = drop_null_column(ml_churner_df, ['tot_trans_cnt_for_12m'])\n",
    "ml_churner_df.shape\n",
    "\n",
    "ml_churner_df = encode_onehot(ml_churner_df)  \n",
    "\n",
    "\n",
    "#We create our feature matrix and our target variable vector.\n",
    "X=ml_churner_df.drop(['is_churned'],axis=1)\n",
    "y=ml_churner_df['is_churned']\n",
    "\n",
    "X_new, selected_columns = select_feature(X, 'ExtraTrees')\n",
    "display(selected_columns)\n",
    "\n",
    "display(X_new.shape)\n",
    "\n",
    "X_train, y_train, X_test, y_test = proc_smote(X_new, y)\n",
    "X_train_org = X_train.copy()\n",
    "\n",
    "display(X_train.shape)\n",
    "\n",
    "X_train, X_test = proc_normalization(X_train, X_test)    \n",
    "\n",
    "\n",
    "model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "                        # Diccionario para almacenar las métricas de comparación de modelos\n",
    "model_eval_comparison = {}                        \n",
    "\n",
    "display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "fit_predict(model_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print_eval_result(model_comparison)\n",
    "\n",
    "\n",
    "\n",
    "# -----------\n",
    "# 평가\n",
    "# -----------\n",
    "test_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "test_df = test_df.drop('cstno', axis=1)\n",
    "\n",
    "\n",
    "test_df = encode_onehot(test_df)  \n",
    "\n",
    "\n",
    "#We create our feature matrix and our target variable vector.\n",
    "X=test_df.drop(['is_churned'],axis=1)\n",
    "y=test_df['is_churned']\n",
    "y_test = y\n",
    "\n",
    "\n",
    "display(selected_columns)\n",
    "X_new = X[selected_columns]\n",
    "\n",
    "\n",
    "X_train_temp, X_test = proc_normalization(X_train_org, X_new.values)   \n",
    "\n",
    "\n",
    "display(X_test.shape)\n",
    "\n",
    "display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "fit_predict(model_eval_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print_eval_result(model_eval_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 테스트 데이터로 평가\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가(학습 및 예측 결과)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인 처리 for 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eval_result(model_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "test_df = test_df.drop('cstno', axis=1)\n",
    "\n",
    "\n",
    "test_df = encode_onehot(test_df)  \n",
    "\n",
    "\n",
    "#We create our feature matrix and our target variable vector.\n",
    "X=test_df.drop(['is_churned'],axis=1)\n",
    "y=test_df['is_churned']\n",
    "y_test = y\n",
    "\n",
    "\n",
    "display(selected_columns)\n",
    "X_new = X[selected_columns]\n",
    "\n",
    "\n",
    "X_train_temp, X_test = proc_normalization(X_train_org, X_new.values)   \n",
    "\n",
    "\n",
    "display(X_test.shape)\n",
    "\n",
    "display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "fit_predict(model_eval_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print_eval_result(model_eval_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['mon_on_book', 'tot_product_count', 'months_inact_for_12m',\n",
    "       'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "       'tot_trans_amt_for_12m', 'tot_trans_cnt_for_12m',\n",
    "       'tot_cnt_ratio_q4_q1'],\n",
    "      dtype='object')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library and Data Loading, Function Definition for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리(Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 전처리\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    x_test['is_churned']=x_test['is_churned'].replace({'Existing Customer':0,'Attrited Customer':1})\n",
    "    \n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------\n",
    "    x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "\n",
    "\n",
    "    # 범주형 데이터 One-Hot 인코딩\n",
    "    # --------------------------\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['education']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['imcome_cat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['marital_stat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['card_type']).drop(columns=['Platinum'])],axis=1)\n",
    "    x_test.drop(columns = ['education','imcome_cat','marital_stat','card_type'],inplace=True)\n",
    "\n",
    "\n",
    "    # Null 처리 1 방식\n",
    "    # ---------------\n",
    "    # x_test.dropna(axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    # Null 처리 2 방식\n",
    "    # ---------------\n",
    "    # x_test.drop(columns = ['sex'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_revol_balance'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_amt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_trans_amt_for_12m'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_cnt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "\n",
    "\n",
    "    # # Null 처리 3 방식\n",
    "    # # ----------------\n",
    "    x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = test_transform(bank_churner_df)\n",
    "\n",
    "y=bank_churner_df['is_churned']\n",
    "X_new=bank_churner_df.drop(['is_churned'], axis=1)\n",
    "\n",
    "X_new = X_new[['age', 'dependent_num', 'tot_product_count', 'months_inact_for_12m',\n",
    "       'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "       'tot_amt_ratio_q4_q1', 'tot_trans_amt_for_12m', 'tot_cnt_ratio_q4_q1']]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "#Normalization\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP LEARNING\n",
    "\n",
    "# Scale the data with StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('auc_keras: ')\n",
    "# --------------------------------\n",
    "\n",
    "# Calculate metrics\n",
    "# print(f\"Model Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "# print(f\"Model F1-Score: {f1_score(y_test, y_pred, average='weighted') * 100:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "# Calculate accuracies per class\n",
    "# accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "# accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "\n",
    "#pred_proba_proba = model.predict_proba(X_test)[:, 1]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "#get_clf_eval(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compute confusion matrix for the Deep learning model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix. Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    #Plot the confusion matrix.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.grid(False)  # <-- Agregar esta línea para evitar el aviso de deprecación\n",
    "    plt.title(title,fontsize=18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Add labels to the cells of the confusion matrix.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['y=0','y=1'],normalize= True,  title='Confusion matrix')\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고객 이탈 예측 분석\n",
    "LightGBM은 91%의 가장 높은 Attrited Customer Recall과 89%의 정밀도를 가지고 있음\n",
    "고객 이탈을 사전에 방지하기 위해서 LightGBM 모델을 사용하는 것이 적합함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
