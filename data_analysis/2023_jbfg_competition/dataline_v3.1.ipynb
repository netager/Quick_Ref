{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JBFG Data Analysis Competition\n",
    "- Null 처리방식 : 2번"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Environment and Library Version\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'DataLine' -nmv --packages numpy,pandas,sklearn,imblearn,tensorflow,plotly,matplotlib,seaborn,missingno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색적 데이터 분석\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library and Data Loading, Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl  \n",
    "import missingno as msno\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "# import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rc('font', family='Malgun Gothic')  # 한글 폰트 설정\n",
    "                                        # 윈도우 폰트 위치 - C:\\Windows\\Fonts\n",
    "plt.figure(figsize=(10,6))              # 그래프 사이즈 설정\n",
    "sns.set(font='Malgun Gothic', rc={'axes.unicode_minus':False}, style='darkgrid') # 마이너스 처리\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속형 데이터 그래프 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_continuous_graphs(df, column, column_desc):\n",
    "\n",
    "       counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "       exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "       churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "       churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "\n",
    "       fig = make_subplots(rows=3, \n",
    "                     cols=2, \n",
    "                     subplot_titles=('전체 건수 분포', '유지/이탈별 사분위', '유지/이탈별 분포'), \n",
    "                     # shared_xaxes=True,\n",
    "                     horizontal_spacing=0.1,\n",
    "                     vertical_spacing=0.1,\n",
    "                     specs=[[{\"secondary_y\": True}, {}],\n",
    "                            [{}, {\"secondary_y\": True}],\n",
    "                            [{\"secondary_y\": True},{}],\n",
    "                            ]\n",
    "                     )\n",
    "\n",
    "       # 전체\n",
    "       # ----\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column],  marker_color=\"red\", name='이탈'), row=1, col=1, secondary_y=False)\n",
    "       # fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], texttemplate=\"%{x}\", marker_color=\"red\"), row=1, col=1, secondary_y=False)\n",
    "       \n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\", name='유지'), row=1, col=1, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=1, col=1, secondary_y=True)\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "\n",
    "\n",
    "       \n",
    "       # Box Graph\n",
    "       # ---------\n",
    "       # fig.add_trace(go.Box(x=exist_counts, \n",
    "       #               name='유지'), row=2, col=1)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']==0][column].sort_values(), name='유지', marker_color=\"blue\"), row=1, col=2)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']!=0][column].sort_values(), name='이탈', marker_color=\"red\"), row=1, col=2)\n",
    "\n",
    "\n",
    "       # Histogram Graph\n",
    "       # ---------------\n",
    "\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\"), row=2, col=1)\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], marker_color=\"red\"), row=2, col=1)\n",
    "\n",
    "       # Scatter Graph\n",
    "       # -------------\n",
    "       fig.add_trace(go.Scatter(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), mode='lines+markers', marker_color=\"red\", name='이탈'), row=2, col=2, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), mode='lines+markers', marker_color='blue', name='유지'), row=2, col=2, secondary_y=False)\n",
    "\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=2, col=2, secondary_y=True)\n",
    "\n",
    "\n",
    "       # 이탈률\n",
    "       # ------\n",
    "       # churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "       fig.add_trace(go.Histogram(x=churn_rates.sort_index()), row=3, col=1)\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=3, col=1)\n",
    "\n",
    "\n",
    "       fig.update_layout(width=1200, \n",
    "                     height=1200, \n",
    "                     showlegend=False,\n",
    "                     barmode='stack',\n",
    "                     hovermode=\"x\",\n",
    "                     )\n",
    "\n",
    "       fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_func(df, column, start_value, units):\n",
    "    bins = np.arange(start_value, df[column].max()+units, units)\n",
    "    bins_label = [str(round(x,2)) for x in bins]\n",
    "    df[f\"{column}_category\"] = pd.cut(df[column], bins, right=True, include_lowest=True, labels=bins_label[:-1])\n",
    "\n",
    "    return df\n",
    "\n",
    "#eda_churner_df = category_func(eda_churner_df, 'mean_util_pct', 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 범주형 데이터 그래프 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_category_graphs(df, column, column_desc):\n",
    "    \n",
    "    counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "    exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "    churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "    churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "    \n",
    "    \n",
    "    fig = make_subplots(rows=3, \n",
    "                    cols=2, \n",
    "                    subplot_titles=('【 전체 현황 】', '【 이탈율 】', '【 사분위 】', f'【 {column_desc} 중 전체 현황 】', f'【 {column_desc} 중 유지 현황 】', f'【 {column_desc} 중 이탈 현황 】'), \n",
    "                    # shared_xaxes=True,\n",
    "                    horizontal_spacing=0.1,\n",
    "                    vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}, {}],\n",
    "                           [{}, {'type':'domain'}],\n",
    "                           [{'type':'domain'}, {'type':'domain'}]]\n",
    "                   )\n",
    "\n",
    "\n",
    "    # 전체 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Bar(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), marker_color=\"red\", offsetgroup=0, name='이탈', \n",
    "                         text=churn_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto'), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Bar(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), marker_color=\"blue\", offsetgroup=0, name='유지', \n",
    "                         texttemplate='%{value:,}', \n",
    "                        #  text=exist_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto', base=churn_counts.sort_index()), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', \n",
    "                             line_shape='linear'), row=1, col=1, secondary_y=True)\n",
    "    \n",
    "    fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "    #fig.update_traces(texttemplate='%{value:,}', hovertemplate = '%{label}, %{value}', row=1, col=1)\n",
    "    # fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "    \n",
    "\n",
    "    # 이탈율\n",
    "    # ------\n",
    "    fig.add_trace(go.Bar(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"red\", name='이탈율'),\n",
    "                  row=1, col=2)\n",
    "\n",
    "    \n",
    "    # 사분위\n",
    "    # ------\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']!=0][column].sort_values(), marker_color=\"red\", name='이탈'), row=2, col=1)\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']==0][column].sort_values(), marker_color=\"blue\", name='유지'), row=2, col=1)\n",
    "\n",
    "\n",
    "    # 유지/이탈 현황\n",
    "    # -------------\n",
    "    fig.add_trace(go.Pie(labels=counts.sort_index().index, values=counts.sort_index(), name=f'{column_desc} 분표 현황', texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=2, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=2, col=2)\n",
    "\n",
    "  \n",
    "    # 유지 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=exist_counts.sort_index().index, values=exist_counts.sort_index(), name=\"유지\", texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=1)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=1)\n",
    "\n",
    "\n",
    "    # 이탈 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=churn_counts.sort_index().index, values=churn_counts.sort_index(), name=\"이탈\", texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=2)\n",
    "\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.5, ax=0, ay=0,\n",
    "                    xref = \"paper\", yref = \"paper\", \n",
    "                    text= \"<b>전체</b>\", \n",
    "                    font_size=20,\n",
    "                  ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.21, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>유지</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>이탈</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    \n",
    "    fig.update_layout(width=1200, \n",
    "                  height=1200, \n",
    "                  showlegend=False,\n",
    "                  title_text=f'『 {column_desc} 』에 따른 분석 그래프',\n",
    "                # barmode='stack'\n",
    "                  hovermode=\"x\",\n",
    "                 )\n",
    "    \n",
    "\n",
    "    fig.show()    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 개략 확인 함수\n",
    "- 데이터 건수, null 비율 등으로 변경해서 사용하면 좋을 듯함."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the percentages of \"Yes\" and \"No\" responses for each education level\n",
    "education_percents = bank_churner_df_org.groupby('education')['is_churned'].value_counts(normalize=True).unstack()\n",
    "education_percents.reset_index(inplace=True)\n",
    "education_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "# Create a DataFrame with the percentages of \"No\" and \"Yes\" responses for each Education_Level value\n",
    "education_percent_table = education_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format the percentages as percentage notation\n",
    "education_percent_table['No_Percentage'] = education_percent_table['No_Percentage'].apply(lambda x: f\"{x:.2%}\")\n",
    "education_percent_table['Yes_Percentage'] = education_percent_table['Yes_Percentage'].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "# Sort the table in descending order based on the \"Yes_Percentage\" value\n",
    "education_percent_table = education_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Print the percentage table without the index column\n",
    "print(education_percent_table[['education', 'No_Percentage', 'Yes_Percentage']].to_string(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = pd.read_csv(\"./data/bank_churner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석 데이터 개략 확인\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 세트 정보 확인\n",
    "- 일부 Feature에 Null 값 존재함을 확인 함\n",
    "- 향후 모델학습시 Null 값 처리에 대한 필요성 확인 함\n",
    "- 모델학습을 의해 Oject 항목을 적절하게 변형할 필요성을 확인 함 - sex, education, marital_stat, imcome_cat, card_type (5개 Features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수치 데이터의 분포값 개략 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결측치 확인 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eda_churner_df.isnull().sum())\n",
    "\n",
    "null_rates = eda_churner_df.isnull().sum() / 8101 * 100\n",
    "null_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 12))\n",
    "msno.bar(eda_churner_df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature별 특징 분석"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_churned : 이탈  여부\n",
    "- 0 : 유지, 1 : 이탈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df[\"is_churned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['is_churned'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['is_churned'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Pie(labels=['유지', '이탈'], values=eda_churner_df['is_churned'].value_counts().sort_index(), name='이탈별 분포', \n",
    "                     texttemplate = \"%{value:,}명 <br><b>(%{percent})</b>\",\n",
    "                     title='<b>전체<br> 8,101</b>',\n",
    "                         textposition = \"inside\"))\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", pull=[0,0.1])\n",
    "fig.update_layout(width=500, \n",
    "                  height=500, \n",
    "                  showlegend=True,\n",
    "                  title_text=\"<b>유지/이탈 분포 현황<b>\",\n",
    "                  title_x = 0.5,\n",
    "                  title_y = 0.9,\n",
    "                  title_xanchor = \"center\",\n",
    "                  title_yanchor = \"middle\")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### age : 나이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(eda_churner_df, 'age', '나이')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나이에 대해 일부 이상치가 있지만, 대체적으로 유지 고객과 이탈 고객의 분포가 일치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이 범주화\n",
    "eda_churner_df = category_func(eda_churner_df, 'age', 20, 10)\n",
    "print_category_graphs(eda_churner_df, 'age_category', '나이 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sex : 성별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'sex', '성별')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dependent_num : 부양가족수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'dependent_num', '부양가족수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### education : 교육수준\n",
    "- Graduate : 대학원\n",
    "- High School : 고졸\n",
    "- Unknown\n",
    "- Uneducated : 미교육\n",
    "- College : 단과대학\n",
    "- Post-Graduate : 보딩스쿨(재수)\n",
    "- Doctorate :박사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'education', '교육수준')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 박사 학위를 받은 고객의 경우 이탈율이 더 높게 나타나고, 고등학교 및 대학 교육을 받은 고객은 이탈율이 더 낮은 것으로 보이나, 박사 학위 고객 수는 가장 낮음\n",
    "\n",
    "    - 박사 학위의 경우 카드사의 혜택을 더 많이 고려하여 이동하는 것으로 보임   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### marital_stat : 결혼상태\n",
    "- Married  : 결혼\n",
    "- Single   : 미혼\n",
    "- Divorced : 이혼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'marital_stat', '결혼상태')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imcome_cat : 수입규모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'imcome_cat', '수입규모')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 소득 수입규모가 12만 달러 이상인 고객, 4만 달러 미만인 고객의 이탈율이 높게 나타나고 있으면,\n",
    "\n",
    "    - 전체 고객대비 소득 수입규모가 40만 달러 미만인 고객이 차지하는 점유율이 높으므로 이 범주의 고객을 중점적으로 관리할 필요가 있음\n",
    "\n",
    "    - 전반적으로 소득 수입규모별 감소율은 큰 차이가 없음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### card_type : 카드종류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'card_type', '카드종류')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 플래티넘 카드 소유자의 건수는 매우 낮음에도 이탈율은 상당히 높게 나타나고 있음.\n",
    "\n",
    "    - 이는 플래티넘 카드를 사용하는 고객의 만족도가 매우 낮은 것으로 보이며 플래티넘 카드의 혜택을 다른 카드사와 비교하여 부족한 부문을 찾아내어 개선하거나 고객이 만족할만한 혜택을 제공하여 이탈율을 낮출 필요가 있음 \n",
    "\n",
    "    - 플래티넘 카드 소지자는 은행 서비스 이용을 중단하는 경향이 있습니다. 수수료가 너무 높거나 연간 서비스가 부족합니까? 수수료 결제됐나요?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mon_on_book : 은행 거래 기간 (개월 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(eda_churner_df, 'mon_on_book', '은행 거래 기간')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'mon_on_book', 10, 10)\n",
    "print_category_graphs(eda_churner_df, 'mon_on_book_category', '은행거래기간 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_product_count : 현재 보유 상품 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'tot_product_count', '현재 보유 상품 개수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 이탈하지 않은 고객의 카드 보유 갯수의 중앙값은 4개이고 이탈한 고객의 중앙값은 3개임.\n",
    "    - 카드의 개수가 적을수록 이탈율이 높아지므로 4개 이상의 카드를 보유할 수 있도록 노력 필요"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### months_inact_for_12m : 최근 12개월 동안 카드 거래가 없었던 개월 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'months_inact_for_12m', '최근 12개월 동안 카드 거래가 없었던 개월 수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 카드 거래 없는 개월수가 4개월 동안 없으면 이탈율이 정점을 찍고 이후 감소하는 경향을 보이고 있음\n",
    "    \n",
    "    - 이탈한 고객의 평균 기간은 3개월이고 유지하는 고객의 경우 2개월 임\n",
    "    \n",
    "    - 1개월 이상 카드 거래가 없는 고객은 잠재적으로 이탈할 확률이 높을 것으로 예상되므로 주기적으로 관리할 필요가 있음\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### contact_cnt_for_12m : 최근 12개월 동안 연락 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(eda_churner_df, 'contact_cnt_for_12m', '최근 12개월 동안 연락 횟수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "   - 고객 접촉 건수와 이탈율 사이에는 명확한 관계가 있어 보임\n",
    "\n",
    "   - 접촉 건수가 많을 수록 이탈율이 높아지는 경향을 보임\n",
    "\n",
    "   - 데이터상 12개월동안 접촉건수가 고객이 접촉한 건수인지, 은행에서 접속한 건수 인지를 구분하여 분석할 필요가 있음\n",
    "      - 은행이 고객을 접촉한 건수라면 연체 등으로 잦은 고객 독촉으로 이탈율이 높아질 가능성 있어 보이며 \n",
    "      \n",
    "      - 고객이 은행을 접촉한 건수라면 카드 관련 서비스의 불만을 원할하게 해결하지 못해 건수가 증가하고 이로인해 이탈율이 상승.<br>\n",
    "   따라서, 카드관련 부서에서는 대고객 접촉 서비스를 세부적으로 분석할 필요가 있으며 대고객 대응 가이드를 점검해 볼 필요가 있음\n",
    "\n",
    "\n",
    "   - 대고객 접촉 채널은 고객 행동을 분석하는데 있어 중요한 항목이므로 세부적으로 관리할 필요가 있으며 정기적으로 분석할 필요가 있음 \n",
    "   - 접촉 채널, 접촉 주체, 접촉 내용 구분, 접촉 세부 내용, 문제해결 여부, 접촉에 대한 만족도 등 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### credit_line : 카드 한도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'credit_line', 0, 5000)\n",
    "print_category_graphs(eda_churner_df, 'credit_line_category', '카드 한도 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_revol_balance : 리볼빙 잔액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'tot_revol_balance', 0, 500)\n",
    "print_category_graphs(eda_churner_df, 'tot_revol_balance_category', '리볼빙 잔액 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 신용카드 결제대상 잔액이 적을수록 이탈에 대한 비율이 커지므로 약 $1,000 이하 잔액이 있는 고객을 대상으로 마케팅할 필요가 있음  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_open_to_buy : 평균 사용가능 신용한도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'mean_open_to_buy', 0, 5000)\n",
    "print_category_graphs(eda_churner_df, 'mean_open_to_buy_category', '평균 사용가능 신용한도 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_amt_ratio_q4_q1 : 1분기 대비 4분기의 거래 금액 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['tot_amt_ratio_q4_q1'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['tot_amt_ratio_q4_q1'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'tot_amt_ratio_q4_q1', 0, 0.2)\n",
    "print_category_graphs(eda_churner_df, 'tot_amt_ratio_q4_q1_category', '1분기 대비 4분기의 거래 금액 비율 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 1분기 대비 4분기의 거래 금액의 변화는 비슷하나 이탈하지 않은 고객의 분포를 보면 이상치가 많음. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_trans_amt_for_12m : 최근 12개월 동안의 거래 금액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['tot_trans_amt_for_12m'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['tot_trans_amt_for_12m'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('최근 12개월 동안의 거래 금액 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_trans_amt_for_12m', data=eda_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'tot_trans_amt_for_12m', 0, 2000)\n",
    "print_category_graphs(eda_churner_df, 'tot_trans_amt_for_12m_category', '최근 12개월 동안의 거래 금액')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 이탈하지 않은 고객의 거래 금액이 더 높은 것으로 나타나고 있으나, 거래 금액별로 이탈율이 다르게 나타나고 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_trans_cnt_for_12m : 최근 12개월 동안의 거래 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['tot_trans_cnt_for_12m'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['tot_trans_cnt_for_12m'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(eda_churner_df, 'tot_trans_cnt_for_12m', '최근 12개월 동안의 거래 횟수')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'tot_trans_cnt_for_12m', 0, 20)\n",
    "print_category_graphs(eda_churner_df, 'tot_trans_cnt_for_12m_category', '최근 12개월 동안의 거래 횟수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 거래건수가 많을수록 유지되는 고객이 많으며 최근 12개월 동인 50번 이하의 고개 이탈율이 높아지므로 정기적으로 거래건수를 분석하여 대응할 필요가 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_cnt_ratio_q4_q1 : 1분기 대비 4분기의 거래 횟수 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['tot_cnt_ratio_q4_q1'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['tot_cnt_ratio_q4_q1'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('1분기 대비 4분기의 거래 횟수 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_cnt_ratio_q4_q1', data=eda_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'tot_cnt_ratio_q4_q1', 0, 0.3)\n",
    "print_category_graphs(eda_churner_df, 'tot_cnt_ratio_q4_q1_category', '1분기 대비 4분기의 거래 횟수 비율')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_util_pct : 평균 한도 소진율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = eda_churner_df['mean_util_pct'].count().sum()\n",
    "tot_null_cnt = eda_churner_df['mean_util_pct'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('평균 한도 소진율 분포 및 이탈 현황')\n",
    "sns.histplot(x='mean_util_pct', data=eda_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df = category_func(eda_churner_df, 'mean_util_pct', 0, 0.2)\n",
    "print_category_graphs(eda_churner_df, 'mean_util_pct_category', '평균 한도 소진율')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 한도 소진율이 낮을수록 이탈율은 올라가는 경향이 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 연속형 데이터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "# for i, col in enumerate(bank_churner_df.drop(['is_churned'], axis=1).select_dtypes(include=['int','float']).columns):\n",
    "for i, col in enumerate(['age', 'mon_on_book', 'credit_line', 'tot_revol_balance','mean_open_to_buy','tot_amt_ratio_q4_q1','tot_trans_amt_for_12m','tot_trans_cnt_for_12m','mean_util_pct']):        \n",
    "# for i, col in enumerate(['age', 'mon_on_book', 'credit_line' ]):    \n",
    "    # We exclude the 'y' column and only consider the columns of numerical type.\n",
    "    # Excluimos la columna 'y' y solo consideramos las columnas de tipo numérico.\n",
    "\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    ax = plt.subplot(4, 4, i+1)  # Creating a subplot for each column.\n",
    "    # Creamos una subfigura para cada columna.\n",
    "\n",
    "     # Plotting the histogram for each column\n",
    "    sns.histplot(data=eda_churner_df, x=col, ax=ax, color='red', kde=True)\n",
    "\n",
    "    # Plotting the KDE curve with custom color and linewidth\n",
    "    # Plotting the histogram for each column.\n",
    "    # Graficamos el histograma para cada columna.\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_xlabel(col, fontsize=18)\n",
    "    ax.set_ylabel('Count', fontsize=18)\n",
    "    \n",
    "plt.suptitle('Data distribution of continuous variables',fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature별 특징 분석 결과 요약"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저소득층에 집중: 구매력은 크지 않지만 대부분의 고객은 저소득층입니다. 저소득층을 위한 프로모션을 시행하는 것은 해당 클러스터 그룹의 고객 이탈을 줄이는 좋은 대안이 될 수 있습니다.\n",
    "\n",
    "- 활동 수준이 낮을 때 조치: 활동 수준이 낮은 고객(트랜잭션 45개 미만)이 조직을 떠날 확률이 더 높다는 것을 확인했습니다. 직원이 활동 수준이 낮은 고객에게 전화를 걸어 그들의 요구 사항에 맞는 새로운 제품을 제안하거나 고객이 우리가 제공하는 서비스에 만족하는지, 개선하기 위해 할 수 있는 것이 있는지 묻는다면 우리는 아마도 무엇에 대해 더 나은 통찰력을 얻을 수 있을 것입니다. 우리는 활동 수준을 높이기 위해 할 수 있습니다.\n",
    "\n",
    "- 회전 잔액이 적은 사람들에게 신용 한도를 늘리시겠습니까? 우리 모델에 따르면 회전 잔액이 낮은 고객은 조직을 떠날 가능성이 더 높습니다. 어쩌면 해당 고객에게 더 높은 신용 잔액을 구현함으로써 해당 세그먼트 그룹이 조직을 떠날 확률이 낮아질 수 있습니다.\n",
    "\n",
    "\n",
    "- 기술적 분석\n",
    "    - 거래금액과 거래횟수가 많을수록 이탈 가능성이 낮아짐\n",
    "    - 플리티넘 카드 보유자 수가 적고 회원 탈퇴율이 높아 좀 더 관심을 가져야 할 분야\n",
    "    - 이탈율은 비활성화 된지 4개월이 지나면 최고조에 달하니 이러한 현생이 발생한 이유를 좀 더 세부적으로 분석할 필요가 있음\n",
    "\n",
    "    - 카드의 종류가 많을수록 이탈 가능성이 줄어들며\n",
    "    - 접촉 건수에 따른 이탈율을 좀더 분석할 필요가 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다변량 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_churner_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=eda_churner_df, x='sex', y='age', hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=eda_churner_df, x='sex', y='age', hue='is_churned')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 성별, 카드 종류별 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=2,subplot_titles=('','<b>Platinum Card Holders','<b>Blue Card Holders<b>','Residuals'),\n",
    "    vertical_spacing=0.09,\n",
    "    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n",
    "           [None                               ,{\"type\": \"pie\"}]            ,                                      \n",
    "          ]\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(values=bank_churner_df.sex.value_counts().values,labels=['<b>여자<b>','<b>남자<b>'],hole=0.3,pull=[0,0.3]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=['Female Platinum Card Holders','Male Platinum Card Holders'],\n",
    "        values=bank_churner_df.query('card_type==\"Platinum\"').sex.value_counts().values,\n",
    "        pull=[0,0.05,0.5],\n",
    "        hole=0.3\n",
    "        \n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Pie(\n",
    "#         labels=['Female Gold Card Holders','Male Blue Card Holders'],\n",
    "#         values=bank_churner_df.query('card_type==\"Gold\"').sex.value_counts().values,\n",
    "#         pull=[0,0.2,0.5],\n",
    "#         hole=0.3\n",
    "#     ),\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=['Female Silver Card Holders','Male Blue Card Holders'],\n",
    "        values=bank_churner_df.query('card_type==\"Silver\"').sex.value_counts().values,\n",
    "        pull=[0,0.2,0.5],\n",
    "        hole=0.3\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    showlegend=True,\n",
    "    title_text=\"<b>Distribution Of Gender And Different Card Statuses<b>\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library and Data Loading, Function Definition for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "\n",
    "# import scikitplot as skplt\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    f11 = f1_score(y_test, pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    # print('오차 행렬')\n",
    "    # print(confusion)\n",
    "    \n",
    "    # ROC-AUC print 추가\n",
    "    # ------------------\n",
    "    # print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    # F1: {3:.4f}, F11: {4:.4f}, AUC:{5:.4f}'.format(accuracy, precision, recall, f1, f11, roc_auc))   \n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "          F1: {3:.4f}, F11: {4:.4f}, AUC:{5:.4f}'.format(accuracy, precision, recall, f1, f11, roc_auc))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve_plot(y_test=None, pred_proba=None):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve_plot(y_test=None, pred_proba=None):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_plot(y_test , pred_proba):\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음. \n",
    "    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba)\n",
    "\n",
    "    # ROC Curve를 plot 곡선으로 그림. \n",
    "    plt.plot(fprs , tprs, label='ROC')\n",
    "    # 가운데 대각선 직선을 그림. \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   \n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel('FPR( 1 - Sensitivity )'); plt.ylabel('TPR( Recall )')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리(Pre-Processing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도, 상관도 분석 - 다중공선성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,20)}) # 그래프 크기\n",
    "\n",
    "corr = bank_churner_df.corr() # 상관행렬 표 만들기\n",
    "sns.heatmap(round(corr,1), \n",
    "            annot=True, # 상관계수 표시\n",
    "            fmt='.1f', # 상관계수 소수점 자리\n",
    "            cmap='coolwarm', # 컬러맵 색상 팔레트 \n",
    "            vmax=1.0, # 상관계수 최댓값 \n",
    "            vmin=-1.0, # 상관계수 최솟값\n",
    "            linecolor='white', # 셀 테두리 색상 \n",
    "            linewidths=.05) # 셀 간격 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hue = 'is_churned', data = bank_churner_df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그래프를 시각적으로 분석하면 특정 변수에 대한 명확한 클러스터링 패턴을 관찰할 수 있습니다. 패턴이 다음과 같을 때 관심이 갑니다.\n",
    "신용 한도의 대각선 그래프를 보면 은행을 떠나기로 결정한 사람들은 신용 한도가 낮은 사람들입니다.\n",
    "신용한도가 높은 사람들은 은행을 떠나지 않기로 결정합니다(파란색 그래프의 정점). 신용 한도와 다른 변수를 그래프로 표시할 때,\n",
    "흥미로운 클러스터링 패턴을 찾을 수 있습니다.\n",
    "얼핏 보면 고객의 체류 여부를 결정하는 데 다음과 같은 변수가 중요한 영향을 미치는 것으로 보입니다.\n",
    "\n",
    "age, credit_lile, tot_revol_balance, tot_amt_ratio_q4_q1, tot_trans_amt_for_12m, tot_cnt_ratio_q4_q1, mean_util_pct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시사점\n",
    "    - 다중공선성: 다중공선성은 회귀 모델에서 두 개 이상의 독립변수가 높은 상관관계를 가질 때 발생합니다. 이로 인해 가변 계수의 해석이 어려워지고 모델의 안정성과 신뢰성이 낮아질 수 있습니다.\n",
    "\n",
    "    - age와 mon_on_book, credit_line과 mean_open_to_buy, tot_trans_cnt_for_12m와 tot_trans_amt_for_12m 사이에도 강한 상관관계가 있음을 관찰\n",
    "이로 인해 mon_on_book, mean_open_to_buy, tot_trans_cnt_for_12m 열을 제거할 예정입니다.\n",
    "\n",
    "컬럼명               Null 건수\n",
    "------------------- ---------  \n",
    "age                         0 - 삭제1\n",
    "sex                       808\n",
    "imcome_cat               1619\n",
    "mon_on_book                 0 - 삭제1 : 삭제\n",
    "credit_line                 0 - 삭제2\n",
    "tot_revol_balance        1521                 - 공선성1 \n",
    "mean_open_to_buy            0 - 삭제2 : 삭제 \n",
    "tot_amt_ratio_q4_q1      2435\n",
    "tot_trans_amt_for_12m    1669 - 삭제3\n",
    "tot_trans_cnt_for_12m    3250 - 삭제3 : 삭제\n",
    "tot_cnt_ratio_q4_q1      1629\n",
    "mean_util_pct            2526                 - 공선성1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = bank_churner_df[['cstno', 'is_churned', 'age', 'dependent_num', 'tot_product_count', 'months_inact_for_12m',\n",
    "        'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "        'tot_amt_ratio_q4_q1', 'tot_trans_amt_for_12m', 'tot_cnt_ratio_q4_q1']]\n",
    "bank_churner_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,20)}) # 그래프 크기\n",
    "corr = bank_churner_df.corr() # 상관행렬 표 만들기\n",
    "sns.heatmap(round(corr,1), \n",
    "            annot=True, # 상관계수 표시\n",
    "            fmt='.1f', # 상관계수 소수점 자리\n",
    "            cmap='coolwarm', # 컬러맵 색상 팔레트 \n",
    "            vmax=1.0, # 상관계수 최댓값 \n",
    "            vmin=-1.0, # 상관계수 최솟값\n",
    "            linecolor='white', # 셀 테두리 색상 \n",
    "            linewidths=.05) # 셀 간격 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hue = 'is_churned', data = bank_churner_df)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    # x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    \n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------\n",
    "    # x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    # x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    # x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "\n",
    "    # x_test = x_test[['is_churned', 'age', 'dependent_num', 'tot_product_count', 'months_inact_for_12m',\n",
    "    #     'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "    #     'tot_amt_ratio_q4_q1', 'tot_trans_amt_for_12m', 'tot_cnt_ratio_q4_q1']]\n",
    "\n",
    "    # 범주형 데이터 One-Hot 인코딩\n",
    "    # --------------------------\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['education']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['imcome_cat']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['marital_stat']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['card_type']).drop(columns=['Platinum'])],axis=1)\n",
    "    # x_test.drop(columns = ['education','imcome_cat','marital_stat','card_type'],inplace=True)\n",
    "\n",
    "\n",
    "    # Null 처리 1 방식\n",
    "    # ---------------\n",
    "    # x_test.dropna(axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    # Null 처리 2 방식\n",
    "    # ---------------\n",
    "    # x_test.drop(columns = ['sex'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_revol_balance'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_amt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_trans_amt_for_12m'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_cnt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "\n",
    "\n",
    "    # # Null 처리 3 방식\n",
    "    # # ----------------\n",
    " #   x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    return x_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = test_transform(bank_churner_df)\n",
    "bank_churner_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create our feature matrix and our target variable vector.\n",
    "X=bank_churner_df.drop(['is_churned'],axis=1)\n",
    "y=bank_churner_df['is_churned']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습에 사용될 중요 Feature 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Selection of the most important features to conduct the training\n",
    "#Selección de las características más importantes para llevar a cabo el entrenamiento\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Establecer la semilla aleatoria para reproducibilidad\n",
    "#Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "#Define a list of available models for selection\n",
    "# Definir una lista de modelos disponibles para selección\n",
    "available_models = {\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "    #'SVM': SVC(kernel='linear'),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'LASSO': Lasso(alpha=0.01),  # Agrega LASSO aquí\n",
    "    #'RFE': RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=10)\n",
    "    # Agrega otros modelos aquí si lo deseas\n",
    "}\n",
    "\n",
    "# Choose the desired model for feature selection\n",
    "chosen_model = 'ExtraTrees'\n",
    "\n",
    "# Create the selected model\n",
    "clf = available_models[chosen_model]\n",
    "\n",
    "#Train the model with the data\n",
    "clf = clf.fit(X.values, y)\n",
    "\n",
    "# Obtain feature importances from the model\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# Create a SelectFromModel object with the trained classifier\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "#Transform the original features to obtain the selected ones\n",
    "X_new = model.transform(X.values)\n",
    "\n",
    "selected_feature_indices = model.get_support(indices=True)\n",
    "\n",
    "#Get the indices of the selected features\n",
    "selected_columns = X.columns[selected_feature_indices]\n",
    "\n",
    "#Print the selected columns\n",
    "print(\"Selected columns:\")\n",
    "print(selected_columns)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Based on the analysis of the graphs, we had predicted that:\n",
    "#At first glance, the following variables seem to have a significant influence on the determination of whether customers stay or not: Customer_Age, Credit_Limit,\n",
    "#Total_Recovering_Bal, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt, Total_Ct_Chng_Q4_Q1, Avg_Utilization_Ratio\n",
    "#It seems that our intuition was correct.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Get the indices of all columns in descending order of importance\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "#Get the names of all columns in the same order\n",
    "sorted_columns = X.columns[sorted_indices]\n",
    "\n",
    "#Get the sorted importances\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#Create a bar chart to display the importance of all columns in descending order\n",
    "sns.barplot(x=sorted_importances, y=sorted_columns, palette=['lightgrey' if i not in selected_feature_indices else 'blue' for i in sorted_indices])\n",
    "\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Feattures\", fontsize=14)\n",
    "plt.title(\"Feature Importance\", fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "#stratify=y: It is used to ensure that the distribution of classes in the training and test sets is similar to the original distribution \n",
    "#of the target variable y. This is particularly useful when dealing with umbalanced classes, as it ensures that both parts of the split have\n",
    "#a similar proportion of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the size of the training and testing sets\n",
    "print(\"Training X size: \", X_train.shape)\n",
    "print(\"Training y size: \", y_train.shape)\n",
    "print(\"Test X size: \", X_test.shape)\n",
    "print(\"Test y size: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(['No', 'Yes'], y_train.value_counts(), color=['blue', 'orange'])\n",
    "plt.xlabel('Response', fontsize=18)\n",
    "plt.ylabel('Couts', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Target Variable Distribution', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE(Synthetic Minority Oversampling Technique)\n",
    "#SMOTE is a technique for oversampling the minority class. Simply adding duplicate records of the minority class often does not add new\n",
    "#information to the model. In SMOTE, new instances are generated from the existing data. To put it simply, SMOTE examines instances of \n",
    "#the minority class and uses the k-nearest neighbors method to select a randomly close neighbor, and a new synthetic instance is created \n",
    "#in the feature space.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train,y_train=sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(['No', 'Yes'], y_train.value_counts(), color=['blue', 'orange'])\n",
    "plt.xlabel('Response', fontsize=18)\n",
    "plt.ylabel('Couts', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Target Variable Distribution', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델별 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with different models\n",
    "#entrenamiento con distintos modelos\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Create a list of tuples with the model name and the classifier instance\n",
    "# Crear una lista de tuplas con el nombre del modelo y la instancia del clasificador\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "                        # Diccionario para almacenar las métricas de comparación de modelos\n",
    "\n",
    "for model_name, classifier in models:\n",
    "    #Fit the model using the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    #Make predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    #Calculate model metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    ## 확인 필요 ??? \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "    cv_accuracy = accuracies.mean()\n",
    "    cv_std = accuracies.std()\n",
    "    accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "    accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1], )\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "    #Print model metrics\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    # print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "    # print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "    # print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "\n",
    "    #Add metrics to the models comparison dictionary\n",
    "    model_comparison[model_name] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std, roc_auc]\n",
    "    # print(classification_report(y_pred, y_test, zero_division=1))\n",
    "    # print(\"-\" * 60)\n",
    "\n",
    "    \n",
    "    get_clf_eval(y_test, y_pred, pred_proba)\n",
    "    # precision_recall_curve_plot(y_test, pred_proba)\n",
    "    # roc_curve_plot(y_test , pred_proba)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble methods in machine learning involve combining multiple models (often weaker models or base models) to create a stronger,\n",
    "#more robust predictive model. The idea behind ensembling is that by combining the predictions of multiple models, the strengths \n",
    "#of each individual model can compensate for the weaknesses of others, leading to improved overall performance.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='soft')  # Puedes usar 'hard' o 'soft' para el voto\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracies = cross_val_score(estimator=voting_classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "cv_accuracy = accuracies.mean()\n",
    "cv_std = accuracies.std()\n",
    "accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0] )\n",
    "accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "\n",
    "print(\"Model: Voting Classifier\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "model_comparison['Voting Classifier'] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std, roc_auc]\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISSON\n",
    "\n",
    "Model_com_df=pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns=['Model Accuracy','Model Accuracy-0','Model Accuracy-1','Model F1-Score','CV Accuracy','CV std', 'AUC']\n",
    "Model_com_df=Model_com_df.sort_values(by='AUC',ascending=False)\n",
    "Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Model_com_df = pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns = ['Model Accuracy', 'Model Accuracy-No', 'Model Accuracy-Yes', 'Model F1-Score', 'CV Accuracy', 'CV std', 'AUC']\n",
    "Model_com_df = Model_com_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "def highlight_below_75(s):\n",
    "    if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "        return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "    else:\n",
    "        return ['color: black'] * len(s)\n",
    "\n",
    "styled_df = Model_com_df.style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV Accuracy']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV Accuracy'])\n",
    "styled_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library and Data Loading, Function Definition for Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리(Pre-Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 전처리\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    x_test['is_churned']=x_test['is_churned'].replace({'Existing Customer':0,'Attrited Customer':1})\n",
    "    \n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------\n",
    "    x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "\n",
    "\n",
    "    # 범주형 데이터 One-Hot 인코딩\n",
    "    # --------------------------\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['education']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['imcome_cat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['marital_stat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['card_type']).drop(columns=['Platinum'])],axis=1)\n",
    "    x_test.drop(columns = ['education','imcome_cat','marital_stat','card_type'],inplace=True)\n",
    "\n",
    "\n",
    "    # Null 처리 1 방식\n",
    "    # ---------------\n",
    "    # x_test.dropna(axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    # Null 처리 2 방식\n",
    "    # ---------------\n",
    "    # x_test.drop(columns = ['sex'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_revol_balance'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_amt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_trans_amt_for_12m'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_cnt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "\n",
    "\n",
    "    # # Null 처리 3 방식\n",
    "    # # ----------------\n",
    "    x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = test_transform(bank_churner_df)\n",
    "\n",
    "y=bank_churner_df['is_churned']\n",
    "X_new=bank_churner_df.drop(['is_churned'], axis=1)\n",
    "\n",
    "X_new = X_new[['age', 'dependent_num', 'tot_product_count', 'months_inact_for_12m',\n",
    "       'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "       'tot_amt_ratio_q4_q1', 'tot_trans_amt_for_12m', 'tot_cnt_ratio_q4_q1']]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "#Normalization\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP LEARNING\n",
    "\n",
    "# Scale the data with StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print('auc_keras: ')\n",
    "# --------------------------------\n",
    "\n",
    "# Calculate metrics\n",
    "# print(f\"Model Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "# print(f\"Model F1-Score: {f1_score(y_test, y_pred, average='weighted') * 100:.2f}%\")\n",
    "# print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "# Calculate accuracies per class\n",
    "# accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "# accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "\n",
    "#pred_proba_proba = model.predict_proba(X_test)[:, 1]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_rf)\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "#get_clf_eval(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compute confusion matrix for the Deep learning model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix. Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    #Plot the confusion matrix.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.grid(False)  # <-- Agregar esta línea para evitar el aviso de deprecación\n",
    "    plt.title(title,fontsize=18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Add labels to the cells of the confusion matrix.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['y=0','y=1'],normalize= True,  title='Confusion matrix')\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 테스트 데이터로 평가\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 전처리\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    x_test['is_churned']=x_test['is_churned'].replace({'Existing Customer':0,'Attrited Customer':1})\n",
    "    \n",
    "    x_test = x_test[['is_churned', 'age', 'dependent_num', 'tot_product_count', 'months_inact_for_12m',\n",
    "        'contact_cnt_for_12m', 'credit_line', 'tot_revol_balance',\n",
    "        'tot_amt_ratio_q4_q1', 'tot_trans_amt_for_12m', 'tot_cnt_ratio_q4_q1']]\n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------\n",
    "    # x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    # x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    # x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "\n",
    "\n",
    "    # 범주형 데이터 One-Hot 인코딩\n",
    "    # --------------------------\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['education']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['imcome_cat']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['marital_stat']).drop(columns=['Unknown'])],axis=1)\n",
    "    # x_test = pd.concat([x_test,pd.get_dummies(x_test['card_type']).drop(columns=['Platinum'])],axis=1)\n",
    "    # x_test.drop(columns = ['education','imcome_cat','marital_stat','card_type'],inplace=True)\n",
    "\n",
    "\n",
    "    # Null 처리 1 방식\n",
    "    # ---------------\n",
    "    # x_test.dropna(axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    # Null 처리 2 방식\n",
    "    # ---------------\n",
    "    # x_test.drop(columns = ['sex'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_revol_balance'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_amt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_trans_amt_for_12m'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_cnt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "\n",
    "\n",
    "    # # Null 처리 3 방식\n",
    "    # # ----------------\n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    return x_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_transform(test_df)\n",
    "\n",
    "y_test=test_df['is_churned']\n",
    "X_test=test_df.drop(['is_churned'], axis=1)\n",
    "\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with different models\n",
    "#entrenamiento con distintos modelos\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#Create a list of tuples with the model name and the classifier instance\n",
    "# Crear una lista de tuplas con el nombre del modelo y la instancia del clasificador\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "\n",
    "model_eval_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "\n",
    "for model_name, classifier in models:\n",
    "    #Fit the model using the training set\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    #Make predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    #Calculate model metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "    cv_accuracy = accuracies.mean()\n",
    "    cv_std = accuracies.std()\n",
    "    accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "    accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "    \n",
    "    #Print model metrics\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    # print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "    # print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "    # print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "\n",
    "    # #Add metrics to the models comparison dictionary\n",
    "    model_eval_comparison[model_name] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std, roc_auc]\n",
    "    # print(classification_report(y_pred, y_test, zero_division=1))\n",
    "    # print(\"-\" * 60)\n",
    "\n",
    "    \n",
    "    get_clf_eval(y_test, y_pred, pred_proba)\n",
    "    # precision_recall_curve_plot(y_test, pred_proba)\n",
    "    # roc_curve_plot(y_test , pred_proba)\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble methods in machine learning involve combining multiple models (often weaker models or base models) to create a stronger,\n",
    "#more robust predictive model. The idea behind ensembling is that by combining the predictions of multiple models, the strengths \n",
    "#of each individual model can compensate for the weaknesses of others, leading to improved overall performance.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='soft')  # Puedes usar 'hard' o 'soft' para el voto\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "accuracies = cross_val_score(estimator=voting_classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "cv_accuracy = accuracies.mean()\n",
    "cv_std = accuracies.std()\n",
    "accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0] )\n",
    "accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    \n",
    "\n",
    "print(\"Model: Voting Classifier\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "model_eval_comparison['Voting Classifier'] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std, roc_auc]\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISSON\n",
    "\n",
    "Model_com_df=pd.DataFrame(model_eval_comparison).T\n",
    "Model_com_df.columns=['Model Accuracy','Model Accuracy-0','Model Accuracy-1','Model F1-Score','CV Accuracy','CV std', 'AUC']\n",
    "Model_com_df=Model_com_df.sort_values(by='AUC',ascending=False)\n",
    "Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Model_com_df = pd.DataFrame(model_eval_comparison).T\n",
    "Model_com_df.columns = ['Model Accuracy', 'Model Accuracy-No', 'Model Accuracy-Yes', 'Model F1-Score', 'CV Accuracy', 'CV std', 'AUC']\n",
    "Model_com_df = Model_com_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "def highlight_below_75(s):\n",
    "    if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "        return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "    else:\n",
    "        return ['color: black'] * len(s)\n",
    "\n",
    "styled_df = Model_com_df.style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV Accuracy']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV Accuracy'])\n",
    "styled_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 고객 이탈 예측 분석\n",
    "LightGBM은 91%의 가장 높은 Attrited Customer Recall과 89%의 정밀도를 가지고 있음\n",
    "고객 이탈을 사전에 방지하기 위해서 LightGBM 모델을 사용하는 것이 적합함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
