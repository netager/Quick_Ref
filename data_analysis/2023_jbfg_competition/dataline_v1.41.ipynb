{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 탐색 과정\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분석을 위한 Library 데이터 준비 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark\n",
    "%watermark -a 'DataLine' -nmv --packages numpy,pandas,sklearn,matplotlib,seaborn,plotly,imblearn,missingno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석을 위한 Library 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl  \n",
    "# import missingno as msno\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scikitplot as skplt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# %matplotlib inline\n",
    "\n",
    "# mpl.rc('font', family='Malgun Gothic')  # 한글 폰트 설정\n",
    "#                                         # 윈도우 폰트 위치 - C:\\Windows\\Fonts\n",
    "# plt.figure(figsize=(10,6))              # 그래프 사이즈 설정\n",
    "# sns.set(font='Malgun Gothic', rc={'axes.unicode_minus':False}, style='darkgrid') # 마이너스 처리\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세트 로딩\n",
    "\"data/bank_churner.csv\"를 판다스 데이터프레임으로 로딩(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "bank_churner_df_org = bank_churner_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 세트 정보 확인\n",
    "- 일부 Feature에 Null 값 존재함을 확인 함\n",
    "- 향후 모델학습시 Null 값 처리에 대한 필요성 확인 함\n",
    "- 모델학습을 의해 Oject 항목을 적절하게 변형할 필요성을 확인 함 - sex, education, marital_stat, imcome_cat, card_type (5개 Features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수치 데이터의 분포값 개략 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치 확인 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'데이터 총건수 = {bank_churner_df.shape[0]}  컬럼별 Null 건수 = {bank_churner_df.isnull().sum()}')\n",
    "\n",
    "# bank_churner_df.isnull().sum() / bank_churner_df.shape[0]\n",
    "print(bank_churner_df.isnull().sum())\n",
    "\n",
    "null_rates = bank_churner_df.isnull().sum() / 8101 * 100\n",
    "null_rates\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def print_category_graphs(df, column, column_desc):\n",
    "    \n",
    "    counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "    exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "    churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "    churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "    \n",
    "    \n",
    "    fig = make_subplots(rows=3, \n",
    "                    cols=2, \n",
    "                    subplot_titles=('【 전체 현황 】', '【 이탈율 】', '【 사분위 】', f'【 {column_desc} 중 전체 현황 】', f'【 {column_desc} 중 유지 현황 】', f'【 {column_desc} 중 이탈 현황 】'), \n",
    "                    # shared_xaxes=True,\n",
    "                    horizontal_spacing=0.1,\n",
    "                    vertical_spacing=0.1,\n",
    "                    specs=[[{\"secondary_y\": True}, {}],\n",
    "                           [{}, {'type':'domain'}],\n",
    "                           [{'type':'domain'}, {'type':'domain'}]]\n",
    "                   )\n",
    "\n",
    "\n",
    "    # 전체 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Bar(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), marker_color=\"red\", offsetgroup=0, name='이탈', \n",
    "                         text=churn_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto'), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    \n",
    "    fig.add_trace(go.Bar(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), marker_color=\"blue\", offsetgroup=0, name='유지', \n",
    "                         texttemplate='%{value:,}', \n",
    "                        #  text=exist_counts.sort_index(), \n",
    "                         hovertemplate = '%{label}: %{value:,}',\n",
    "                         textposition='auto', base=churn_counts.sort_index()), row=1, col=1, secondary_y=False)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', \n",
    "                             line_shape='linear'), row=1, col=1, secondary_y=True)\n",
    "    \n",
    "    fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "    #fig.update_traces(texttemplate='%{value:,}', hovertemplate = '%{label}, %{value}', row=1, col=1)\n",
    "    # fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
    "    \n",
    "\n",
    "    # 이탈율\n",
    "    # ------\n",
    "    fig.add_trace(go.Bar(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"red\", name='이탈율'),\n",
    "                  row=1, col=2)\n",
    "\n",
    "    \n",
    "    # 사분위\n",
    "    # ------\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']!=0][column], marker_color=\"red\", name='이탈'), row=2, col=1)\n",
    "    fig.add_trace(go.Box(x=df[df['is_churned']==0][column], marker_color=\"blue\", name='유지'), row=2, col=1)\n",
    "\n",
    "\n",
    "    # 유지/이탈 현황\n",
    "    # -------------\n",
    "    fig.add_trace(go.Pie(labels=counts.sort_index().index, values=counts.sort_index(), name=f'{column_desc} 분표 현황', title='전체', texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=2, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=2, col=2)\n",
    "\n",
    "  \n",
    "    # 유지 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=exist_counts.sort_index().index, values=exist_counts.sort_index(), name=\"유지\", title='유지',texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=1)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=1)\n",
    "\n",
    "\n",
    "    # 이탈 현황\n",
    "    # ---------\n",
    "    fig.add_trace(go.Pie(labels=churn_counts.sort_index().index, values=churn_counts.sort_index(), name=\"이탈\", title='이탈',texttemplate = \"%{label}: %{value:,} <br>(%{percent})\",\n",
    "                         textposition = \"inside\"), row=3, col=2)\n",
    "    fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", row=3, col=2)\n",
    "\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.5, ax=0, ay=0,\n",
    "                    xref = \"paper\", yref = \"paper\", \n",
    "                    text= \"<b>전체</b>\", \n",
    "                    font_size=20,\n",
    "                  ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.21, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>유지</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    fig.add_annotation(dict(x=0.73, y=0.13, ax=0, ay=0,\n",
    "                        xref = \"paper\", yref = \"paper\", \n",
    "                        text= \"<b>이탈</b>\", \n",
    "                        font_size=20,\n",
    "                      ))\n",
    "\n",
    "    \n",
    "    fig.update_layout(width=1200, \n",
    "                  height=1200, \n",
    "                  showlegend=False,\n",
    "                  title_text=f'『 {column_desc} 』에 따른 분석 그래프',\n",
    "                # barmode='stack'\n",
    "                  hovermode=\"x\",\n",
    "                 )\n",
    "    \n",
    "\n",
    "    fig.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_continuous_graphs(df, column, column_desc):\n",
    "\n",
    "       counts = df[column].value_counts() # 해당 컬럼의 속성별 합계\n",
    "       exist_counts = df[df['is_churned'] == 0][column].value_counts() # 유지 - 해당 컬럼의 속성별 합계\n",
    "       churn_counts = df[df['is_churned'] != 0][column].value_counts() # 이탈 - 해당 컬럼의 속성별 합계\n",
    "       churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "\n",
    "       fig = make_subplots(rows=3, \n",
    "                     cols=2, \n",
    "                     subplot_titles=('전체 건수 분포', '유지/이탈별 사분위', '유지/이탈별 분포'), \n",
    "                     # shared_xaxes=True,\n",
    "                     horizontal_spacing=0.1,\n",
    "                     vertical_spacing=0.1,\n",
    "                     specs=[[{\"secondary_y\": True}, {}],\n",
    "                            [{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
    "                            [{\"secondary_y\": True},{}],\n",
    "                            ]\n",
    "\n",
    "                     )\n",
    "\n",
    "       # 전체\n",
    "       # ----\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column],  marker_color=\"red\"), row=1, col=1, secondary_y=False)\n",
    "       # fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], texttemplate=\"%{x}\", marker_color=\"red\"), row=1, col=1, secondary_y=False)\n",
    "       \n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\"), row=1, col=1, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=1, col=1, secondary_y=True)\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=1, col=1)\n",
    "\n",
    "\n",
    "       \n",
    "       # Box Graph\n",
    "       # ---------\n",
    "       # fig.add_trace(go.Box(x=exist_counts, \n",
    "       #               name='유지'), row=2, col=1)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']==0][column], name='유지', marker_color=\"blue\"), row=1, col=2)\n",
    "       fig.add_trace(go.Box(x=df[df['is_churned']!=0][column], name='이탈', marker_color=\"red\"), row=1, col=2)\n",
    "\n",
    "\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']==0][column], marker_color=\"blue\"), row=2, col=1)\n",
    "       fig.add_trace(go.Histogram(x=df[df['is_churned']!=0][column], marker_color=\"red\"), row=2, col=1)\n",
    "\n",
    "       fig.add_trace(go.Scatter(x=churn_counts.sort_index().index, y=churn_counts.sort_index(), mode='lines+markers', marker_color=\"red\", name='이탈'), row=2, col=2, secondary_y=False)\n",
    "       fig.add_trace(go.Scatter(x=exist_counts.sort_index().index, y=exist_counts.sort_index(), mode='lines+markers', marker_color='blue', name='유지'), row=2, col=2, secondary_y=False)\n",
    "\n",
    "       fig.add_trace(go.Scatter(x=churn_rates.sort_index().index, y=churn_rates.sort_index(), marker_color=\"green\", name='이탈율', line_shape='linear'),\n",
    "                     row=2, col=2, secondary_y=True)\n",
    "\n",
    "\n",
    "       # 이탈률\n",
    "       # ------\n",
    "       # churn_rates = df[df['is_churned'] == 1][column].value_counts() / df[column].value_counts() # 해당 컴럼의 속성별 이탈율    \n",
    "       fig.add_trace(go.Histogram(x=churn_rates.sort_index()), row=3, col=1)\n",
    "\n",
    "       fig.update_yaxes(secondary_y=True, range=[0, 1], row=3, col=1)\n",
    "\n",
    "       fig.update_layout(width=1200, \n",
    "                     height=1200, \n",
    "                     showlegend=False,\n",
    "                     barmode='stack'\n",
    "                     )\n",
    "\n",
    "       fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature별 특징 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_churned : 이탈  여부\n",
    "- 0 : 유지, 1 : 이탈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df[\"is_churned\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['is_churned'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['is_churned'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Pie(labels=['유지', '이탈'], values=bank_churner_df['is_churned'].value_counts().sort_index(), name='이탈별 분포', \n",
    "                    #  texttemplate = \"<b>%{label}:</b> %{value:,}명 <br>(%{percent})\",\n",
    "                     texttemplate = \"%{value:,}명 <br><b>(%{percent})</b>\",\n",
    "                     title='<b>전체<br> 8,101</b>',\n",
    "                         textposition = \"inside\"))\n",
    "fig.update_traces(hole=.4, hoverinfo=\"label+percent+name\", pull=[0,0.1])\n",
    "fig.update_layout(width=500, \n",
    "                  height=500, \n",
    "                  showlegend=True,\n",
    "                  title_text=\"<b>유지/이탈 분포 현황<b>\",\n",
    "                  title_x = 0.5,\n",
    "                  title_y = 0.9,\n",
    "                  title_xanchor = \"center\",\n",
    "                  title_yanchor = \"middle\")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age : 나이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'age', '나이')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 나이에 대해 일부 이상치가 있지만, 대체적으로 유지 고객과 이탈 고객의 분포가 일치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이를 나이대별로 범주화\n",
    "def age_categorize(age):\n",
    "    age = (age // 10) * 10\n",
    "    return age\n",
    "\n",
    "bank_churner_df['age_category'] = bank_churner_df.age.apply(age_categorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'age_category', '나이 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sex : 성별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'sex', '성별')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "남성과 여성의 감소율에는 큰 차이가 없음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dependent_num : 부양가족수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'dependent_num', '부양가족수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 부양가족수별 고객이탈율은 비슷하게 형태를 보이고 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### education : 교육수준\n",
    "- Graduate : 대학원\n",
    "- High School : 고졸\n",
    "- Unknown\n",
    "- Uneducated : 미교육\n",
    "- College : 단과대학\n",
    "- Post-Graduate : 보딩스쿨(재수)\n",
    "- Doctorate :박사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'education', '교육수준')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "박사 학위를 받은 고객의 경우 이탈율이 더 높게 나타나고, 고등학교 및 대학 교육을 받은 고객은 이탈율이 더 낮은 것으로 보이나, \n",
    "박사 학위 고객 수는 가장 낮음\n",
    "박사 학위의 경우 카드사의 혜택을 더 많이 고려하여 이동하는 것으로 보임   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### marital_stat : 결혼상태\n",
    "- Married  : 결혼\n",
    "- Single   : 미혼\n",
    "- Divorced : 이혼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'marital_stat', '결혼상태')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결혼 상태에 따른 고객 감소율의 차이는 없는 것으로 보임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imcome_cat : 수입규모\n",
    "- Less than $40K    2277\n",
    "- $40K - $60K       1151\n",
    "- $60K - $80K        891\n",
    "- $80K - $120K       988\n",
    "- $120K +            473\n",
    "- Unknown            702\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'imcome_cat', '수입규모')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소득 수입규모가 12만 달러 이상인 고객, 4만 달러 미만인 고객의 이탈율이 높게 나타나고 있으면, \n",
    "전체 고객대비 소득 수입규모가 40만 달러 미만인 고객이 차지하는 점유율이 높으므로 이 범주의 고객을 중점적으로 관리할 필요가 \n",
    "있음\n",
    "전반적으로 소득 수입규모별 감소율은 큰 차이가 없음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### card_type : 카드종류\n",
    "- Blue       \n",
    "- Silver     \n",
    "- Gold       \n",
    "- Platinum   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'card_type', '카드종류')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "플래티넘 카드 소유자의 건수는 매우 낮음에도 이탈율은 상당히 높게 나타나고 있음.\n",
    "이는 플래티넘 카드를 사용하는 고객의 만족도가 매우 낮은 것으로 보이며 플래티넘 카드의 혜택을 다른 카드사와 비교하여 \n",
    "부족한 부문을 찾아내어 개선하거나 고객이 만족할만한 혜택을 제공하여 이탈율을 낮출 필요가 있음 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mon_on_book : 은행 거래 기간\n",
    "- 은행 거래 개월 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'mon_on_book', '은행 거래 기간')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은행 거래 기간 변환\n",
    "# 30대, 40대, 50대\n",
    " \n",
    "def calcUseMonth(mon_on_book):\n",
    "    mon_on_book = (mon_on_book // 10) * 10\n",
    "    return mon_on_book\n",
    "\n",
    "\n",
    "bank_churner_df['mon_on_book_category'] = bank_churner_df.mon_on_book.apply(calcUseMonth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'mon_on_book_category', '은행거래기간 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은행거래 기간에 따른 이탈율은 큰 차이가 없음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_product_count : 현재 보유 상품 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_product_count', '현재 보유 상품 개수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이탈하지 않은 고객의 카드 보유 갯수의 중앙값은 4개이고 이탈한 고객의 중앙값은 3개임.\n",
    "- 카드의 개수가 적을수록 이탈율이 높아지므로 4개 이상의 카드를 보유할 수 있도록 노력 필요"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### months_inact_for_12m : 최근 12개월 동안 카드 거래가 없었던 개월 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'months_inact_for_12m', '최근 12개월 동안 카드 거래가 없었던 개월 수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카드 거래 없는 개월수가 4개월 동안 없으면 이탈율이 정점을 찍고 이후 감소하는 경향을 보이고 있음\n",
    "- 이탈한 고객의 평균 기간은 3개월이고 유지하는 고객의 경우 2개월 임\n",
    "- 1개월 이상 카드 거래가 없는 고객은 잠재적으로 이탈할 확률이 높을 것으로 예상되므로 주기적으로 관리할 필요가 있음\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contact_cnt_for_12m : 최근 12개월 동안 연락 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'contact_cnt_for_12m', '최근 12개월 동안 연락 횟수')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 고객 접촉 건수와 이탈율 사이에는 명확한 관계가 있어 보임\n",
    "- 접촉 건수가 많을 수록 이탈율이 높아지는 경향을 보임\n",
    "\n",
    "- 데이터상 12개월동안 접촉건수가 고객이 접촉한 건수인지, 은행에서 접속한 건수 인지를 구분하여 분석할 필요가 있음\n",
    "   - 은행이 고객을 접촉한 건수라면 연체 등으로 잦은 고객 독촉으로 이탈율이 높아질 가능성 있어 보이며 \n",
    "   - 고객이 은행을 접촉한 건수라면 카드 관련 서비스의 불만을 원할하게 해결하지 못해 건수가 증가하고 이로인해 이탈율이 상승.<br>\n",
    "   따라서, 카드관련 부서에서는 대고객 접촉 서비스를 세부적으로 분석할 필요가 있으며 대고객 대응 가이드를 점검해 볼 필요가 있음\n",
    "\n",
    "\n",
    "- 대고객 접촉 채널은 고객 행동을 분석하는데 있어 중요한 항목이므로 세부적으로 관리할 필요가 있으며 정기적으로 분석할 필요가 있음 \n",
    "- 접촉 채널, 접촉 주체, 접촉 내용 구분, 접촉 세부 내용, 문제해결 여부, 접촉에 대한 만족도 등 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### credit_line : 카드 한도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['credit_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'credit_line', '카드 한도')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('카드 이탈 여부별 한도별 분포')\n",
    "sns.histplot(x='credit_line', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcCreditLine(credit_line):\n",
    "    credit_line = (credit_line // 1000) * 1000\n",
    "    return credit_line\n",
    "\n",
    "bank_churner_df['credit_line_category'] = bank_churner_df.credit_line.apply(calcCreditLine)\n",
    "print_category_graphs(bank_churner_df, 'credit_line_category', '카드 한도 범주')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신용한도에 따른 이탈율은 큰 특징은 없음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_revol_balance : 리볼빙 잔액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'tot_revol_balance', '리볼빙 잔액')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('리볼빙 잔액 분포 및 이탈여부 현황')\n",
    "sns.histplot(x='tot_revol_balance', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRevolBal(tot_revol_balance):\n",
    "    tot_revol_balance = (tot_revol_balance // 1000) * 1000\n",
    "    return tot_revol_balance\n",
    "\n",
    "bank_churner_df['tot_revol_balance_category'] = bank_churner_df.tot_revol_balance.apply(calcRevolBal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_revol_balance_category', '리볼빙 잔액 범주')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신용카드 결제대상 잔액이 적을수록 이탈에 대한 비율이 커지므로 약 $1,000 이하 잔액이 있는 고객을 대상으로 마케팅할 필요가 있음  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean_open_to_buy : 평균 사용가능 신용한도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'mean_open_to_buy', '평균 사용가능 신용한도')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('평균 사용가능 신용한도 분포 및 이탈 현황')\n",
    "sns.histplot(x='mean_open_to_buy', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMeanOpenBuy(mean_open_to_buy):\n",
    "    mean_open_to_buy = (mean_open_to_buy // 1000) * 1000\n",
    "    return mean_open_to_buy\n",
    "\n",
    "bank_churner_df['mean_open_to_buy_category'] = bank_churner_df.mean_open_to_buy.apply(calcMeanOpenBuy)\n",
    "\n",
    "# 범주화를 줄일 필요가 있음 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['mean_open_to_buy_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'mean_open_to_buy_category', '평균 사용가능 신용한도 범주')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_amt_ratio_q4_q1 : 1분기 대비 4분기의 거래 금액 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'tot_amt_ratio_q4_q1', '1분기 대비 4분기의 거래 금액 비율')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['tot_amt_ratio_q4_q1'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['tot_amt_ratio_q4_q1'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('1분기 대비 4분기의 거래 금액 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_amt_ratio_q4_q1', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ceil(bank_churner_df.tot_amt_ratio_q4_q1 * 10) / 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주화\n",
    "\n",
    "bank_churner_df['tot_amt_ratio_q4_q1_category'] = np.floor(bank_churner_df[bank_churner_df['tot_amt_ratio_q4_q1'].notnull()]['tot_amt_ratio_q4_q1'] * 10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_amt_ratio_q4_q1_category', '1분기 대비 4분기의 거래 금액 비율 범주')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1분기 대비 4분기의 거래 금액의 변화는 비슷하나 이탈하지 않은 고객의 분포를 보면 이상치가 많음. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_trans_amt_for_12m : 최근 12개월 동안의 거래 금액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'tot_trans_amt_for_12m', '최근 12개월 동안의 거래 금액')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['tot_trans_amt_for_12m'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['tot_trans_amt_for_12m'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('최근 12개월 동안의 거래 금액 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_trans_amt_for_12m', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 금액 범주화 : $1000 단위\n",
    "bank_churner_df['tot_trans_amt_for_12m_category'] = bank_churner_df.tot_trans_amt_for_12m.apply(calcMeanOpenBuy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_trans_amt_for_12m_category', '최근 12개월 동안의 거래 금액 범주')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('최근 12개월 동안의 거래 금액 분포 및 이탈 현황')\n",
    "sns.countplot(x='tot_trans_amt_for_12m_category', data=bank_churner_df, hue='is_churned')\n",
    "\n",
    "# 범주화할 필요가 있음???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.boxplot(data=bank_churner_df, x='tot_trans_amt_for_12m', y='is_churned', orient='h', ax=axs[0])\n",
    "sns.kdeplot(data=bank_churner_df, x='tot_trans_amt_for_12m', hue='is_churned', common_norm=False, ax=axs[1])\n",
    "axs[0].set_ylabel('')\n",
    "axs[1].set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이탈하지 않은 고객의 거래 금액이 더 높은 것으로 나타나고 있으나, 거래 금액별로 이탈율이 다르게 나타나고 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_trans_cnt_for_12m : 최근 12개월 동안의 거래 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'tot_trans_cnt_for_12m', '최근 12개월 동안의 거래 횟수')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['tot_trans_cnt_for_12m'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['tot_trans_cnt_for_12m'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('최근 12개월 동안의 거래 횟수 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_trans_cnt_for_12m', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거래 횟수 범주화 : 10 단위\n",
    "bank_churner_df['tot_trans_cnt_for_12m_category'] = bank_churner_df['tot_trans_cnt_for_12m'] // 10 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_trans_cnt_for_12m_category', '최근 12개월 동안의 거래 횟수 범주')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('최근 12개월 동안의 거래 금액 분포 및 이탈 현황')\n",
    "sns.countplot(x='tot_trans_cnt_for_12m_category', data=bank_churner_df, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['tot_trans_cnt_for_12m_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "거개건수가 많을수록 유지되는 고객이 많으며 최근 12개월 동인 50번 이하의 고개 이탈율이 높아지므로 정기적으로 거래건수를 분석하여 대응할 필요가 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tot_cnt_ratio_q4_q1 : 1분기 대비 4분기의 거래 횟수 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'tot_cnt_ratio_q4_q1', '1분기 대비 4분기의 거래 횟수 비율')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['tot_cnt_ratio_q4_q1'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['tot_cnt_ratio_q4_q1'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('1분기 대비 4분기의 거래 횟수 분포 및 이탈 현황')\n",
    "sns.histplot(x='tot_cnt_ratio_q4_q1', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['tot_cnt_ratio_q4_q1_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주화\n",
    "\n",
    "bank_churner_df['tot_cnt_ratio_q4_q1_category'] = np.floor(bank_churner_df[bank_churner_df['tot_cnt_ratio_q4_q1'].notnull()]['tot_cnt_ratio_q4_q1'] * 10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'tot_cnt_ratio_q4_q1_category', '1분기 대비 4분기의 거래 횟수 비율 범주')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('1분기 대비 4분기의 거래 횟수 분포 및 이탈 현황')\n",
    "sns.countplot(x='tot_cnt_ratio_q4_q1_category', data=bank_churner_df, hue='is_churned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df_cnt_ratio_churn = bank_churner_df[bank_churner_df['is_churned'] == 1][['tot_cnt_ratio_q4_q1_category', 'tot_cnt_ratio_q4_q1']]\n",
    "bank_churner_df_cnt_ratio_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('이탈 고객중 1분기 대비 4분기의 거래 횟수 분포')\n",
    "sns.histplot(x='tot_cnt_ratio_q4_q1', data=bank_churner_df_cnt_ratio_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('이탈 고객중 1분기 대비 4분기의 거래 횟수 분포')\n",
    "sns.countplot(x='tot_cnt_ratio_q4_q1_category', data=bank_churner_df_cnt_ratio_churn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean_util_pct : 평균 한도 소진율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_continuous_graphs(bank_churner_df, 'mean_util_pct', '평균 한도 소진율')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['mean_util_pct'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cnt = bank_churner_df['mean_util_pct'].count().sum()\n",
    "tot_null_cnt = bank_churner_df['mean_util_pct'].isnull().sum()\n",
    "print(f'전체 데이터 건수 = {tot_cnt:,} Null 건수 = {tot_null_cnt:,} 전체 데이터 중 널 비율 =  {round(tot_null_cnt / tot_cnt,2)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('평균 한도 소진율 분포 및 이탈 현황')\n",
    "sns.histplot(x='mean_util_pct', data=bank_churner_df, kde=True, hue='is_churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주화\n",
    "\n",
    "bank_churner_df['mean_util_pct_category'] = np.floor(bank_churner_df[bank_churner_df['mean_util_pct'].notnull()]['mean_util_pct'] * 10)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_category_graphs(bank_churner_df, 'mean_util_pct_category', '평균 한도 소진율 범주')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('평균 한도 소진율 분포 및 이탈 현황')\n",
    "sns.countplot(x='mean_util_pct_category', data=bank_churner_df, hue='is_churned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df['mean_util_pct_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df_mean_util_pct_churn = bank_churner_df[bank_churner_df['is_churned'] == 1][['mean_util_pct_category', 'mean_util_pct']]\n",
    "bank_churner_df_mean_util_pct_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('이탈 고객중 평균 한도 소진율 분포')\n",
    "sns.histplot(x='mean_util_pct', data=bank_churner_df_mean_util_pct_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('이탈 고객중 평균 한도 소진율 분포')\n",
    "sns.countplot(x='mean_util_pct_category', data=bank_churner_df_mean_util_pct_churn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한도 소진율이 낮을수록 이탈율은 올라가는 경향이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "# for i, col in enumerate(bank_churner_df.drop(['is_churned'], axis=1).select_dtypes(include=['int','float']).columns):\n",
    "for i, col in enumerate(['age', 'mon_on_book', 'credit_line', 'tot_revol_balance','mean_open_to_buy','tot_amt_ratio_q4_q1','tot_trans_amt_for_12m','tot_trans_cnt_for_12m','mean_util_pct']):        \n",
    "# for i, col in enumerate(['age', 'mon_on_book', 'credit_line' ]):    \n",
    "    # We exclude the 'y' column and only consider the columns of numerical type.\n",
    "    # Excluimos la columna 'y' y solo consideramos las columnas de tipo numérico.\n",
    "\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    ax = plt.subplot(4, 4, i+1)  # Creating a subplot for each column.\n",
    "    # Creamos una subfigura para cada columna.\n",
    "\n",
    "     # Plotting the histogram for each column\n",
    "    sns.histplot(data=bank_churner_df, x=col, ax=ax, color='red', kde=True)\n",
    "\n",
    "    # Plotting the KDE curve with custom color and linewidth\n",
    "    # Plotting the histogram for each column.\n",
    "    # Graficamos el histograma para cada columna.\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_xlabel(col, fontsize=18)\n",
    "    ax.set_ylabel('Count', fontsize=18)\n",
    "    \n",
    "plt.suptitle('Data distribution of continuous variables',fontsize=18)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, col in enumerate(['age', 'mon_on_book', 'credit_line', tot_revol_balance','mean_open_to_buy','tot_amt_ratio_q4_q1','tot_trans_amt_for_12m','tot_trans_cnt_for_12m','mean_util_pct']):    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다변량 분석"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성별, 카드 종류별 비율\n",
    "- Blue       \n",
    "- Silver     \n",
    "- Gold       \n",
    "- Platinum   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=2, cols=2,subplot_titles=('','<b>Platinum Card Holders','<b>Blue Card Holders<b>','Residuals'),\n",
    "    vertical_spacing=0.09,\n",
    "    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n",
    "           [None                               ,{\"type\": \"pie\"}]            ,                                      \n",
    "          ]\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(values=bank_churner_df.sex.value_counts().values,labels=['<b>여자<b>','<b>남자<b>'],hole=0.3,pull=[0,0.3]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=['Female Platinum Card Holders','Male Platinum Card Holders'],\n",
    "        values=bank_churner_df.query('card_type==\"Platinum\"').sex.value_counts().values,\n",
    "        pull=[0,0.05,0.5],\n",
    "        hole=0.3\n",
    "        \n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Pie(\n",
    "#         labels=['Female Gold Card Holders','Male Blue Card Holders'],\n",
    "#         values=bank_churner_df.query('card_type==\"Gold\"').sex.value_counts().values,\n",
    "#         pull=[0,0.2,0.5],\n",
    "#         hole=0.3\n",
    "#     ),\n",
    "#     row=2, col=1\n",
    "# )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=['Female Silver Card Holders','Male Blue Card Holders'],\n",
    "        values=bank_churner_df.query('card_type==\"Silver\"').sex.value_counts().values,\n",
    "        pull=[0,0.2,0.5],\n",
    "        hole=0.3\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    showlegend=True,\n",
    "    title_text=\"<b>Distribution Of Gender And Different Card Statuses<b>\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 과정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도, 상관도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = bank_churner_df_org.corr() # 상관행렬 표 만들기\n",
    "sns.heatmap(round(corr,1), \n",
    "            annot=True, # 상관계수 표시\n",
    "            fmt='.1f', # 상관계수 소수점 자리\n",
    "            cmap='coolwarm', # 컬러맵 색상 팔레트 \n",
    "            vmax=1.0, # 상관계수 최댓값 \n",
    "            vmin=-1.0, # 상관계수 최솟값\n",
    "            linecolor='white', # 셀 테두리 색상 \n",
    "            linewidths=.05) # 셀 간격 \n",
    "sns.set(rc={'figure.figsize':(20,20)}) # 그래프 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = bank_churner_df_org)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중공선성: 다중공선성은 회귀 모델에서 두 개 이상의 독립변수가 높은 상관관계를 가질 때 발생합니다. 이로 인해 가변 계수의 해석이 어려워지고 모델의 안정성과 신뢰성이 낮아질 수 있습니다.\n",
    "\n",
    "age와 mon_on_book, credit_line과 mean_open_to_buy, tot_trans_cnt_for_12m와 tot_trans_amt_for_12m 사이에도 강한 상관관계가 있음을 관찰\n",
    "이로 인해 mon_on_book, mean_open_to_buy, tot_trans_cnt_for_12m 열을 제거할 예정입니다.\n",
    "\n",
    "컬럼명               Null 건수\n",
    "------------------- ---------  \n",
    "age                         0 - 삭제1\n",
    "sex                       808\n",
    "imcome_cat               1619\n",
    "mon_on_book                 0 - 삭제1 : 삭제\n",
    "credit_line                 0 - 삭제2\n",
    "tot_revol_balance        1521                 - 공선성1 \n",
    "mean_open_to_buy            0 - 삭제2 : 삭제 \n",
    "tot_amt_ratio_q4_q1      2435\n",
    "tot_trans_amt_for_12m    1669 - 삭제3\n",
    "tot_trans_cnt_for_12m    3250 - 삭제3 : 삭제\n",
    "tot_cnt_ratio_q4_q1      1629\n",
    "mean_util_pct            2526                 - 공선성1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화, 표준화, 불필요 컬럼 삭제, 다중공선성 컬럼 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "bank_churner_df_org = bank_churner_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "bank_churner_df_org = bank_churner_df.copy()\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    \n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "    \n",
    "    return x_test\n",
    "\n",
    "bank_churner_df_org = test_transform(bank_churner_df_org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = bank_churner_df_org.corr() # 상관행렬 표 만들기\n",
    "sns.heatmap(round(corr,1), \n",
    "            annot=True, # 상관계수 표시\n",
    "            fmt='.1f', # 상관계수 소수점 자리\n",
    "            cmap='coolwarm', # 컬러맵 색상 팔레트 \n",
    "            vmax=1.0, # 상관계수 최댓값 \n",
    "            vmin=-1.0, # 상관계수 최솟값\n",
    "            linecolor='white', # 셀 테두리 색상 \n",
    "            linewidths=.05) # 셀 간격 \n",
    "sns.set(rc={'figure.figsize':(20,20)}) # 그래프 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = bank_churner_df_org)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(hue = 'is_churned', data = bank_churner_df_org)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그래프를 시각적으로 분석하면 특정 변수에 대한 명확한 클러스터링 패턴을 관찰할 수 있습니다. 패턴이 다음과 같을 때 관심이 갑니다.\n",
    "신용 한도의 대각선 그래프를 보면 은행을 떠나기로 결정한 사람들은 신용 한도가 낮은 사람들입니다.\n",
    "신용한도가 높은 사람들은 은행을 떠나지 않기로 결정합니다(파란색 그래프의 정점). 신용 한도와 다른 변수를 그래프로 표시할 때,\n",
    "흥미로운 클러스터링 패턴을 찾을 수 있습니다.\n",
    "얼핏 보면 고객의 체류 여부를 결정하는 데 다음과 같은 변수가 중요한 영향을 미치는 것으로 보입니다.\n",
    "\n",
    "age, credit_lile, tot_revol_balance, tot_amt_ratio_q4_q1, tot_trans_amt_for_12m, tot_cnt_ratio_q4_q1, mean_util_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Education level\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.countplot(x=\"education\", data=bank_churner_df_org,hue='is_churned',edgecolor='black')\n",
    "plt.xlabel(\"education\",fontsize=18)\n",
    "plt.ylabel(\"Customers\",fontsize=18)\n",
    "plt.title(\"education vs Customer churn\",fontsize=18)\n",
    "legend = plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"], fontsize=14)\n",
    "legend.get_frame().set_facecolor('0.9')\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "#plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"],fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentages of \"Yes\" and \"No\" responses for each education level\n",
    "education_percents = bank_churner_df_org.groupby('education')['is_churned'].value_counts(normalize=True).unstack()\n",
    "education_percents.reset_index(inplace=True)\n",
    "education_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "# Create a DataFrame with the percentages of \"No\" and \"Yes\" responses for each Education_Level value\n",
    "education_percent_table = education_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format the percentages as percentage notation\n",
    "education_percent_table['No_Percentage'] = education_percent_table['No_Percentage'].apply(lambda x: f\"{x:.2%}\")\n",
    "education_percent_table['Yes_Percentage'] = education_percent_table['Yes_Percentage'].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "# Sort the table in descending order based on the \"Yes_Percentage\" value\n",
    "education_percent_table = education_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Print the percentage table without the index column\n",
    "print(education_percent_table[['education', 'No_Percentage', 'Yes_Percentage']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the percentages of \"Yes\" and \"No\" responses for each education level\n",
    "education_percents = bank_churner_df_org.groupby('education')['is_churned'].value_counts(normalize=True).unstack()\n",
    "education_percents.reset_index(inplace=True)\n",
    "education_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "# Create a DataFrame with the percentages of response 0 and 1 for each Education_Level value\n",
    "education_percent_table = education_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format percentages as percentage notation\n",
    "education_percent_table['No_Percentage'] = education_percent_table['No_Percentage'].apply(lambda x: x * 100)\n",
    "education_percent_table['Yes_Percentage'] = education_percent_table['Yes_Percentage'].apply(lambda x: x * 100)\n",
    "\n",
    "# Sort the table in descending order of \"Yes_Percentage\"\n",
    "education_percent_table = education_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Use the \"pastel\" color palette from Seaborn\n",
    "custom_palette = sns.color_palette(\"pastel\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "# Flatten the array of subplots for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create donut charts for each education level in descending order of Yes_Percentage\n",
    "for index, row in education_percent_table.iterrows():\n",
    "    labels = ['No', 'Yes']\n",
    "    sizes = [row['No_Percentage'], row['Yes_Percentage']]\n",
    "    explode = (0.1, 0)  # Explode the first slice (No)\n",
    "\n",
    "    ax = axes[index]\n",
    "    patches, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=custom_palette,\n",
    "                                       autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                       textprops={'fontsize': 18})\n",
    "    for text in texts:\n",
    "        text.set_fontsize(20)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(20)\n",
    "\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle\n",
    "    ax.set_title(row['education'], fontsize=25)\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(education_percent_table), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#교육 수준이 높은 사람들이 은행을 더 자주 떠나는 경향이 있음을 관찰할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marital_Status\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.countplot(x=\"marital_stat\", data=bank_churner_df_org,hue='is_churned',edgecolor='black')\n",
    "plt.xlabel(\"Marital Status\",fontsize=18)\n",
    "plt.ylabel(\"Customers\",fontsize=18)\n",
    "plt.title(\"Marital status vs Customer churn\",fontsize=18)\n",
    "legend = plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"], fontsize=14)\n",
    "legend.get_frame().set_facecolor('0.9')\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "#plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"],fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the percentages of \"Yes\" and \"No\" responses for each Marital status\n",
    "marital_percents = bank_churner_df_org.groupby('marital_stat')['is_churned'].value_counts(normalize=True).unstack()\n",
    "marital_percents.reset_index(inplace=True)\n",
    "marital_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "# Create a DataFrame with the percentages of response 0 and 1 for each Marital_status value\n",
    "marital_percent_table = marital_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format percentages as percentage notation\n",
    "marital_percent_table['No_Percentage'] = marital_percent_table['No_Percentage'].apply(lambda x: x * 100)\n",
    "marital_percent_table['Yes_Percentage'] = marital_percent_table['Yes_Percentage'].apply(lambda x: x * 100)\n",
    "\n",
    "# Sort the table in descending order of \"Yes_Percentage\"\n",
    "marital_percent_table = marital_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Use the \"pastel\" color palette from Seaborn\n",
    "custom_palette = sns.color_palette(\"pastel\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "# Flatten the array of subplots for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create donut charts for each marital status in descending order of Yes_Percentage\n",
    "for index, row in marital_percent_table.iterrows():\n",
    "    labels = ['No', 'Yes']\n",
    "    sizes = [row['No_Percentage'], row['Yes_Percentage']]\n",
    "    explode = (0.1, 0)  # Explode the first slice (No)\n",
    "\n",
    "    ax = axes[index]\n",
    "    patches, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=custom_palette,\n",
    "                                       autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                       textprops={'fontsize': 18})\n",
    "    for text in texts:\n",
    "        text.set_fontsize(20)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(20)\n",
    "\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle\n",
    "    ax.set_title(row['marital_stat'], fontsize=25)\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(marital_percent_table), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income Category\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.countplot(x=\"imcome_cat\", data=bank_churner_df_org, hue='is_churned',edgecolor='black')\n",
    "plt.xlabel(\"Income\",fontsize=18)\n",
    "plt.ylabel(\"Customers\",fontsize=18)\n",
    "plt.title(\"Income vs Customer churn\",fontsize=18)\n",
    "legend = plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"], fontsize=14)\n",
    "legend.get_frame().set_facecolor('0.9')\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "#plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"],fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the percentages of \"Yes\" and \"No\" responses for each Income Category\n",
    "income_percents = bank_churner_df_org.groupby('imcome_cat')['is_churned'].value_counts(normalize=True).unstack()\n",
    "income_percents.reset_index(inplace=True)\n",
    "income_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "income_percent_table = income_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format percentages as percentage notation\n",
    "income_percent_table['No_Percentage'] = income_percent_table['No_Percentage'].apply(lambda x: x * 100)\n",
    "income_percent_table['Yes_Percentage'] = income_percent_table['Yes_Percentage'].apply(lambda x: x * 100)\n",
    "\n",
    "# Sort the table in descending order of \"Yes_Percentage\"\n",
    "income_percent_table = income_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Use the \"pastel\" color palette from Seaborn\n",
    "custom_palette = sns.color_palette(\"pastel\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "# Flatten the array of subplots for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create donut charts for each marital status in descending order of Yes_Percentage\n",
    "for index, row in income_percent_table.iterrows():\n",
    "    labels = ['No', 'Yes']\n",
    "    sizes = [row['No_Percentage'], row['Yes_Percentage']]\n",
    "    explode = (0.1, 0)  # Explode the first slice (No)\n",
    "\n",
    "    ax = axes[index]\n",
    "    patches, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=custom_palette,\n",
    "                                       autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                       textprops={'fontsize': 18})\n",
    "    for text in texts:\n",
    "        text.set_fontsize(20)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(20)\n",
    "\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle\n",
    "    ax.set_title(row['imcome_cat'], fontsize=25)\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(income_percent_table), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Card Category\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.countplot(x=\"card_type\", data=bank_churner_df_org,hue='is_churned',edgecolor='black')\n",
    "plt.xlabel(\"Card\",fontsize=18)\n",
    "plt.ylabel(\"Customers\",fontsize=18)\n",
    "plt.title(\"card vs Customer churn\",fontsize=18)\n",
    "legend = plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"], fontsize=14)\n",
    "legend.get_frame().set_facecolor('0.9')\n",
    "legend.get_frame().set_alpha(0.8)\n",
    "#plt.legend(title=\"Response\", labels=[\"No\", \"Yes\"],fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "card_percents = bank_churner_df_org.groupby('card_type')['is_churned'].value_counts(normalize=True).unstack()\n",
    "card_percents.reset_index(inplace=True)\n",
    "card_percents.fillna(0, inplace=True)  # Fill with 0 for missing values\n",
    "\n",
    "card_percent_table = card_percents.rename(columns={0: 'No_Percentage', 1: 'Yes_Percentage'})\n",
    "\n",
    "# Format percentages as percentage notation\n",
    "card_percent_table['No_Percentage'] = card_percent_table['No_Percentage'].apply(lambda x: x * 100)\n",
    "card_percent_table['Yes_Percentage'] = card_percent_table['Yes_Percentage'].apply(lambda x: x * 100)\n",
    "\n",
    "# Sort the table in descending order of \"Yes_Percentage\"\n",
    "card_percent_table = card_percent_table.sort_values(by='Yes_Percentage', ascending=False)\n",
    "\n",
    "# Use the \"pastel\" color palette from Seaborn\n",
    "custom_palette = sns.color_palette(\"pastel\")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "# Flatten the array of subplots for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create donut charts for each marital status in descending order of Yes_Percentage\n",
    "for index, row in card_percent_table.iterrows():\n",
    "    labels = ['No', 'Yes']\n",
    "    sizes = [row['No_Percentage'], row['Yes_Percentage']]\n",
    "    explode = (0.1, 0)  # Explode the first slice (No)\n",
    "\n",
    "    ax = axes[index]\n",
    "    patches, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=custom_palette,\n",
    "                                       autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                                       textprops={'fontsize': 18})\n",
    "    for text in texts:\n",
    "        text.set_fontsize(20)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(20)\n",
    "\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle\n",
    "    ax.set_title(row['card_type'], fontsize=25)\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(len(card_percent_table), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "플래티넘 카드 소지자는 은행 서비스 이용을 중단하는 경향이 있습니다. 수수료가 너무 높거나 연간 서비스가 부족합니까?\n",
    "#수수료 결제됐나요?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA 분석 결과"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저소득층에 집중: 구매력은 크지 않지만 대부분의 고객은 저소득층입니다. 저소득층을 위한 프로모션을 시행하는 것은 해당 클러스터 그룹의 고객 이탈을 줄이는 좋은 대안이 될 수 있습니다.\n",
    "활동 수준이 낮을 때 조치: 활동 수준이 낮은 고객(트랜잭션 45개 미만)이 조직을 떠날 확률이 더 높다는 것을 확인했습니다. 직원이 활동 수준이 낮은 고객에게 전화를 걸어 그들의 요구 사항에 맞는 새로운 제품을 제안하거나 고객이 우리가 제공하는 서비스에 만족하는지, 개선하기 위해 할 수 있는 것이 있는지 묻는다면 우리는 아마도 무엇에 대해 더 나은 통찰력을 얻을 수 있을 것입니다. 우리는 활동 수준을 높이기 위해 할 수 있습니다.\n",
    "회전 잔액이 적은 사람들에게 신용 한도를 늘리시겠습니까? 우리 모델에 따르면 회전 잔액이 낮은 고객은 조직을 떠날 가능성이 더 높습니다. 어쩌면 해당 고객에게 더 높은 신용 잔액을 구현함으로써 해당 세그먼트 그룹이 조직을 떠날 확률이 낮아질 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------ 여기서부터 시작"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측 실행 - Test Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_clf_eval() 함수 \n",
    "# -------------------\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))   \n",
    "\n",
    "\n",
    "def precision_recall_curve_plot(y_test=None, pred_proba_c1=None):\n",
    "    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n",
    "    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n",
    "    \n",
    "    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n",
    "    plt.figure(figsize=(8,6))\n",
    "    threshold_boundary = thresholds.shape[0]\n",
    "    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n",
    "    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n",
    "    \n",
    "    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    \n",
    "    # x축, y축 label과 legend, 그리고 grid 설정\n",
    "    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def roc_curve_plot(y_test , pred_proba_c1):\n",
    "    # 임곗값에 따른 FPR, TPR 값을 반환 받음. \n",
    "    fprs , tprs , thresholds = roc_curve(y_test ,pred_proba_c1)\n",
    "\n",
    "    # ROC Curve를 plot 곡선으로 그림. \n",
    "    plt.plot(fprs , tprs, label='ROC')\n",
    "    # 가운데 대각선 직선을 그림. \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    # FPR X 축의 Scale을 0.1 단위로 변경, X,Y 축명 설정등   \n",
    "    start, end = plt.xlim()\n",
    "    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel('FPR( 1 - Sensitivity )'); plt.ylabel('TPR( Recall )')\n",
    "    plt.legend()\n",
    "    plt.show()    \n",
    "\n",
    "# roc_curve_plot(y_test, lr_clf.predict_proba(X_test)[:, 1] )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "bank_churner_df_org = bank_churner_df.copy()\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    \n",
    "    \n",
    "    # 다중공선성 컬럼 제거\n",
    "    # -------------------\n",
    "    x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "\n",
    "\n",
    "    # 범주형 데이터 One-Hot 인코딩\n",
    "    # --------------------------\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['education']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['imcome_cat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['marital_stat']).drop(columns=['Unknown'])],axis=1)\n",
    "    x_test = pd.concat([x_test,pd.get_dummies(x_test['card_type']).drop(columns=['Platinum'])],axis=1)\n",
    "    x_test.drop(columns = ['education','imcome_cat','marital_stat','card_type'],inplace=True)\n",
    "\n",
    "\n",
    "    # Null 처리 1 방식\n",
    "    # ---------------\n",
    "    # x_test.dropna(axis=0, inplace=True)\n",
    "\n",
    "    \n",
    "    # Null 처리 2 방식\n",
    "    # ---------------\n",
    "    # x_test.drop(columns = ['sex'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_revol_balance'], inplace=True)\n",
    "    # x_test.drop(columns = ['tot_amt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_trans_amt_for_12m'], inplace=True)        \n",
    "    # x_test.drop(columns = ['tot_cnt_ratio_q4_q1'], inplace=True)        \n",
    "    # x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "\n",
    "\n",
    "    # # Null 처리 3 방식\n",
    "    # # ----------------\n",
    "    x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    return x_test\n",
    "\n",
    "bank_churner_df_org = test_transform(bank_churner_df_org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "# -----\n",
    "# Null 처리 1 방식 : (1768, 30)\n",
    "\n",
    "bank_churner_df_org.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression으로 학습 및 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 데이터 세트 X, 레이블 데이터 세트 y를 추출. \n",
    "# 맨 끝이 Outcome 컬럼으로 레이블 값임. 컬럼 위치 -1을 이용해 추출 \n",
    "X = bank_churner_df_org.drop(['is_churned'],axis=1)\n",
    "y = bank_churner_df_org['is_churned']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=156, stratify=y)\n",
    "\n",
    "# 로지스틱 회귀로 학습,예측 및 평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "pred = lr_clf.predict(X_test)\n",
    "pred_proba = lr_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_test , pred, pred_proba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision recall 곡선 그림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1]\n",
    "precision_recall_curve_plot(y_test, pred_proba_c1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류결정 임곗값을 변경하면서 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "def get_eval_by_threshold(y_test , pred_proba_c1, thresholds):\n",
    "    # thresholds 리스트 객체내의 값을 차례로 iteration하면서 Evaluation 수행.\n",
    "    for custom_threshold in thresholds:\n",
    "        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) \n",
    "        custom_predict = binarizer.transform(pred_proba_c1)\n",
    "        print('임곗값:',custom_threshold)\n",
    "        get_clf_eval(y_test , custom_predict, pred_proba_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3 , 0.33 ,0.36,0.39, 0.42 , 0.45 ,0.48, 0.50]\n",
    "pred_proba = lr_clf.predict_proba(X_test)\n",
    "get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임곗값를 0.48로 설정한 Binarizer 생성\n",
    "binarizer = Binarizer(threshold=0.42)\n",
    "\n",
    "# 위에서 구한 lr_clf의 predict_proba() 예측 확률 array에서 1에 해당하는 컬럼값을 Binarizer변환. \n",
    "pred_th_042 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) \n",
    "\n",
    "get_clf_eval(y_test , pred_th_042, pred_proba[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------  여기까지 한세트임 - 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create our feature matrix and our target variable vector.\n",
    "X=bank_churner_df_org.drop(['is_churned'],axis=1)\n",
    "y=bank_churner_df_org['is_churned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of the most important features to conduct the training\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a list of available models for selection\n",
    "available_models = {\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "    #'SVM': SVC(kernel='linear'),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'LASSO': Lasso(alpha=0.01),  # Agrega LASSO aquí\n",
    "    #'RFE': RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=10)\n",
    "    # Agrega otros modelos aquí si lo deseas\n",
    "}\n",
    "\n",
    "# Choose the desired model for feature selection\n",
    "chosen_model = 'ExtraTrees'  \n",
    "\n",
    "# Create the selected model\n",
    "clf = available_models[chosen_model]\n",
    "\n",
    "#Train the model with the data\n",
    "# Entrenar el modelo con los datos\n",
    "clf = clf.fit(X.values, y)\n",
    "\n",
    "#Obtain feature importances from the model\n",
    "# Obtener importancias de características del modelo\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "#Create a SelectFromModel object with the trained classifier\n",
    "# Crear un objeto SelectFromModel con el clasificador entrenado\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "#Transform the original features to obtain the selected ones\n",
    "# Transformar las características originales para obtener las seleccionadas\n",
    "X_new = model.transform(X.values)\n",
    "\n",
    "# Obtener los índices de las características seleccionadas\n",
    "selected_feature_indices = model.get_support(indices=True)\n",
    "\n",
    "#Get the indices of the selected features\n",
    "# Obtener los nombres de las columnas seleccionadas\n",
    "selected_columns = X.columns[selected_feature_indices]\n",
    "#Print the selected columns\n",
    "# Imprimir las columnas seleccionadas\n",
    "print(\"Selected columns:\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the analysis of the graphs, we had predicted that:\n",
    "#At first glance, the following variables seem to have a significant influence on the determination of whether customers stay or not: Customer_Age, Credit_Limit,\n",
    "#Total_Recovering_Bal, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt, Total_Ct_Chng_Q4_Q1, Avg_Utilization_Ratio\n",
    "#It seems that our intuition was correct.\n",
    "\n",
    "\n",
    "#Con el análisis de las gráficas habíamos predicho que :\n",
    "#A simple vista parecen tener gran peso para la determinación de la permanencia o no las siguientes variables; Customer_Age,Credit_Limit,\n",
    "#Total_Recovering_Bal, Total_Amt_Chng_Q4_Q1, Total_Trans_Amt,Total_Ct_Chng_Q4_Q1,Avg_Utilization_Ratio\n",
    "#Al parecer nuestra intuición fue correcta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Get the indices of all columns in descending order of importance\n",
    "# Obtener los índices de todas las columnas en orden descendente de importancia\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "#Get the names of all columns in the same order\n",
    "# Obtener los nombres de todas las columnas en el mismo orden\n",
    "sorted_columns = X.columns[sorted_indices]\n",
    "\n",
    "#Get the sorted importances\n",
    "# Obtener las importancias ordenadas\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#Create a bar chart to display the importance of all columns in descending order\n",
    "# Crear un gráfico de barras para mostrar la importancia de todas las columnas en orden descendente\n",
    "sns.barplot(x=sorted_importances, y=sorted_columns, palette=['lightgrey' if i not in selected_feature_indices else 'blue' for i in sorted_indices])\n",
    "\n",
    "plt.xlabel(\"Importance\", fontsize=14)\n",
    "plt.ylabel(\"Feattures\", fontsize=14)\n",
    "plt.title(\"Feature Importance\", fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "#Entrenamiento del modelo\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and test sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "#stratify=y: It is used to ensure that the distribution of classes in the training and test sets is similar to the original distribution \n",
    "#of the target variable y. This is particularly useful when dealing with umbalanced classes, as it ensures that both parts of the split have\n",
    "#a similar proportion of each class.\n",
    "\n",
    "#stratify=y: Se utiliza para garantizar\n",
    "#que la distribución de las clases en el conjunto de entrenamiento y prueba sea similar a la distribución original de la variable objetivo y.\n",
    "#Esto es especialmente útil cuando tienes clases desequilibradas, ya que asegura que ambas partes de la división tengan una proporción similar\n",
    "#de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying the size of the training and testing sets\n",
    "#verificamos el tamaño de los set de entrenamiento y testeo\n",
    "print(\"Training X size: \", X_train.shape)\n",
    "print(\"Training y size: \", y_train.shape)\n",
    "print(\"Test X size: \", X_test.shape)\n",
    "print(\"Test y size: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(['No', 'Yes'], y_train.value_counts(), color=['blue', 'orange'])\n",
    "plt.xlabel('Response', fontsize=18)\n",
    "plt.ylabel('Couts', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Target Variable Distribution', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE(Synthetic Minority Oversampling Technique)\n",
    "#SMOTE is a technique for oversampling the minority class. Simply adding duplicate records of the minority class often does not add new\n",
    "#information to the model. In SMOTE, new instances are generated from the existing data. To put it simply, SMOTE examines instances of \n",
    "#the minority class and uses the k-nearest neighbors method to select a randomly close neighbor, and a new synthetic instance is created \n",
    "#in the feature space.\n",
    "\n",
    "#Class imbalance, where one or more classes are significantly less frequent than others, is a common challenge in machine learning.\n",
    "#The presence of minority classes can cause the model to be biased towards the majority classes and have difficulty learning patterns from \n",
    "#the minority classes. This is where techniques like SMOTE (Synthetic Minority Oversampling Technique) can help by generating synthetic\n",
    "#instances to balance the classes and improve the model's performance in predicting minority classes.\n",
    "\n",
    "\n",
    "#SMOTE (Técnica de Sobremuestreo Sintético de la Clase Minoritaria) es una técnica para sobremuestrear la clase minoritaria. \n",
    "#Simplemente agregar registros duplicados de la clase minoritaria a menudo no agrega información nueva al modelo.\n",
    "#En SMOTE, se generan nuevas instancias a partir de los datos existentes. Si lo explicamos en palabras sencillas,\n",
    "#SMOTE examina las instancias de la clase minoritaria y utiliza el método de los k vecinos más cercanos para seleccionar\n",
    "#un vecino cercano al azar, y se crea una nueva instancia sintética aleatoria en el espacio de características.\n",
    "\n",
    "#El desequilibrio de clases, donde una o más clases son significativamente menos frecuentes que otras,\n",
    "#es un desafío común en el aprendizaje automático. La presencia de clases minoritarias puede hacer que el\n",
    "#modelo sea sesgado hacia las clases mayoritarias y que tenga dificultades para aprender patrones de las clases minoritarias.\n",
    "#Es aquí donde técnicas como SMOTE (Técnica de Sobremuestreo Sintético de la Clase Minoritaria) pueden ayudar al generar instancias\n",
    "#sintéticas para equilibrar las clases y mejorar el rendimiento del modelo en la predicción de clases minoritarias.\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train,y_train=sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(['No', 'Yes'], y_train.value_counts(), color=['blue', 'orange'])\n",
    "plt.xlabel('Response', fontsize=18)\n",
    "plt.ylabel('Couts', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Target Variable Distribution', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with different models\n",
    "#entrenamiento con distintos modelos\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#Create a list of tuples with the model name and the classifier instance\n",
    "# Crear una lista de tuplas con el nombre del modelo y la instancia del clasificador\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "                        # Diccionario para almacenar las métricas de comparación de modelos\n",
    "\n",
    "for model_name, classifier in models:\n",
    "    #Fit the model using the training set\n",
    "    # Ajustar el modelo usando el conjunto de entrenamiento\n",
    "    classifier.fit(X_train, y_train)\n",
    "    #Make predictions on the test set\n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    #Calculate model metrics\n",
    "    # Calcular métricas del modelo\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5, scoring=\"recall\")\n",
    "    cv_accuracy = accuracies.mean()\n",
    "    cv_std = accuracies.std()\n",
    "    accuracy_class_0 = accuracy_score(y_pred[y_test == 0], y_test[y_test == 0])\n",
    "    accuracy_class_1 = accuracy_score(y_pred[y_test == 1], y_test[y_test == 1])\n",
    "    #Print model metrics\n",
    "    # Imprimir métricas del modelo\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "    print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "    #Add metrics to the models comparison dictionary\n",
    "    # Agregar métricas al diccionario de comparación de modelos\n",
    "    model_comparison[model_name] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std]\n",
    "    print(classification_report(y_pred, y_test, zero_division=1))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble methods in machine learning involve combining multiple models (often weaker models or base models) to create a stronger,\n",
    "#more robust predictive model. The idea behind ensembling is that by combining the predictions of multiple models, the strengths \n",
    "#of each individual model can compensate for the weaknesses of others, leading to improved overall performance.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='soft')  # Puedes usar 'hard' o 'soft' para el voto\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "accuracies = cross_val_score(estimator=voting_classifier, X=X_train, y=y_train, cv=5, scoring=\"recall\")\n",
    "cv_accuracy = accuracies.mean()\n",
    "cv_std = accuracies.std()\n",
    "accuracy_class_0 = accuracy_score(y_pred[y_test == 0], y_test[y_test == 0])\n",
    "accuracy_class_1 = accuracy_score(y_pred[y_test == 1], y_test[y_test == 1])\n",
    "\n",
    "print(\"Modelo: Voting Classifier\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "model_comparison['Voting Classifier'] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std]\n",
    "print(classification_report(y_pred, y_test, zero_division=1))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models Comparisson\n",
    "# Comparación de modelos\n",
    "for model_name, metrics in model_comparison.items():\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(f\"Model Accuracy: {metrics[0] * 100:.2f}%\")\n",
    "    print(f\"Model F1-Score: {metrics[3] * 100:.2f}%\")\n",
    "    print(f\"Cross Val Accuracy: {metrics[4] * 100:.2f}%\")\n",
    "    print(f\"Cross Val Standard Deviation: {metrics[5] * 100:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "#Compare the performance of the Voting Classifier with the individual models    \n",
    "# Comparar el rendimiento del Voting Classifier con los modelos individuales\n",
    "voting_metrics = model_comparison['Voting Classifier']\n",
    "for model_name in model_comparison:\n",
    "    if model_name != 'Voting Classifier':\n",
    "        individual_metrics = model_comparison[model_name]\n",
    "        print(f\"Comparando con {model_name}:\")\n",
    "        print(f\"Mejora en Accuracy: {voting_metrics[0] - individual_metrics[0]:.2f}\")\n",
    "        print(f\"Mejora en F1-Score: {voting_metrics[3] - individual_metrics[3]:.2f}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL COMPARISSON\n",
    "#COMPARACIÓN DE MODELOS\n",
    "\n",
    "Model_com_df=pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns=['Model Accuracy','Model Accuracy-0','Model Accuracy-1','Model F1-Score','CV Accuracy','CV std']\n",
    "Model_com_df=Model_com_df.sort_values(by='Model F1-Score',ascending=False)\n",
    "Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Model_com_df = pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns = ['Model Accuracy', 'Model Accuracy-No', 'Model Accuracy-Yes', 'Model F1-Score', 'CV Accuracy', 'CV std']\n",
    "Model_com_df = Model_com_df.sort_values(by='Model F1-Score', ascending=False)\n",
    "\n",
    "def highlight_below_75(s):\n",
    "    if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "        return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "    else:\n",
    "        return ['color: black'] * len(s)\n",
    "\n",
    "styled_df = Model_com_df.style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV Accuracy']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV Accuracy'])\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the XGBoost algorithm has the best values for Model Accuracy and F1-Score, making it a viable option to implement the model.\n",
    "#However, the Voting Classifier has parameters very similar to XGBoost but with significantly better CV Accuracy, making it a more robust model.\n",
    "#Ensemble methods in machine learning involve combining multiple models (often weaker models or base models) to create a stronger,\n",
    "#more robust predictive model. The idea behind ensembling is that by combining the predictions of multiple models, the strengths \n",
    "#of each individual model can compensate for the weaknesses of others, leading to improved overall performance.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)),\n",
    "    ('Xg Boost', XGBClassifier())\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(estimators=models, voting='soft')  # Puedes usar 'hard' o 'soft' para el voto\n",
    "\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "f1 = f1_score(y_pred, y_test, average='weighted')\n",
    "accuracies = cross_val_score(estimator=voting_classifier, X=X_train, y=y_train, cv=5, scoring=\"recall\")\n",
    "cv_accuracy = accuracies.mean()\n",
    "cv_std = accuracies.std()\n",
    "accuracy_class_0 = accuracy_score(y_pred[y_test == 0], y_test[y_test == 0])\n",
    "accuracy_class_1 = accuracy_score(y_pred[y_test == 1], y_test[y_test == 1])\n",
    "\n",
    "print(\"Modelo: Voting Classifier\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "\n",
    "model_comparison['Voting Classifier'] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_accuracy, cv_std]\n",
    "print(classification_report(y_pred, y_test, zero_division=1))\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for the best model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix. Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    #Plot the confusion matrix.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.grid(False)\n",
    "    plt.title(title,fontsize=18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Add labels to the cells of the confusion matrix.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['y=0','y=1'],normalize= True,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEP LEARNING\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Scale the data with StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"Model Accuracy: {accuracy_score(y_pred, y_test) * 100:.2f}%\")\n",
    "print(f\"Model F1-Score: {f1_score(y_pred, y_test, average='weighted') * 100:.2f}%\")\n",
    "print(classification_report(y_pred, y_test, zero_division=1))\n",
    "\n",
    "# Calculate accuracies per class\n",
    "accuracy_class_0 = accuracy_score(y_pred[y_test == 0], y_test[y_test == 0])\n",
    "accuracy_class_1 = accuracy_score(y_pred[y_test == 1], y_test[y_test == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix for the Deep learning model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix. Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    #Plot the confusion matrix.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.grid(False)  # <-- Agregar esta línea para evitar el aviso de deprecación\n",
    "    plt.title(title,fontsize=18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    # Add labels to the cells of the confusion matrix.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",fontsize=16,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=15)\n",
    "    plt.xlabel('Predicted label',fontsize=15)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['y=0','y=1'],normalize= True,  title='Confusion matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이탈고객 예측 과정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이탈고객 예측을 위한 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models & cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# results and reports\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dataframe from the classification report\n",
    "def report(y_true, predictions):\n",
    "    report_string = classification_report(y_true, predictions)\n",
    "\n",
    "    # Parse the string and convert it into a dataframe\n",
    "    report_list = [line.split() for line in report_string.split('\\n')[2:-5]]\n",
    "    report_df = pd.DataFrame(report_list, columns=['class', 'precision', 'recall', 'f1-score', 'support'])\n",
    "    report_df.set_index('class', inplace=True)\n",
    "    return report_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 세트 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/BankChurner_kaggle.csv\")\n",
    "bank_churner_df = bank_churner_df.iloc[:, :-2]\n",
    "bank_churner_df = bank_churner_df.drop('CLIENTNUM', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이탈고객 예측 모델 생성을 위한 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank_churner_df.drop(['Attrition_Flag'], axis=1)\n",
    "y = bank_churner_df['Attrition_Flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_dummies = pd.get_dummies(X_train)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_dummies, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델별 학습 및 평가"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf_model, pd.get_dummies(X), y, cv=5)\n",
    "\n",
    "# The 'cv' parameter is the number of folds, 'scores' will contain the accuracy of the model on each fold\n",
    "print(\"Score per fold: \", scores)\n",
    "print(\"Average score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf_model.predict(pd.get_dummies(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = classification_report(y_test, predictions)\n",
    "aa\n",
    "# print(aa.split()) \n",
    "#pd.read(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_list = [classification_report(y_test, predictions).split('\\n')[2:-5]]\n",
    "report_list\n",
    "report_df = pd.DataFrame(report_list)\n",
    "report_df\n",
    "#report_df = pd.DataFrame(report_list, columns=['class', 'precision', 'recall', 'f1-score', 'support'])\n",
    "    # report_df.set_index('class', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report = report(y_test, predictions)\n",
    "rf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "feature_df = pd.DataFrame(list(zip(X_resampled.columns, feature_importance)), \n",
    "                          columns=['Feature_Name', 'Importance'])\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Top 10 Features\n",
    "feature_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = lgb.LGBMClassifier(n_estimators=500, random_state=42, boosting_type='GOSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lgbm_model, pd.get_dummies(X), y, cv=5)\n",
    "\n",
    "# The 'cv' parameter is the number of folds, 'scores' will contain the accuracy of the model on each fold\n",
    "print(\"Score per fold: \", scores)\n",
    "print(\"Average score: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lgbm_model.predict(pd.get_dummies(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_report = report(y_test, predictions)\n",
    "lgbm_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the feature importance\n",
    "feature_importance = lgbm_model.feature_importances_\n",
    "\n",
    "feature_df = pd.DataFrame(list(zip(X_resampled.columns, feature_importance)), \n",
    "                          columns=['Feature_Name', 'Importance'])\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Top 10 Features\n",
    "feature_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = bank_churner_df.drop(['Attrition_Flag'], axis=1)\n",
    "y = bank_churner_df['Attrition_Flag']\n",
    "\n",
    "# scale the data\n",
    "X_num_data = [col for col in X.columns if X[col].dtype != 'object']\n",
    "X_num_data = scaler.fit_transform(X[X_num_data])\n",
    "X_num_data = pd.DataFrame(X_num_data, columns=[col for col in X.columns if X[col].dtype != 'object'])\n",
    "\n",
    "# one hot encode\n",
    "X_dum_data = [col for col in X.columns if X[col].dtype == 'object']\n",
    "X_dum_data = pd.get_dummies(X[X_dum_data])\n",
    "\n",
    "# Combine scaled and one hot encoded data\n",
    "X = pd.merge(X_num_data, X_dum_data, left_index=True, right_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logistic_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_report = report(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "X = bank_churner_df.drop(['Attrition_Flag'], axis=1)\n",
    "y = bank_churner_df['Attrition_Flag'].map({'Existing_Customer': 1, 'Attrited_Customer': 0})\n",
    "\n",
    "# scale the data\n",
    "X_num_data = [col for col in X.columns if X[col].dtype != 'object']\n",
    "X_num_data = scaler.fit_transform(X[X_num_data])\n",
    "X_num_data = pd.DataFrame(X_num_data, columns=[col for col in X.columns if X[col].dtype != 'object'])\n",
    "\n",
    "# one hot encode\n",
    "X_dum_data = [col for col in X.columns if X[col].dtype == 'object']\n",
    "X_dum_data = pd.get_dummies(X[X_dum_data])\n",
    "\n",
    "# Combine scaled and one hot encoded data\n",
    "X = pd.merge(X_num_data, X_dum_data, left_index=True, right_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_resampled.shape[1],)),\n",
    "    keras.layers.Dense(16, activation='sigmoid'),\n",
    "    keras.layers.Dense(4, activation='sigmoid'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.00005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = tf_model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=500, batch_size=128, verbose=0, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = tf_model.predict(X_test)\n",
    "\n",
    "# convert predicted probabilities into binary values\n",
    "predictions = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# convert 2d array to 1d\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "predictions = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.map({1: 'Existing_Customer', 0: 'Attrited_Customer'})\n",
    "y_true = y_test.map({1: 'Existing_Customer', 0: 'Attrited_Customer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_report = report(y_true, predictions)\n",
    "tf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report = pd.concat([lgbm_report, rf_report, logistic_report, tf_report], keys=['LightGBM', \n",
    "                                                                      'Random Forest', \n",
    "                                                                      'Logistic Regression', \n",
    "                                                                      'Tensorflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결론"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기술적 분석\n",
    "거래금액과 거래횟수가 많을수록 이탈 가능성이 낮아짐\n",
    "플리티넘 카드 보유자 수가 적고 회원 탈퇴율이 높아 좀 더 관심을 가져야 할 분야\n",
    "이탈율은 비활성화 된지 4개월이 지나면 최고조에 달하니 이러한 현생이 발생한 이유를 좀 더 세부적으로 분석할 필요가 있음\n",
    "\n",
    "카드의 종류가 많을수록 이탈 가능성이 줄어들며\n",
    "접촉 건수에 따른 이탈율을 좀더 분석할 필요가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 고객 이탈 예측 분석\n",
    "LightGBM은 91%의 가장 높은 Attrited Customer Recall과 89%의 정밀도를 가지고 있음\n",
    "고객 이탈을 사전에 방지하기 위해서 LightGBM 모델을 사용하는 것이 적합함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
