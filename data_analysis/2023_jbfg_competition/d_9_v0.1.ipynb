{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# processing libraries \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "\n",
    "# model libraries \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "bank_churner_df_org = bank_churner_df.copy()\n",
    "\n",
    "def test_transform(x_test):\n",
    "    ''' 전처리 함수 정의'''\n",
    "    \n",
    "    # 불필요 컬럼 제거(고객번호)\n",
    "    # -------------------------\n",
    "    # x_test = x_test.drop('cstno', axis=1)\n",
    "    \n",
    "    \n",
    "    # 성별 변환('F':0, 'M':1)\n",
    "    # -------------------------\n",
    "    #x_test['sex']=x_test['sex'].replace({'F':0,'M':1})\n",
    "    \n",
    "    \n",
    "    # # 다중공선성 컬럼 제거\n",
    "    # # -------------------------\n",
    "    # x_test = x_test.drop('mon_on_book', axis = 1)\n",
    "    # x_test = x_test.drop('mean_open_to_buy', axis = 1)\n",
    "    # x_test = x_test.drop('tot_trans_cnt_for_12m', axis = 1)\n",
    "    \n",
    "    \n",
    "    # Null 처리\n",
    "    x_test.drop(columns = ['mean_util_pct'], inplace=True)\n",
    "    x_test.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    return x_test\n",
    "\n",
    "bank_churner_df_org = test_transform(bank_churner_df_org)\n",
    "df = bank_churner_df_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4).style.set_properties(**{\"background-color\": \"#b2c4cc\",\"color\": \"black\", \"border-color\": \"black\", \"font-size\":\"8pt\", 'width': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().style.set_properties(**{\"background-color\": \"#b2c4cc\",\"color\": \"black\", \"border-color\": \"black\", \"font-size\":\"8pt\", 'width': 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of churn and non-churn \n",
    "counts = df.is_churned.value_counts()\n",
    "perc_churn = (counts[1] / (counts[0] + counts[1])) * 100\n",
    "\n",
    "# no. of duplicates \n",
    "duplicates = len(df[df.duplicated()])\n",
    "\n",
    "# no of missing values\n",
    "missing_values = df.isnull().sum().sum()\n",
    "\n",
    "# Data types in dataset\n",
    "types = df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Churn Rate = %.1f %%\"%(perc_churn))\n",
    "print('Number of Duplicate Entries: %d'%(duplicates))\n",
    "print('Number of Missing Values: %d'%(missing_values))\n",
    "print('Number of Features: %d'%(df.shape[1]))\n",
    "print('Number of Customers: %d'%(df.shape[0]))\n",
    "print('Data Types and Frequency in Dataset:')\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from https://www.kaggle.com/code/winternguyen/churning-customers-98-95-detected#Step-2.-Exploratory-data-analysis\n",
    "\n",
    "heat = df.corr()\n",
    "plt.figure(figsize = [16,8])\n",
    "plt.title(\"Correlation between numerical features\", size = 25, pad = 20, color = '#8cabb6')\n",
    "sns.heatmap(heat, cmap = sns.diverging_palette(20, 220, n = 200), annot = False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Point: 'Avg_open_to_buy' and 'Credit_Limit' are highly correlated.\n",
    "Key Point: 'Total_Trans_Amt' and 'Total_Trans_Ct' are closely correlated.\n",
    "Key Point: 'Total_Revolving_Bal', 'Credit_Limit' and 'Avg_open_to_buy' all seem to be connect to the 'Avg_Utilization_Ratio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make gender and outcome numerical\n",
    "df['sex'] = df['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "# drop client id\n",
    "df = df.drop('cstno', axis = 1)\n",
    "\n",
    "# Code adapted from: https://www.kaggle.com/code/andreshg/churn-prediction-0-99-auc-h2o-sklearn-smote#2.-Feature-Engeneering\n",
    "catcols = df.select_dtypes(exclude = ['int64','float64']).columns\n",
    "intcols = df.select_dtypes(include = ['int64']).columns\n",
    "floatcols = df.select_dtypes(include = ['Float64']).columns\n",
    "\n",
    "# one-hot encoding on categorical columns\n",
    "df = pd.get_dummies(df, columns = catcols)\n",
    "\n",
    "# minmax scaling numeric features \n",
    "for col in df[floatcols]:\n",
    "    df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "    \n",
    "for col in df[intcols]:\n",
    "    df[col] = MinMaxScaler().fit_transform(df[[col]])\n",
    "    \n",
    "print('New Number of Features: %d'%(df.shape[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y\n",
    "X = df.drop('is_churned', axis = 1)\n",
    "y = df['is_churned']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficients Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = df.corr()\n",
    "plt.figure(figsize=[16,8])\n",
    "plt.title(\"Correlation between numerical features\", size = 25, pad = 20, color = '#8cabb6')\n",
    "sns.heatmap(heat,cmap = sns.diverging_palette(20, 220, n = 200), annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://www.kaggle.com/code/winternguyen/churning-customers-98-95-detected#Step-4:-Feature-Selection\n",
    "\n",
    "print(\"Correlation Coefficient of all the Features\")\n",
    "corr = df.corr()\n",
    "corr.sort_values([\"is_churned\"], ascending = False, inplace = True)\n",
    "correlations = corr.is_churned\n",
    "a = correlations[correlations > 0.1]\n",
    "b = correlations[correlations < -0.1]\n",
    "top_corr_features = a.append(b)\n",
    "top_corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://www.kaggle.com/code/andreshg/churn-prediction-0-99-auc-h2o-sklearn-smote#4.-Feature-Selection\n",
    "\n",
    "def plot_importances(model, model_name, features_to_plot, feature_names):\n",
    "    #fit model and performances\n",
    "    model.fit(X,y)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # sort and rank importances\n",
    "    indices = np.argsort(importances)\n",
    "    best_features = np.array(feature_names)[indices][-features_to_plot:]\n",
    "    values = importances[indices][-features_to_plot:]\n",
    "    \n",
    "    # plot a graph\n",
    "    y_ticks = np.arange(0, features_to_plot)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y_ticks, values, color = '#b2c4cc')\n",
    "    ax.set_yticklabels(best_features)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_title(\"%s Feature Importances\"%(model_name))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def best_features(model, features_to_plot, feature_names):\n",
    "    # get list of best features \n",
    "    model.fit(X,y)\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)\n",
    "    best_features = np.array(feature_names)[indices][-features_to_plot:]\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X.columns)\n",
    "\n",
    "model1 = RandomForestClassifier(random_state = 1234)\n",
    "plot_importances(model1, 'Random Forest', 10, feature_names)\n",
    "\n",
    "model2 = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1.0, max_depth = 1, random_state = 0)\n",
    "plot_importances(model2, 'XGBoost', 10, feature_names)\n",
    "\n",
    "model3 = AdaBoostClassifier(n_estimators = 100, learning_rate = 1.0, random_state = 0)\n",
    "plot_importances(model3, 'AdaBoost', 10, feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using selectkbest to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the F-value between label/feature for classification tasks\n",
    "f_selector = SelectKBest(f_classif, k = 10)\n",
    "f_selector.fit_transform(X, y)\n",
    "f_selector_best = f_selector.get_feature_names_out()\n",
    "print(f_selector_best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose best components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_best = list(best_features(model1, 10, feature_names))\n",
    "XG_best = list(best_features(model2, 10, feature_names))\n",
    "ada_best = list(best_features(model3, 10, feature_names))\n",
    "top_corr_features = list(top_corr_features.index[1:])\n",
    "f_selector_best = list(f_selector_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_overall = forest_best + XG_best + ada_best + top_corr_features + f_selector_best\n",
    "\n",
    "# create a dictionary with the number of times features appear \n",
    "from collections import Counter\n",
    "count_best_features = dict(Counter(best_features_overall))\n",
    "\n",
    "# list of the features without any repeatitions\n",
    "features_no_repeats = list(dict.fromkeys(best_features_overall))\n",
    "\n",
    "display(count_best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of features with high counts in the dictionary\n",
    "def get_features(threshold):\n",
    "    # remove features below a certain number of appearances\n",
    "    chosen_features = []\n",
    "    for i in features_no_repeats:\n",
    "        if count_best_features[i] > threshold:\n",
    "            chosen_features.append(i)\n",
    "    return chosen_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = get_features(2)\n",
    "chosen_features.remove('mean_open_to_buy')\n",
    "#chosen_features.remove('Avg_Utilization_Ratio')\n",
    "chosen_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best threshold for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, model_name, X, y, threshold):\n",
    "    # make X the chosen subset\n",
    "    chosen_features = get_features(threshold)\n",
    "    X = X[chosen_features]\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(train_x,train_y)\n",
    "    model.score(test_x, test_y)\n",
    "    pred_test = model.predict(test_x)\n",
    "    \n",
    "    # get metrics\n",
    "    f1 = metrics.f1_score(test_y, pred_test)\n",
    "    test_acc = metrics.accuracy_score(test_y, pred_test)\n",
    "    con = metrics.confusion_matrix(test_y, pred_test)\n",
    "    \n",
    "    print(con,'%s model with %s threshold: %.4f F1-score and %.4f accuracy'%(model_name, threshold, f1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run ranges of possible thresholds\n",
    "for i in range(0,5):\n",
    "   eval_model(model1, 'forest', X, y, i)\n",
    "    \n",
    "for i in range(0,5):\n",
    "   eval_model(model2, 'XGBoost', X, y, i)\n",
    "\n",
    "for i in range(0,5):\n",
    "   eval_model(model3, 'AdaBoost', X, y, i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best threshold is 2 in the top two classfiers but is 1 for the AdaBoost so we are going with 2. The threshold of 2 also removes 'credit_limit' that is highly correlated with 'Avg_Open_To_Buy' adverting that issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "model1.fit(train_x,train_y)\n",
    "model1.score(test_x, test_y)\n",
    "pred_test = model1.predict(test_x)\n",
    "    \n",
    "f1 = metrics.f1_score(test_y, pred_test)\n",
    "test_acc = metrics.accuracy_score(test_y, pred_test)\n",
    "con = metrics.confusion_matrix(test_y, pred_test)\n",
    "    \n",
    "print(con,f1,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reduced dataset\n",
    "chosen_features = get_features(2)\n",
    "#chosen_features.remove('Avg_Open_To_Buy')\n",
    "#chosen_features.remove('Avg_Utilization_Ratio')\n",
    "Xnew = X[chosen_features]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(Xnew, y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "model1.fit(train_x,train_y)\n",
    "model1.score(test_x, test_y)\n",
    "pred_test = model1.predict(test_x)\n",
    "    \n",
    "f1 = metrics.f1_score(test_y, pred_test)\n",
    "test_acc = metrics.accuracy_score(test_y, pred_test)\n",
    "con = metrics.confusion_matrix(test_y, pred_test)\n",
    "    \n",
    "print(con,f1,test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model4 = BaggingClassifier(KNeighborsClassifier(n_neighbors = 7), max_samples = 0.8, max_features = 0.8)\n",
    "\n",
    "eval_model(model4, 'KNN', X, y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model5 = LogisticRegression(random_state=1)\n",
    "eval_model(model5, 'Logistic', X, y, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The two above have very low F1-scores suggesting issues with the model.\n",
    "\n",
    "From the tests run above the random forest is the best performing model so we will hypertune these parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of possible parameters\n",
    "n = [400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "depth = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "rand = [600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1250]\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100, max_depth = 15, random_state = 750)\n",
    "eval_model(forest, 'forest', X, y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_forest(model, model_name, X, y, threshold, n, depth, rand):\n",
    "    # create subset from feature selection\n",
    "    chosen_features = get_features(threshold)\n",
    "    # chosen_features.remove('Avg_Open_To_Buy')\n",
    "    # chosen_features.remove('Avg_Utilization_Ratio')\n",
    "    X = X[chosen_features]\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "    model.fit(train_x,train_y)\n",
    "    model.score(test_x, test_y)\n",
    "    pred_test = model.predict(test_x)\n",
    "    \n",
    "    f1 = metrics.f1_score(test_y, pred_test)\n",
    "    test_acc = metrics.accuracy_score(test_y, pred_test)\n",
    "    con = metrics.confusion_matrix(test_y, pred_test)\n",
    "    \n",
    "    print('Model: %s Threshold: %s F1-Score %.4f Accuracy: %.4f n_estimators: %s depth: %s rand: %s'%(model_name, threshold, f1, test_acc,n,depth,rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for loops for each parameter and carry forward the best one \n",
    "for i in n:\n",
    "   forest = RandomForestClassifier(n_estimators = i, max_depth = 10, random_state = 750)\n",
    "   eval_forest(forest, 'forest', X, y, 2, i, 10, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in depth:\n",
    "   forest = RandomForestClassifier(n_estimators = 850, max_depth = i, random_state = 750)\n",
    "   eval_forest(forest, 'forest', X, y, 2, 850, i, 750) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in rand:\n",
    "   forest = RandomForestClassifier(n_estimators = 850, max_depth = 19, random_state = i)\n",
    "   eval_forest(forest, 'forest', X, y, 2, 850, 19, i) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 850, max_depth = 19, random_state = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = get_features(2)\n",
    "# chosen_features.remove('Avg_Open_To_Buy')\n",
    "# chosen_features.remove('Avg_Utilization_Ratio')\n",
    "X_new = X[chosen_features]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X_new, y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "forest.fit(train_x,train_y)\n",
    "forest.score(test_x, test_y)\n",
    "pred_test = forest.predict(test_x)\n",
    "    \n",
    "f1 = metrics.f1_score(test_y, pred_test)\n",
    "test_acc = metrics.accuracy_score(test_y, pred_test)\n",
    "con = metrics.confusion_matrix(test_y, pred_test)\n",
    "precision = metrics.precision_score(test_y, pred_test)\n",
    "recall = metrics.recall_score(test_y, pred_test)\n",
    "roc = metrics.roc_auc_score(test_y, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score', test_acc)\n",
    "print('Precision', precision)\n",
    "print('Recall', recall)\n",
    "print('F1-Score', f1)\n",
    "print('ROC Score', roc)\n",
    "print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "from sklearn.model_selection import cross_validate \n",
    "\n",
    "cv_results = cross_validate(forest, X_new, y, scoring = ('f1', 'accuracy', 'roc_auc'), cv = 8)\n",
    "sorted(cv_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test_roc_auc'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test_f1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['test_accuracy'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
