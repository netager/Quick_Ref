{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JBFG Data Analysis Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #!pip install watermark\n",
    "# %load_ext watermark\n",
    "# %watermark -a 'DataLine' -nmv --packages numpy,pandas,sklearn,imblearn,tensorflow,plotly,matplotlib,seaborn,missingno,lightgbm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 데이터 및 Null 건수 확인"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode_onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩 처리 \n",
    "# ----------------\n",
    "def encode_onehot(df):\n",
    "    '''\n",
    "        데이터프레임의 object type 컬럼을 원-핫 인코딩하는 함수\n",
    "        \n",
    "        Args:\n",
    "            df (df) : DataFrame\n",
    "        Return:\n",
    "            DataFrame\n",
    "    '''\n",
    "    catcols = df.select_dtypes(exclude = ['int64','float64']).columns\n",
    "    df = pd.get_dummies(df, columns = catcols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 Feature 식별\n",
    "# ----------------\n",
    "def select_feature(df, y_labels, chosen_model):\n",
    "\n",
    "    np.random.seed(42)    \n",
    "    \n",
    "    available_models = {\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'RFE': RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=13),\n",
    "    # 'LGBMC': LGBMClassifier(),\n",
    "    'LGBMC': LGBMClassifier(n_estimators=700, random_state=42, boosting_type='GOSS'),\n",
    "    'LGBMR': LGBMRegressor(),\n",
    "    'Xg Boost':XGBClassifier(booster='gbtree', importance_type='gain', eval_metric='auc'),\n",
    "    }\n",
    "\n",
    "    # Create the selected model\n",
    "    clf = available_models[chosen_model]\n",
    "\n",
    "    clf = clf.fit(df.values, y_labels)                                     # Train\n",
    "\n",
    "    if chosen_model == 'LGBMC' or chosen_model == 'LGBMR': \n",
    "        feature_importances = clf.booster_.feature_importance(importance_type=\"gain\")\n",
    "    else:        \n",
    "        feature_importances = clf.feature_importances_\n",
    "\n",
    "\n",
    "    chosen_model = SelectFromModel(clf, prefit=True)\n",
    "    X_df = chosen_model.transform(df.values) \n",
    "    selected_feature_indices = chosen_model.get_support(indices=True)\n",
    "\n",
    "    selected_columns = df.columns[selected_feature_indices]         # Get the indices of the selected features\n",
    "    \n",
    "    return X_df, selected_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proc_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_smote(X_new, y):\n",
    "    #Model Training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_new, y, test_size=0.25, stratify=y, random_state=0)\n",
    "\n",
    "    sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train, y_train=sm.fit_resample(X_train,y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proc_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def proc_normalization(X_train, X_test):\n",
    "    scaler=StandardScaler()\n",
    "    # scaler = QuantileTransformer()\n",
    "    # scaler = PowerTransformer()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit_predict_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예측 및 평가\n",
    "# -----------\n",
    "def fit_predict_eval(proc_type, drop_no, group_no, model_comparison, X_train, y_train, X_test, y_test):\n",
    "   \n",
    "    # 초기화\n",
    "    # ------\n",
    "    best_roc_auc = 0\n",
    "    \n",
    "    # Define Models\n",
    "    # ------------- \n",
    "    # No: 1 origin\n",
    "    # models = [\n",
    "    #     # ('LogisticRegression', LogisticRegression()),\n",
    "    #     # ('DecisionTree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "    #     # ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "    #     # ('NaiveBayes', GaussianNB()),\n",
    "    #     # ('RandomForest', RandomForestClassifier(n_estimators=700, criterion='entropy', random_state=0)),\n",
    "    #     ('LightGBM', LGBMClassifier(n_estimators=700, random_state=42, boosting_type='GOSS')),\n",
    "    #     ('XgBoost', XGBClassifier(n_estimators=700, random_state=42, eval_metric='auc')),\n",
    "    #     # ('ExtraTrees', ExtraTreesClassifier(n_estimators=700)),\n",
    "    # ]\n",
    "\n",
    "    # No: 2\n",
    "    # models = [\n",
    "    #     ('LightGBM', LGBMClassifier()),\n",
    "    #     ('XgBoost', XGBClassifier()),\n",
    "    # ]\n",
    "\n",
    "    # No: 3\n",
    "    # models = [\n",
    "    #     ('LightGBM', LGBMClassifier(n_estimators=700, random_state=42)),\n",
    "    #     ('XgBoost', XGBClassifier(n_estimators=700, random_state=42,)),\n",
    "    # ]\n",
    "\n",
    "    # No: 4\n",
    "    best = {'learning_rate': 0.1406105325029019, 'max_depth': 106.0, 'min_child_samples': 64.0, 'num_leaves': 41.0, 'subsample': 0.9462293554201169}\n",
    "    models = [\n",
    "        ('LightGBM', LGBMClassifier(n_estimators=700, num_leaves=int(best['num_leaves']),\n",
    "                           max_depth=int(best['max_depth']),\n",
    "                           min_child_samples=int(best['min_child_samples']), \n",
    "                           subsample=round(best['subsample'], 5),\n",
    "                           learning_rate=round(best['learning_rate'], 5))),\n",
    "        ('XgBoost', XGBClassifier(n_estimators=700, random_state=42,)),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Model Fit and Testing\n",
    "    # ---------------------\n",
    "    for model_name, classifier in models:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 학습\n",
    "        # ----            \n",
    "        classifier.fit(X_train, y_train)            # Fit\n",
    "        \n",
    "        # 학습된 모델 저장\n",
    "        # ---------------\n",
    "        # file_name = f'./models/{model_name}.pkl'\n",
    "        # print\n",
    "        # joblib.dump(classifier, file_name)\n",
    "\n",
    "        # 평가\n",
    "        # ---- \n",
    "        y_pred = classifier.predict(X_test)         # Test\n",
    "        pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test , y_pred)\n",
    "        recall = recall_score(y_test , y_pred)\n",
    "        # f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        auces = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=skf, scoring=\"roc_auc\")\n",
    "        cv_auc = auces.mean()\n",
    "        cv_std = auces.std()\n",
    "        \n",
    "        accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "        accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1], )\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        \n",
    "        # Collect Result\n",
    "        # --------------\n",
    "        model_comparison[f'{model_name}_{proc_type}_{drop_no}_{group_no}'] = [accuracy, accuracy_class_0, accuracy_class_1, precision, recall, f1, cv_auc, cv_std, roc_auc]\n",
    "        \n",
    "        \n",
    "        # Best ROC_AUC Value Return\n",
    "        # -------------------------\n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "            \n",
    "        \n",
    "        # Print Log\n",
    "        # ---------    \n",
    "        cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_time = time.time()\n",
    "        delta_time = end_time - start_time\n",
    "        print(f'[모델별] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}_{group_no}], Model Name: {model_name:<18}, BEST AUC: {best_roc_auc:0.6f}, AUC: {roc_auc:0.6f}')\n",
    "\n",
    "    return best_roc_auc\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print_eval_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_result(model_comparison):\n",
    "\n",
    "    # # MODEL COMPARISSON\n",
    "    # Model_com_df=pd.DataFrame(model_comparison).T\n",
    "    # Model_com_df.columns=['Accuracy','Accuracy-0','Accuracy-1', 'Precision', 'Recall', 'F1-Score','CV AUC','CV std', 'AUC']\n",
    "    # Model_com_df=Model_com_df.sort_values(by='AUC',ascending=False)\n",
    "    # # display(Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma'))\n",
    "\n",
    "    Model_com_df = pd.DataFrame(model_comparison).T\n",
    "    Model_com_df.columns = ['Accuracy', 'Accuracy-No', 'Accuracy-Yes', 'Precision', 'Recall', 'F1-Score', 'CV AUC', 'CV std', 'AUC']\n",
    "    Model_com_df = Model_com_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "    def highlight_below_75(s):\n",
    "        if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "            return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "        else:\n",
    "            return ['color: black'] * len(s)\n",
    "\n",
    "    # styled_df = Model_com_df.iloc[:10,:].style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV AUC']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV AUC'])\n",
    "    styled_df = Model_com_df.style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV AUC']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV AUC'])\n",
    "    display(styled_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(df):\n",
    "    \n",
    "    # 데이터 변환\n",
    "    # ------------------- \n",
    "    df = df.drop('cstno', axis=1)\n",
    "    df = df.drop('sex', axis=1)\n",
    "    # after_drop_cnt=len(df)\n",
    "    df['imcome_cat']=df['imcome_cat'].replace({'Less than $40K':40000, '$40K - $60K':50000, '$60K - $80K':70000, '$80K - $120K':100000, '$120K +':120000, 'Unknown':63000})\n",
    "   \n",
    "\n",
    "    # 결측치 처리\n",
    "    # ----------\n",
    "    # df = df.fillna(df.mean(numeric_only=True))\n",
    "    df = df.groupby(['marital_stat']).apply(lambda x: x.fillna(x.mean(numeric_only=True)))\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    # after_drop_cnt=len(df)\n",
    "\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    # ----------------\n",
    "    df = encode_onehot(df)     \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 테스트 내용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 테스트1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform_pre1(df, drop_column, groupby_column):\n",
    "    df = df.drop('cstno', axis=1)\n",
    "\n",
    "    for col_name in drop_column:\n",
    "        df = df.drop(col_name, axis=1)\n",
    "\n",
    "    if 'imcome_cat' not in drop_column:\n",
    "        df['imcome_cat']=df['imcome_cat'].replace({'Less than $40K':40000, '$40K - $60K':50000, '$60K - $80K':70000, '$80K - $120K':100000, '$120K +':120000, 'Unknown':63000})\n",
    "\n",
    "    df = df.groupby(groupby_column).apply(lambda x: x.fillna(x.mean(numeric_only=True)))\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "        \n",
    "    df = encode_onehot(df)  \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def proc_null_groupby_test():\n",
    "    from itertools import combinations\n",
    "\n",
    "    model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "    model_eval_comparison = {}                        \n",
    "\n",
    "\n",
    "    # 전처리 테스트 함수\n",
    "    # -----------------\n",
    "    # def drop_null_column_pre(df, drop_list):\n",
    "    #     for col_name in drop_list:\n",
    "    #         df = df.drop(col_name, axis=1)\n",
    "\n",
    "    #     return df\n",
    "\n",
    "\n",
    "            \n",
    "    # 전처리 테스트 예측\n",
    "    # -----------------\n",
    "\n",
    "    # 데이터 로드 및 고객번호 삭제\n",
    "    fit_df = pd.read_csv(\"./data/bank_churner.csv\") # 학습을 위한 데이터 로드\n",
    "    eval_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "    tot_cnt = fit_df.shape\n",
    "\n",
    "    fit_df_org = fit_df.copy()\n",
    "    eval_df_org = eval_df.copy()\n",
    "    fit_df = fit_df.drop('cstno', axis=1)\n",
    "\n",
    "    fit_df_columns = fit_df.columns\n",
    "    best_auc = 0\n",
    "\n",
    "    # 결측치 및 다중공선성 처리\n",
    "    # -----------------------\n",
    "    result_list = []\n",
    "    drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "    for j in range(1, len(drop_target_columns)+1):\n",
    "        for i in combinations(drop_target_columns, j):\n",
    "            result_list.append(list(i))\n",
    "\n",
    "    # result_list = [['sex'], ['sex', 'age']]\n",
    "\n",
    "    for drop_no, drop_column in enumerate(result_list):\n",
    "        for group_no, groupby_column in enumerate(fit_df_columns):\n",
    "            start_time = time.time()\n",
    "            if groupby_column == 'is_churned' or groupby_column in drop_column:\n",
    "                continue\n",
    "\n",
    "            fit_df = fit_df_org\n",
    "            eval_df = eval_df_org\n",
    "            tot_cnt = fit_df.shape\n",
    "            \n",
    "            # print(f'drop_column: {drop_column}, groupby_column: {groupby_column}')\n",
    "            \n",
    "        # -----------------------------------------------------------------------------------    \n",
    "            # 평가 for Competition\n",
    "            # -----------------------------------------------------------------------------------\n",
    "\n",
    "            # 전처리 단계\n",
    "            # -----------\n",
    "            fit_df = test_transform_pre1(fit_df, drop_column, groupby_column)\n",
    "            eval_df = test_transform_pre1(eval_df, drop_column, groupby_column)\n",
    "            after_drop_cnt = len(fit_df)\n",
    "            \n",
    "            \n",
    "            # 평가를 위한 데이터 분리\n",
    "            # ---------------------\n",
    "            X_train=fit_df.drop(['is_churned'],axis=1)\n",
    "            y_train=fit_df['is_churned']\n",
    "            \n",
    "            X_eval=eval_df.drop(['is_churned'],axis=1)\n",
    "            y_eval=eval_df['is_churned']\n",
    "\n",
    "\n",
    "            # 중요 Feature Column 선택\n",
    "            # -----------------------\n",
    "            X_new, selected_columns = select_feature(X_train, y_train, 'ExtraTrees')\n",
    "            X_eval = X_eval[selected_columns]\n",
    "\n",
    "\n",
    "            # Train and Test 데이터 생성 및 가공\n",
    "            # ---------------------------------\n",
    "            X_train, y_train, X_test_temp, y_test_temp = proc_smote(X_new, y_train)\n",
    "            after_smote_cnt = X_train.shape\n",
    "\n",
    "            # Evaluation 데이터 생성 및 가공\n",
    "            # ---------------------------------\n",
    "            X_train, X_eval = proc_normalization(X_train, X_eval.values)   \n",
    "\n",
    "\n",
    "            # 최종 평가\n",
    "            # --------\n",
    "            proc_type='E'\n",
    "            # eval_auc = fit_predict(proc_type, drop_no, model_eval_comparison, X_train_for_evaluation, y_train_for_evaluation, X_eval, y_eval)\n",
    "            eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "            \n",
    "            if eval_auc > best_auc:\n",
    "                best_type = f'{proc_type}_{drop_no}_{group_no}'\n",
    "                best_auc = eval_auc\n",
    "                \n",
    "\n",
    "\n",
    "            # 최종 평가 로그 출력\n",
    "            # ------------------\n",
    "            cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end_time = time.time()\n",
    "            delta_time = end_time - start_time\n",
    "            # print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, AUC: {test_auc:0.6f}, 처리 건수: {len(eval_df)}, 최종 평가 건수: {len(X_eval)}')\n",
    "            print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [G{proc_type}_{drop_no}_{group_no}], best-type: [{best_type}], Best-AUC: {best_auc:0.6f}, AUC: {eval_auc:0.6f}, tot_cnt: {tot_cnt}, after_drop_cnt: {after_drop_cnt}, after_smote_cnt: {after_smote_cnt}, groupby_column: {groupby_column}, drop_column: {drop_column}')\n",
    "\n",
    "            # print_eval_result(model_eval_comparison)\n",
    "\n",
    "\n",
    "# 테스트시 아래의 주석 풀고 실행\n",
    "# ----------------------------\n",
    "# proc_null_groupby_test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 테스트2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_null_drop_test():\n",
    "    from itertools import combinations\n",
    "\n",
    "    model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "    model_eval_comparison = {}                        \n",
    "\n",
    "    def test_transform_pre2(df, drop_list):\n",
    "        \n",
    "        # 데이터 변환\n",
    "        # ---------- \n",
    "        df = df.drop('cstno', axis=1)\n",
    "\n",
    "\n",
    "        # 결측치 처리\n",
    "        # -----------\n",
    "        for col_name in drop_list:\n",
    "            df = df.drop(col_name, axis=1)\n",
    "\n",
    "        if 'imcome_cat' not in drop_list:\n",
    "            df['imcome_cat']=df['imcome_cat'].replace({'Less than $40K':40000, '$40K - $60K':50000, '$60K - $80K':70000, '$80K - $120K':100000, '$120K +':120000, 'Unknown':63000})\n",
    "\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "            \n",
    "\n",
    "        # One-Hot Encoding\n",
    "        # ----------------\n",
    "        df = encode_onehot(df)  \n",
    "    \n",
    "        return df\n",
    "\n",
    "            \n",
    "    # -----------\n",
    "    # 예측\n",
    "    # -----------\n",
    "\n",
    "    # 데이터 로드 및 고객번호 삭제\n",
    "    fit_df = pd.read_csv(\"./data/bank_churner.csv\") # 학습을 위한 데이터 로드\n",
    "    eval_df = pd.read_csv(\"./data/test_churner.csv\")\n",
    "    tot_cnt = fit_df.shape\n",
    "\n",
    "    fit_df_org = fit_df.copy()\n",
    "    eval_df_org = eval_df.copy()\n",
    "\n",
    "    best_auc = 0\n",
    "\n",
    "    # Null 처리\n",
    "    result_list = []\n",
    "    drop_target_columns = ['sex','imcome_cat', 'tot_amt_ratio_q4_q1', 'mean_util_pct', 'tot_trans_cnt_for_12m','age','mean_open_to_buy','tot_trans_amt_for_12m']\n",
    "    for j in range(1, len(drop_target_columns)+1):\n",
    "        for i in combinations(drop_target_columns, j):\n",
    "            result_list.append(list(i))\n",
    "\n",
    "    # result_list = [['sex'], ['sex', 'age', 'imcome_cat']]\n",
    "\n",
    "    for drop_no, drop_column in enumerate(result_list):\n",
    "        start_time = time.time()\n",
    "\n",
    "        fit_df = fit_df_org\n",
    "        eval_df = eval_df_org\n",
    "        \n",
    "        # -----------------------------------------------------------------------------------    \n",
    "        # 평가 for Competition\n",
    "        # -----------------------------------------------------------------------------------\n",
    "\n",
    "        # 전처리 단계\n",
    "        # -----------\n",
    "        fit_df = test_transform_pre2(fit_df, drop_column)\n",
    "        eval_df = test_transform_pre2(eval_df, drop_column)\n",
    "        after_drop_cnt = len(fit_df)\n",
    "        \n",
    "        \n",
    "        # 평가를 위한 데이터 분리\n",
    "        # ---------------------\n",
    "        X_train=fit_df.drop(['is_churned'],axis=1)\n",
    "        y_train=fit_df['is_churned']\n",
    "        X_train_cnt = X_train.shape\n",
    "        \n",
    "        X_eval=eval_df.drop(['is_churned'],axis=1)\n",
    "        y_eval=eval_df['is_churned']\n",
    "\n",
    "\n",
    "        # 중요 Feature Column 선택\n",
    "        # -----------------------\n",
    "        X_new, selected_columns = select_feature(X_train, y_train, 'ExtraTrees')\n",
    "        X_eval = X_eval[selected_columns]\n",
    "\n",
    "\n",
    "        # Train and Test 데이터 생성 및 가공\n",
    "        # ---------------------------------\n",
    "        X_train, y_train, X_test_temp, y_test_temp = proc_smote(X_new, y_train)\n",
    "        after_smote_cnt = X_train.shape\n",
    "\n",
    "        # Evaluation 데이터 생성 및 가공\n",
    "        # ---------------------------------\n",
    "        X_train, X_eval = proc_normalization(X_train, X_eval.values)   \n",
    "\n",
    "\n",
    "        # 최종 평가\n",
    "        # --------\n",
    "        proc_type='E'\n",
    "        group_no=1\n",
    "        # eval_auc = fit_predict(proc_type, drop_no, model_eval_comparison, X_train_for_evaluation, y_train_for_evaluation, X_eval, y_eval)\n",
    "        eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "        \n",
    "        if eval_auc > best_auc:\n",
    "            best_type = f'{proc_type}_{drop_no}_{group_no}'\n",
    "            best_auc = eval_auc\n",
    "            \n",
    "\n",
    "\n",
    "        # 최종 평가 로그 출력\n",
    "        # ------------------\n",
    "        cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_time = time.time()\n",
    "        delta_time = end_time - start_time\n",
    "        # print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, AUC: {test_auc:0.6f}, 처리 건수: {len(eval_df)}, 최종 평가 건수: {len(X_eval)}')\n",
    "        print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}_{group_no}], best-type: [{best_type}], Best-AUC: {best_auc:0.6f}, AUC: {eval_auc:0.6f}, tot_cnt: {tot_cnt}, after_drop_cnt: {after_drop_cnt}, after_smote_cnt: {after_smote_cnt}, X_train_cnt: {X_train_cnt}, drop_column: {drop_column}')\n",
    "\n",
    "\n",
    "        # print_eval_result(model_eval_comparison)\n",
    "\n",
    "\n",
    "# 테스트시 아래의 주석 풀고 실행\n",
    "# ----------------------------\n",
    "# proc_null_drop_test()        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 튜닝 단계"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 및 Test 단계"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측 및 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델별] 2023-09-19 13:12:42, 0:00:03, [T_1_1], Model Name: LightGBM          , BEST AUC: 0.974129, AUC: 0.974129\n",
      "[모델별] 2023-09-19 13:12:52, 0:00:09, [T_1_1], Model Name: XgBoost           , BEST AUC: 0.974129, AUC: 0.969663\n",
      "[테스트] 2023-09-19 13:12:52, 0:00:13, [GT_1_1], best-type: [T_1_1], Best-AUC: 0.974129, AUC: 0.974129, tot_cnt: 8101, after_drop_cnt: 8101, after_smote_cnt: 10200, groupby_column: None, drop_column: None\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "# ----------\n",
    "ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "tot_cnt = len(ml_churner_df)\n",
    "\n",
    "# 결과 저장소 초기화\n",
    "# -----------------\n",
    "model_test_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "# model_eval_comparison = {}                        \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "ml_churner_df = test_transform(ml_churner_df)\n",
    "after_drop_cnt = len(ml_churner_df)\n",
    "\n",
    "\n",
    "# ML 데이터 분리\n",
    "# --------------\n",
    "X_Features = ml_churner_df.drop(['is_churned'],axis=1)\n",
    "y_labels = ml_churner_df['is_churned']\n",
    "\n",
    "\n",
    "# 중요 Feature Column 선택\n",
    "# -----------------------\n",
    "# X_new, selected_columns = select_feature(X, y, 'Xg Boost')\n",
    "X_Features_new, selected_columns = select_feature(X_Features, y_labels, 'ExtraTrees')\n",
    "\n",
    "\n",
    "# Train and Test 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, y_train, X_test, y_test = proc_smote(X_Features_new, y_labels)\n",
    "# X_train_for_normalization = X_train.copy()\n",
    "after_smote_cnt = len(X_train)\n",
    "\n",
    "\n",
    "# Normalization\n",
    "# -------------\n",
    "X_train, X_test = proc_normalization(X_train, X_test)    \n",
    "\n",
    "\n",
    "# Pridict 및 Test 평가\n",
    "# --------------------\n",
    "proc_type='T'\n",
    "drop_no = 1\n",
    "group_no = 1\n",
    "best_auc = 0\n",
    "test_auc = fit_predict_eval(proc_type, drop_no, group_no, model_test_comparison, X_train, y_train, X_test, y_test)\n",
    "# test_auc = fit_predict_eval_tunning(proc_type, drop_no, group_no, model_test_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "if test_auc > best_auc:\n",
    "    best_type = f'{proc_type}_{drop_no}_{group_no}'\n",
    "    best_auc = test_auc\n",
    "\n",
    "\n",
    "# 예측 및 테스트 로그 출력\n",
    "# ----------------------\n",
    "cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_time = time.time()\n",
    "delta_time = end_time - start_time\n",
    "groupby_column = None\n",
    "drop_column = None\n",
    "# print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], AUC: {test_auc:0.6f}, tot_cnt: {tot_cnt:<6}, after_drop_cnt : {after_drop_cnt:<6}, after_smote_cnt: {after_smote_cnt:<6}, X_train:{X_train.shape}, y_train:{y_train.shape}, X_test:{X_test.shape}, y_test:{y_test.shape}')\n",
    "print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [G{proc_type}_{drop_no}_{group_no}], best-type: [{best_type}], Best-AUC: {best_auc:0.6f}, AUC: {test_auc:0.6f}, tot_cnt: {tot_cnt}, after_drop_cnt: {after_drop_cnt}, after_smote_cnt: {after_smote_cnt}, groupby_column: {groupby_column}, drop_column: {drop_column}')\n",
    "\n",
    "\n",
    "# print_eval_result(model_comparison)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_80b60_row0_col0, #T_80b60_row0_col1, #T_80b60_row0_col2, #T_80b60_row0_col3, #T_80b60_row0_col4, #T_80b60_row0_col5, #T_80b60_row0_col6 {\n",
       "  background-color: yellow;\n",
       "  color: black;\n",
       "}\n",
       "#T_80b60_row0_col8, #T_80b60_row1_col7 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_80b60_row1_col0, #T_80b60_row1_col1, #T_80b60_row1_col2, #T_80b60_row1_col3, #T_80b60_row1_col4, #T_80b60_row1_col5, #T_80b60_row1_col6 {\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_80b60\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_80b60_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_80b60_level0_col1\" class=\"col_heading level0 col1\" >Accuracy-No</th>\n",
       "      <th id=\"T_80b60_level0_col2\" class=\"col_heading level0 col2\" >Accuracy-Yes</th>\n",
       "      <th id=\"T_80b60_level0_col3\" class=\"col_heading level0 col3\" >Precision</th>\n",
       "      <th id=\"T_80b60_level0_col4\" class=\"col_heading level0 col4\" >Recall</th>\n",
       "      <th id=\"T_80b60_level0_col5\" class=\"col_heading level0 col5\" >F1-Score</th>\n",
       "      <th id=\"T_80b60_level0_col6\" class=\"col_heading level0 col6\" >CV AUC</th>\n",
       "      <th id=\"T_80b60_level0_col7\" class=\"col_heading level0 col7\" >CV std</th>\n",
       "      <th id=\"T_80b60_level0_col8\" class=\"col_heading level0 col8\" >AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_80b60_level0_row0\" class=\"row_heading level0 row0\" >LightGBM_T_1_1</th>\n",
       "      <td id=\"T_80b60_row0_col0\" class=\"data row0 col0\" >94.37%</td>\n",
       "      <td id=\"T_80b60_row0_col1\" class=\"data row0 col1\" >97.77%</td>\n",
       "      <td id=\"T_80b60_row0_col2\" class=\"data row0 col2\" >76.62%</td>\n",
       "      <td id=\"T_80b60_row0_col3\" class=\"data row0 col3\" >86.76%</td>\n",
       "      <td id=\"T_80b60_row0_col4\" class=\"data row0 col4\" >76.62%</td>\n",
       "      <td id=\"T_80b60_row0_col5\" class=\"data row0 col5\" >81.37%</td>\n",
       "      <td id=\"T_80b60_row0_col6\" class=\"data row0 col6\" >99.64%</td>\n",
       "      <td id=\"T_80b60_row0_col7\" class=\"data row0 col7\" >0.000567</td>\n",
       "      <td id=\"T_80b60_row0_col8\" class=\"data row0 col8\" >0.974129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_80b60_level0_row1\" class=\"row_heading level0 row1\" >XgBoost_T_1_1</th>\n",
       "      <td id=\"T_80b60_row1_col0\" class=\"data row1 col0\" >93.78%</td>\n",
       "      <td id=\"T_80b60_row1_col1\" class=\"data row1 col1\" >97.30%</td>\n",
       "      <td id=\"T_80b60_row1_col2\" class=\"data row1 col2\" >75.38%</td>\n",
       "      <td id=\"T_80b60_row1_col3\" class=\"data row1 col3\" >84.19%</td>\n",
       "      <td id=\"T_80b60_row1_col4\" class=\"data row1 col4\" >75.38%</td>\n",
       "      <td id=\"T_80b60_row1_col5\" class=\"data row1 col5\" >79.55%</td>\n",
       "      <td id=\"T_80b60_row1_col6\" class=\"data row1 col6\" >99.56%</td>\n",
       "      <td id=\"T_80b60_row1_col7\" class=\"data row1 col7\" >0.000744</td>\n",
       "      <td id=\"T_80b60_row1_col8\" class=\"data row1 col8\" >0.969663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17d47147a00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_eval_result(model_test_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 단계 ~ 평가자가 Competition 평가를 위해 사용 하는 단계"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"./data/test_churner.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "# eval_df = pd.read_csv(\"./data/test_churner_kaggle_all.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "\n",
    "fit_df = pd.read_csv(\"./data/bank_churner.csv\") # 학습을 위한 데이터 로드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측 및 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델별] 2023-09-19 13:12:56, 0:00:03, [E_1_1], Model Name: LightGBM          , BEST AUC: 0.990255, AUC: 0.990255\n",
      "[모델별] 2023-09-19 13:13:06, 0:00:09, [E_1_1], Model Name: XgBoost           , BEST AUC: 0.990255, AUC: 0.988054\n",
      "[평  가] 2023-09-19 13:13:06, 0:00:14, [E_1], best_auc: 0.990255, fit_tot_cnt: (8101, 21), eval_tot_cnt: (2026, 21), fit_drop_cnt : (8101, 31), eval_drop_cnt : (2026, 31), X_train: (10200, 13), y_train: (10200,), X_eval: (2026, 13), y_eval:(2026,)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------    \n",
    "# 평가 for Competition\n",
    "# -----------------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "model_eval_comparison = {}                        \n",
    "fit_tot_cnt = fit_df.shape\n",
    "eval_tot_cnt = eval_df.shape\n",
    "\n",
    "\n",
    "# 전처리 단계\n",
    "# -----------\n",
    "fit_df = test_transform(fit_df)\n",
    "eval_df = test_transform(eval_df)\n",
    "fit_drop_cnt = fit_df.shape\n",
    "eval_drop_cnt = eval_df.shape\n",
    "\n",
    "    \n",
    "# 평가를 위한 데이터 분리\n",
    "# ---------------------\n",
    "X_Features=fit_df.drop(['is_churned'],axis=1)\n",
    "y_labels=fit_df['is_churned']\n",
    "\n",
    "X_eval=eval_df.drop(['is_churned'],axis=1)\n",
    "y_eval=eval_df['is_churned']\n",
    "\n",
    "\n",
    "# 중요 Feature Column 선택\n",
    "# -----------------------\n",
    "X_Features_new, selected_columns = select_feature(X_Features, y_labels, 'ExtraTrees')\n",
    "# X_new, selected_columns = select_feature(X_train, y_train, 'LGBMC')\n",
    "X_eval = X_eval[selected_columns]\n",
    "\n",
    "\n",
    "# Train and Test 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, y_train, X_test_temp, y_test_temp = proc_smote(X_Features_new, y_labels)\n",
    "\n",
    "\n",
    "# Evaluation 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, X_eval = proc_normalization(X_train, X_eval.values)   \n",
    "\n",
    "\n",
    "# 최종 평가\n",
    "# --------\n",
    "proc_type='E'\n",
    "drop_no = 1\n",
    "group_no =1\n",
    "\n",
    "\n",
    "# eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "if eval_auc > best_auc:\n",
    "    best_type = f'{proc_type}_{drop_no}_{group_no}'\n",
    "    best_auc = eval_auc\n",
    "\n",
    "\n",
    "# 최종 평가 로그 출력\n",
    "# ------------------\n",
    "cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_time = time.time()\n",
    "delta_time = end_time - start_time\n",
    "print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], best_auc: {best_auc:0.6f}, fit_tot_cnt: {fit_tot_cnt}, eval_tot_cnt: {eval_tot_cnt}, fit_drop_cnt : {fit_drop_cnt}, eval_drop_cnt : {eval_drop_cnt}, X_train: {X_train.shape}, y_train: {y_train.shape}, X_eval: {X_eval.shape}, y_eval:{y_eval.shape}')\n",
    "\n",
    "\n",
    "# print_eval_result(model_eval_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_46ccb_row0_col0, #T_46ccb_row0_col1, #T_46ccb_row0_col2, #T_46ccb_row0_col3, #T_46ccb_row0_col4, #T_46ccb_row0_col5, #T_46ccb_row0_col6, #T_46ccb_row1_col2, #T_46ccb_row1_col4 {\n",
       "  background-color: yellow;\n",
       "  color: black;\n",
       "}\n",
       "#T_46ccb_row0_col8, #T_46ccb_row1_col7 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_46ccb_row1_col0, #T_46ccb_row1_col1, #T_46ccb_row1_col3, #T_46ccb_row1_col5, #T_46ccb_row1_col6 {\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_46ccb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_46ccb_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_46ccb_level0_col1\" class=\"col_heading level0 col1\" >Accuracy-No</th>\n",
       "      <th id=\"T_46ccb_level0_col2\" class=\"col_heading level0 col2\" >Accuracy-Yes</th>\n",
       "      <th id=\"T_46ccb_level0_col3\" class=\"col_heading level0 col3\" >Precision</th>\n",
       "      <th id=\"T_46ccb_level0_col4\" class=\"col_heading level0 col4\" >Recall</th>\n",
       "      <th id=\"T_46ccb_level0_col5\" class=\"col_heading level0 col5\" >F1-Score</th>\n",
       "      <th id=\"T_46ccb_level0_col6\" class=\"col_heading level0 col6\" >CV AUC</th>\n",
       "      <th id=\"T_46ccb_level0_col7\" class=\"col_heading level0 col7\" >CV std</th>\n",
       "      <th id=\"T_46ccb_level0_col8\" class=\"col_heading level0 col8\" >AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_46ccb_level0_row0\" class=\"row_heading level0 row0\" >LightGBM_E_1_1</th>\n",
       "      <td id=\"T_46ccb_row0_col0\" class=\"data row0 col0\" >96.15%</td>\n",
       "      <td id=\"T_46ccb_row0_col1\" class=\"data row0 col1\" >97.59%</td>\n",
       "      <td id=\"T_46ccb_row0_col2\" class=\"data row0 col2\" >88.69%</td>\n",
       "      <td id=\"T_46ccb_row0_col3\" class=\"data row0 col3\" >87.61%</td>\n",
       "      <td id=\"T_46ccb_row0_col4\" class=\"data row0 col4\" >88.69%</td>\n",
       "      <td id=\"T_46ccb_row0_col5\" class=\"data row0 col5\" >88.15%</td>\n",
       "      <td id=\"T_46ccb_row0_col6\" class=\"data row0 col6\" >99.64%</td>\n",
       "      <td id=\"T_46ccb_row0_col7\" class=\"data row0 col7\" >0.000567</td>\n",
       "      <td id=\"T_46ccb_row0_col8\" class=\"data row0 col8\" >0.990255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_46ccb_level0_row1\" class=\"row_heading level0 row1\" >XgBoost_E_1_1</th>\n",
       "      <td id=\"T_46ccb_row1_col0\" class=\"data row1 col0\" >95.90%</td>\n",
       "      <td id=\"T_46ccb_row1_col1\" class=\"data row1 col1\" >97.29%</td>\n",
       "      <td id=\"T_46ccb_row1_col2\" class=\"data row1 col2\" >88.69%</td>\n",
       "      <td id=\"T_46ccb_row1_col3\" class=\"data row1 col3\" >86.31%</td>\n",
       "      <td id=\"T_46ccb_row1_col4\" class=\"data row1 col4\" >88.69%</td>\n",
       "      <td id=\"T_46ccb_row1_col5\" class=\"data row1 col5\" >87.48%</td>\n",
       "      <td id=\"T_46ccb_row1_col6\" class=\"data row1 col6\" >99.56%</td>\n",
       "      <td id=\"T_46ccb_row1_col7\" class=\"data row1 col7\" >0.000744</td>\n",
       "      <td id=\"T_46ccb_row1_col8\" class=\"data row1 col8\" >0.988054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17d4c3dc460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_eval_result(model_eval_comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"./data/test_churner.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "# eval_df = pd.read_csv(\"./data/test_churner_kaggle_all.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "\n",
    "fit_df = pd.read_csv(\"./data/bank_churner.csv\") # 학습을 위한 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델별] 2023-09-19 13:13:11, 0:00:03, [E_1_1], Model Name: LightGBM          , BEST AUC: 0.990255, AUC: 0.990255\n",
      "[모델별] 2023-09-19 13:13:21, 0:00:09, [E_1_1], Model Name: XgBoost           , BEST AUC: 0.990255, AUC: 0.988054\n",
      "[평  가] 2023-09-19 13:13:21, 0:00:14, [E_1], best_auc: 0.990255, fit_tot_cnt: (8101, 21), eval_tot_cnt: (2026, 21), fit_drop_cnt : (8101, 31), eval_drop_cnt : (2026, 31), X_train: (10200, 13), y_train: (10200,), X_eval: (2026, 13), y_eval:(2026,)\n",
      "[1]\ttraining's auc: 0.947246\ttraining's binary_logloss: 0.606997\tvalid_1's auc: 0.5\tvalid_1's binary_logloss: 0.607502\n",
      "[2]\ttraining's auc: 0.954415\ttraining's binary_logloss: 0.540712\tvalid_1's auc: 0.5\tvalid_1's binary_logloss: 0.548303\n",
      "[3]\ttraining's auc: 0.966035\ttraining's binary_logloss: 0.484324\tvalid_1's auc: 0.504978\tvalid_1's binary_logloss: 0.506701\n",
      "[4]\ttraining's auc: 0.972323\ttraining's binary_logloss: 0.43683\tvalid_1's auc: 0.483494\tvalid_1's binary_logloss: 0.482528\n",
      "[5]\ttraining's auc: 0.976108\ttraining's binary_logloss: 0.398234\tvalid_1's auc: 0.476154\tvalid_1's binary_logloss: 0.496626\n",
      "[6]\ttraining's auc: 0.980323\ttraining's binary_logloss: 0.361964\tvalid_1's auc: 0.475619\tvalid_1's binary_logloss: 0.479705\n",
      "[7]\ttraining's auc: 0.981715\ttraining's binary_logloss: 0.33415\tvalid_1's auc: 0.443949\tvalid_1's binary_logloss: 0.500264\n",
      "[8]\ttraining's auc: 0.983032\ttraining's binary_logloss: 0.309046\tvalid_1's auc: 0.403043\tvalid_1's binary_logloss: 0.509672\n",
      "[9]\ttraining's auc: 0.984475\ttraining's binary_logloss: 0.287502\tvalid_1's auc: 0.399018\tvalid_1's binary_logloss: 0.498646\n",
      "[10]\ttraining's auc: 0.98667\ttraining's binary_logloss: 0.266846\tvalid_1's auc: 0.479\tvalid_1's binary_logloss: 0.481587\n",
      "[11]\ttraining's auc: 0.988273\ttraining's binary_logloss: 0.248174\tvalid_1's auc: 0.504472\tvalid_1's binary_logloss: 0.486515\n",
      "[12]\ttraining's auc: 0.989474\ttraining's binary_logloss: 0.233282\tvalid_1's auc: 0.504472\tvalid_1's binary_logloss: 0.474966\n",
      "[13]\ttraining's auc: 0.990297\ttraining's binary_logloss: 0.218713\tvalid_1's auc: 0.420421\tvalid_1's binary_logloss: 0.494392\n",
      "[14]\ttraining's auc: 0.991049\ttraining's binary_logloss: 0.206788\tvalid_1's auc: 0.420421\tvalid_1's binary_logloss: 0.481138\n",
      "[15]\ttraining's auc: 0.992143\ttraining's binary_logloss: 0.19314\tvalid_1's auc: 0.443227\tvalid_1's binary_logloss: 0.49731\n",
      "[16]\ttraining's auc: 0.993223\ttraining's binary_logloss: 0.179707\tvalid_1's auc: 0.443227\tvalid_1's binary_logloss: 0.484612\n",
      "[17]\ttraining's auc: 0.993886\ttraining's binary_logloss: 0.168883\tvalid_1's auc: 0.467403\tvalid_1's binary_logloss: 0.489095\n",
      "[18]\ttraining's auc: 0.994378\ttraining's binary_logloss: 0.159742\tvalid_1's auc: 0.473043\tvalid_1's binary_logloss: 0.508461\n",
      "[19]\ttraining's auc: 0.994843\ttraining's binary_logloss: 0.151114\tvalid_1's auc: 0.473043\tvalid_1's binary_logloss: 0.490342\n",
      "[20]\ttraining's auc: 0.995291\ttraining's binary_logloss: 0.142672\tvalid_1's auc: 0.473043\tvalid_1's binary_logloss: 0.485471\n",
      "[21]\ttraining's auc: 0.995654\ttraining's binary_logloss: 0.135419\tvalid_1's auc: 0.47313\tvalid_1's binary_logloss: 0.503019\n",
      "[22]\ttraining's auc: 0.995912\ttraining's binary_logloss: 0.129673\tvalid_1's auc: 0.4736\tvalid_1's binary_logloss: 0.486578\n",
      "[23]\ttraining's auc: 0.996205\ttraining's binary_logloss: 0.123573\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.504381\n",
      "[24]\ttraining's auc: 0.996596\ttraining's binary_logloss: 0.11683\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.486956\n",
      "[25]\ttraining's auc: 0.996909\ttraining's binary_logloss: 0.110727\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.4743\n",
      "[26]\ttraining's auc: 0.997182\ttraining's binary_logloss: 0.105486\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.466111\n",
      "[27]\ttraining's auc: 0.997397\ttraining's binary_logloss: 0.100973\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.461828\n",
      "[28]\ttraining's auc: 0.997603\ttraining's binary_logloss: 0.0963879\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.460752\n",
      "[29]\ttraining's auc: 0.997788\ttraining's binary_logloss: 0.0921636\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.462721\n",
      "[30]\ttraining's auc: 0.997959\ttraining's binary_logloss: 0.0883221\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.467352\n",
      "[31]\ttraining's auc: 0.998159\ttraining's binary_logloss: 0.0843875\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.474307\n",
      "[32]\ttraining's auc: 0.998297\ttraining's binary_logloss: 0.0811944\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.483289\n",
      "[33]\ttraining's auc: 0.998426\ttraining's binary_logloss: 0.0781379\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.489312\n",
      "[34]\ttraining's auc: 0.998557\ttraining's binary_logloss: 0.0752805\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.501038\n",
      "[35]\ttraining's auc: 0.998667\ttraining's binary_logloss: 0.0726844\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.51412\n",
      "[36]\ttraining's auc: 0.998769\ttraining's binary_logloss: 0.0701982\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.528652\n",
      "[37]\ttraining's auc: 0.998855\ttraining's binary_logloss: 0.0677051\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.544263\n",
      "[38]\ttraining's auc: 0.998964\ttraining's binary_logloss: 0.0652674\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.560751\n",
      "[39]\ttraining's auc: 0.999073\ttraining's binary_logloss: 0.0626461\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.578226\n",
      "[40]\ttraining's auc: 0.999138\ttraining's binary_logloss: 0.0608223\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.596313\n",
      "[41]\ttraining's auc: 0.999206\ttraining's binary_logloss: 0.0588584\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.615223\n",
      "[42]\ttraining's auc: 0.999267\ttraining's binary_logloss: 0.0572143\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.634636\n",
      "[43]\ttraining's auc: 0.999303\ttraining's binary_logloss: 0.0555383\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.654481\n",
      "[44]\ttraining's auc: 0.999353\ttraining's binary_logloss: 0.0536517\tvalid_1's auc: 0.473662\tvalid_1's binary_logloss: 0.674697\n",
      "[45]\ttraining's auc: 0.999398\ttraining's binary_logloss: 0.0520058\tvalid_1's auc: 0.542309\tvalid_1's binary_logloss: 0.664523\n",
      "[46]\ttraining's auc: 0.999435\ttraining's binary_logloss: 0.0505508\tvalid_1's auc: 0.542309\tvalid_1's binary_logloss: 0.685059\n",
      "[47]\ttraining's auc: 0.999501\ttraining's binary_logloss: 0.0490968\tvalid_1's auc: 0.538096\tvalid_1's binary_logloss: 0.705567\n",
      "[48]\ttraining's auc: 0.999528\ttraining's binary_logloss: 0.0478123\tvalid_1's auc: 0.538096\tvalid_1's binary_logloss: 0.693165\n",
      "[49]\ttraining's auc: 0.999575\ttraining's binary_logloss: 0.0463266\tvalid_1's auc: 0.488344\tvalid_1's binary_logloss: 0.695662\n",
      "[50]\ttraining's auc: 0.999613\ttraining's binary_logloss: 0.0450448\tvalid_1's auc: 0.488296\tvalid_1's binary_logloss: 0.712624\n",
      "[51]\ttraining's auc: 0.999651\ttraining's binary_logloss: 0.0439072\tvalid_1's auc: 0.548061\tvalid_1's binary_logloss: 0.708019\n",
      "[52]\ttraining's auc: 0.999686\ttraining's binary_logloss: 0.0425149\tvalid_1's auc: 0.548061\tvalid_1's binary_logloss: 0.722845\n",
      "[53]\ttraining's auc: 0.999705\ttraining's binary_logloss: 0.0413625\tvalid_1's auc: 0.548061\tvalid_1's binary_logloss: 0.739509\n",
      "[54]\ttraining's auc: 0.999737\ttraining's binary_logloss: 0.0403173\tvalid_1's auc: 0.548061\tvalid_1's binary_logloss: 0.755211\n",
      "[55]\ttraining's auc: 0.999757\ttraining's binary_logloss: 0.0394312\tvalid_1's auc: 0.54853\tvalid_1's binary_logloss: 0.770703\n",
      "[56]\ttraining's auc: 0.999785\ttraining's binary_logloss: 0.0383018\tvalid_1's auc: 0.5267\tvalid_1's binary_logloss: 0.772668\n",
      "[57]\ttraining's auc: 0.999807\ttraining's binary_logloss: 0.0372351\tvalid_1's auc: 0.5267\tvalid_1's binary_logloss: 0.762618\n",
      "[58]\ttraining's auc: 0.999828\ttraining's binary_logloss: 0.0362336\tvalid_1's auc: 0.5267\tvalid_1's binary_logloss: 0.777467\n",
      "[59]\ttraining's auc: 0.99985\ttraining's binary_logloss: 0.0352837\tvalid_1's auc: 0.532217\tvalid_1's binary_logloss: 0.776026\n",
      "[60]\ttraining's auc: 0.999862\ttraining's binary_logloss: 0.0342989\tvalid_1's auc: 0.497194\tvalid_1's binary_logloss: 0.780437\n",
      "[61]\ttraining's auc: 0.999877\ttraining's binary_logloss: 0.033471\tvalid_1's auc: 0.497002\tvalid_1's binary_logloss: 0.778744\n",
      "[62]\ttraining's auc: 0.999884\ttraining's binary_logloss: 0.0326933\tvalid_1's auc: 0.497002\tvalid_1's binary_logloss: 0.795263\n",
      "[63]\ttraining's auc: 0.999897\ttraining's binary_logloss: 0.0319419\tvalid_1's auc: 0.497208\tvalid_1's binary_logloss: 0.787973\n",
      "[64]\ttraining's auc: 0.999918\ttraining's binary_logloss: 0.0310714\tvalid_1's auc: 0.494903\tvalid_1's binary_logloss: 0.80067\n",
      "[65]\ttraining's auc: 0.999927\ttraining's binary_logloss: 0.0303329\tvalid_1's auc: 0.495085\tvalid_1's binary_logloss: 0.822241\n",
      "[66]\ttraining's auc: 0.999936\ttraining's binary_logloss: 0.0294322\tvalid_1's auc: 0.495085\tvalid_1's binary_logloss: 0.835241\n",
      "[67]\ttraining's auc: 0.999943\ttraining's binary_logloss: 0.0287153\tvalid_1's auc: 0.495065\tvalid_1's binary_logloss: 0.857106\n",
      "[68]\ttraining's auc: 0.999952\ttraining's binary_logloss: 0.028063\tvalid_1's auc: 0.499109\tvalid_1's binary_logloss: 0.861535\n",
      "[69]\ttraining's auc: 0.999956\ttraining's binary_logloss: 0.0274254\tvalid_1's auc: 0.52964\tvalid_1's binary_logloss: 0.851297\n",
      "[70]\ttraining's auc: 0.99996\ttraining's binary_logloss: 0.0268151\tvalid_1's auc: 0.52964\tvalid_1's binary_logloss: 0.863403\n",
      "[71]\ttraining's auc: 0.999967\ttraining's binary_logloss: 0.0261514\tvalid_1's auc: 0.529555\tvalid_1's binary_logloss: 0.862861\n",
      "[72]\ttraining's auc: 0.999972\ttraining's binary_logloss: 0.025479\tvalid_1's auc: 0.530292\tvalid_1's binary_logloss: 0.884711\n",
      "[73]\ttraining's auc: 0.999977\ttraining's binary_logloss: 0.0248132\tvalid_1's auc: 0.521423\tvalid_1's binary_logloss: 0.875016\n",
      "[74]\ttraining's auc: 0.99998\ttraining's binary_logloss: 0.0242129\tvalid_1's auc: 0.539828\tvalid_1's binary_logloss: 0.873339\n",
      "[75]\ttraining's auc: 0.999982\ttraining's binary_logloss: 0.0236679\tvalid_1's auc: 0.531259\tvalid_1's binary_logloss: 0.865395\n",
      "[76]\ttraining's auc: 0.999986\ttraining's binary_logloss: 0.0230907\tvalid_1's auc: 0.504548\tvalid_1's binary_logloss: 0.868522\n",
      "[77]\ttraining's auc: 0.999987\ttraining's binary_logloss: 0.0224995\tvalid_1's auc: 0.504548\tvalid_1's binary_logloss: 0.867998\n",
      "[78]\ttraining's auc: 0.99999\ttraining's binary_logloss: 0.022047\tvalid_1's auc: 0.504548\tvalid_1's binary_logloss: 0.890242\n",
      "[79]\ttraining's auc: 0.999991\ttraining's binary_logloss: 0.0215951\tvalid_1's auc: 0.487083\tvalid_1's binary_logloss: 0.897662\n",
      "[80]\ttraining's auc: 0.999992\ttraining's binary_logloss: 0.021046\tvalid_1's auc: 0.494591\tvalid_1's binary_logloss: 0.886582\n",
      "[81]\ttraining's auc: 0.999994\ttraining's binary_logloss: 0.0206378\tvalid_1's auc: 0.494591\tvalid_1's binary_logloss: 0.908844\n",
      "[82]\ttraining's auc: 0.999996\ttraining's binary_logloss: 0.020071\tvalid_1's auc: 0.491196\tvalid_1's binary_logloss: 0.886324\n",
      "[83]\ttraining's auc: 0.999997\ttraining's binary_logloss: 0.0196596\tvalid_1's auc: 0.494666\tvalid_1's binary_logloss: 0.878852\n",
      "[84]\ttraining's auc: 0.999997\ttraining's binary_logloss: 0.0191064\tvalid_1's auc: 0.494848\tvalid_1's binary_logloss: 0.880553\n",
      "[85]\ttraining's auc: 0.999998\ttraining's binary_logloss: 0.0186329\tvalid_1's auc: 0.494848\tvalid_1's binary_logloss: 0.896711\n",
      "[86]\ttraining's auc: 0.999998\ttraining's binary_logloss: 0.0182014\tvalid_1's auc: 0.494848\tvalid_1's binary_logloss: 0.888613\n",
      "[87]\ttraining's auc: 0.999998\ttraining's binary_logloss: 0.0177279\tvalid_1's auc: 0.494696\tvalid_1's binary_logloss: 0.884546\n",
      "[88]\ttraining's auc: 0.999998\ttraining's binary_logloss: 0.0172252\tvalid_1's auc: 0.494436\tvalid_1's binary_logloss: 0.887132\n",
      "[89]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0168189\tvalid_1's auc: 0.494436\tvalid_1's binary_logloss: 0.873117\n",
      "[90]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0164111\tvalid_1's auc: 0.494436\tvalid_1's binary_logloss: 0.858908\n",
      "[91]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.016067\tvalid_1's auc: 0.494436\tvalid_1's binary_logloss: 0.881013\n",
      "[92]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.0157313\tvalid_1's auc: 0.504327\tvalid_1's binary_logloss: 0.863477\n",
      "[93]\ttraining's auc: 0.999999\ttraining's binary_logloss: 0.015405\tvalid_1's auc: 0.504327\tvalid_1's binary_logloss: 0.885577\n",
      "[94]\ttraining's auc: 1\ttraining's binary_logloss: 0.0150734\tvalid_1's auc: 0.504327\tvalid_1's binary_logloss: 0.877941\n",
      "[95]\ttraining's auc: 1\ttraining's binary_logloss: 0.0147566\tvalid_1's auc: 0.51775\tvalid_1's binary_logloss: 0.869244\n",
      "[96]\ttraining's auc: 1\ttraining's binary_logloss: 0.0143896\tvalid_1's auc: 0.51777\tvalid_1's binary_logloss: 0.868234\n",
      "[97]\ttraining's auc: 1\ttraining's binary_logloss: 0.0140535\tvalid_1's auc: 0.51777\tvalid_1's binary_logloss: 0.890322\n",
      "[98]\ttraining's auc: 1\ttraining's binary_logloss: 0.0137324\tvalid_1's auc: 0.517875\tvalid_1's binary_logloss: 0.887963\n",
      "[99]\ttraining's auc: 1\ttraining's binary_logloss: 0.0133905\tvalid_1's auc: 0.517417\tvalid_1's binary_logloss: 0.88748\n",
      "[100]\ttraining's auc: 1\ttraining's binary_logloss: 0.0131006\tvalid_1's auc: 0.517647\tvalid_1's binary_logloss: 0.909445\n",
      "[101]\ttraining's auc: 1\ttraining's binary_logloss: 0.012823\tvalid_1's auc: 0.517647\tvalid_1's binary_logloss: 0.92126\n",
      "[102]\ttraining's auc: 1\ttraining's binary_logloss: 0.0125583\tvalid_1's auc: 0.517647\tvalid_1's binary_logloss: 0.933587\n",
      "[103]\ttraining's auc: 1\ttraining's binary_logloss: 0.0122704\tvalid_1's auc: 0.517647\tvalid_1's binary_logloss: 0.944681\n",
      "[104]\ttraining's auc: 1\ttraining's binary_logloss: 0.0119647\tvalid_1's auc: 0.517647\tvalid_1's binary_logloss: 0.959481\n",
      "[105]\ttraining's auc: 1\ttraining's binary_logloss: 0.0117135\tvalid_1's auc: 0.51755\tvalid_1's binary_logloss: 0.970252\n",
      "[106]\ttraining's auc: 1\ttraining's binary_logloss: 0.0114428\tvalid_1's auc: 0.51532\tvalid_1's binary_logloss: 0.975002\n",
      "[107]\ttraining's auc: 1\ttraining's binary_logloss: 0.0111597\tvalid_1's auc: 0.51532\tvalid_1's binary_logloss: 0.960942\n",
      "[108]\ttraining's auc: 1\ttraining's binary_logloss: 0.0109203\tvalid_1's auc: 0.515164\tvalid_1's binary_logloss: 0.961099\n",
      "[109]\ttraining's auc: 1\ttraining's binary_logloss: 0.010653\tvalid_1's auc: 0.519022\tvalid_1's binary_logloss: 0.950201\n",
      "[110]\ttraining's auc: 1\ttraining's binary_logloss: 0.0103988\tvalid_1's auc: 0.519022\tvalid_1's binary_logloss: 0.972556\n",
      "[111]\ttraining's auc: 1\ttraining's binary_logloss: 0.0101637\tvalid_1's auc: 0.537285\tvalid_1's binary_logloss: 0.97175\n",
      "[112]\ttraining's auc: 1\ttraining's binary_logloss: 0.00993581\tvalid_1's auc: 0.546266\tvalid_1's binary_logloss: 0.981324\n",
      "[113]\ttraining's auc: 1\ttraining's binary_logloss: 0.00970069\tvalid_1's auc: 0.558333\tvalid_1's binary_logloss: 0.964531\n",
      "[114]\ttraining's auc: 1\ttraining's binary_logloss: 0.0094808\tvalid_1's auc: 0.558235\tvalid_1's binary_logloss: 0.964499\n",
      "[115]\ttraining's auc: 1\ttraining's binary_logloss: 0.0092357\tvalid_1's auc: 0.573875\tvalid_1's binary_logloss: 0.947201\n",
      "[116]\ttraining's auc: 1\ttraining's binary_logloss: 0.00906052\tvalid_1's auc: 0.580069\tvalid_1's binary_logloss: 0.952655\n",
      "[117]\ttraining's auc: 1\ttraining's binary_logloss: 0.00886953\tvalid_1's auc: 0.57998\tvalid_1's binary_logloss: 0.97478\n",
      "[118]\ttraining's auc: 1\ttraining's binary_logloss: 0.00866003\tvalid_1's auc: 0.579957\tvalid_1's binary_logloss: 0.973991\n",
      "[119]\ttraining's auc: 1\ttraining's binary_logloss: 0.00849638\tvalid_1's auc: 0.579957\tvalid_1's binary_logloss: 0.963808\n",
      "[120]\ttraining's auc: 1\ttraining's binary_logloss: 0.00832655\tvalid_1's auc: 0.579957\tvalid_1's binary_logloss: 0.955445\n",
      "[121]\ttraining's auc: 1\ttraining's binary_logloss: 0.00812783\tvalid_1's auc: 0.579957\tvalid_1's binary_logloss: 0.977822\n",
      "[122]\ttraining's auc: 1\ttraining's binary_logloss: 0.00792884\tvalid_1's auc: 0.592109\tvalid_1's binary_logloss: 0.976939\n",
      "[123]\ttraining's auc: 1\ttraining's binary_logloss: 0.00777099\tvalid_1's auc: 0.59204\tvalid_1's binary_logloss: 0.980269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]\ttraining's auc: 1\ttraining's binary_logloss: 0.00760645\tvalid_1's auc: 0.590793\tvalid_1's binary_logloss: 0.967188\n",
      "[125]\ttraining's auc: 1\ttraining's binary_logloss: 0.00745037\tvalid_1's auc: 0.592142\tvalid_1's binary_logloss: 0.951965\n",
      "[126]\ttraining's auc: 1\ttraining's binary_logloss: 0.00728707\tvalid_1's auc: 0.592142\tvalid_1's binary_logloss: 0.974316\n",
      "[127]\ttraining's auc: 1\ttraining's binary_logloss: 0.00714225\tvalid_1's auc: 0.592142\tvalid_1's binary_logloss: 0.974649\n",
      "[128]\ttraining's auc: 1\ttraining's binary_logloss: 0.00699682\tvalid_1's auc: 0.592142\tvalid_1's binary_logloss: 0.97495\n",
      "ROC AUC: 0.9802\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------    \n",
    "# 평가 for Competition\n",
    "# -----------------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "model_eval_comparison = {}                        \n",
    "fit_tot_cnt = fit_df.shape\n",
    "eval_tot_cnt = eval_df.shape\n",
    "\n",
    "\n",
    "# 전처리 단계\n",
    "# -----------\n",
    "fit_df = test_transform(fit_df)\n",
    "eval_df = test_transform(eval_df)\n",
    "fit_drop_cnt = fit_df.shape\n",
    "eval_drop_cnt = eval_df.shape\n",
    "\n",
    "    \n",
    "# 평가를 위한 데이터 분리\n",
    "# ---------------------\n",
    "X_Features=fit_df.drop(['is_churned'],axis=1)\n",
    "y_labels=fit_df['is_churned']\n",
    "\n",
    "X_eval=eval_df.drop(['is_churned'],axis=1)\n",
    "y_eval=eval_df['is_churned']\n",
    "\n",
    "\n",
    "# 중요 Feature Column 선택\n",
    "# -----------------------\n",
    "X_Features_new, selected_columns = select_feature(X_Features, y_labels, 'ExtraTrees')\n",
    "# X_new, selected_columns = select_feature(X_train, y_train, 'LGBMC')\n",
    "X_eval = X_eval[selected_columns]\n",
    "\n",
    "\n",
    "# Train and Test 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, y_train, X_test, y_test = proc_smote(X_Features_new, y_labels)\n",
    "\n",
    "\n",
    "# Evaluation 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, X_eval = proc_normalization(X_train, X_eval.values)   \n",
    "\n",
    "\n",
    "# 최종 평가\n",
    "# --------\n",
    "proc_type='E'\n",
    "drop_no = 1\n",
    "group_no =1\n",
    "\n",
    "\n",
    "# eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "eval_auc = fit_predict_eval(proc_type, drop_no, group_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "if eval_auc > best_auc:\n",
    "    best_type = f'{proc_type}_{drop_no}_{group_no}'\n",
    "    best_auc = eval_auc\n",
    "\n",
    "\n",
    "# 최종 평가 로그 출력\n",
    "# ------------------\n",
    "cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_time = time.time()\n",
    "delta_time = end_time - start_time\n",
    "print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], best_auc: {best_auc:0.6f}, fit_tot_cnt: {fit_tot_cnt}, eval_tot_cnt: {eval_tot_cnt}, fit_drop_cnt : {fit_drop_cnt}, eval_drop_cnt : {eval_drop_cnt}, X_train: {X_train.shape}, y_train: {y_train.shape}, X_eval: {X_eval.shape}, y_eval:{y_eval.shape}')\n",
    "\n",
    "\n",
    "# print_eval_result(model_eval_comparison)\n",
    "best = {'learning_rate': 0.1406105325029019, 'max_depth': 106.0, 'min_child_samples': 64.0, 'num_leaves': 41.0, 'subsample': 0.9462293554201169}\n",
    "lgbm_clf =  LGBMClassifier(n_estimators=700, num_leaves=int(best['num_leaves']),\n",
    "                           max_depth=int(best['max_depth']),\n",
    "                           min_child_samples=int(best['min_child_samples']), \n",
    "                           subsample=round(best['subsample'], 5),\n",
    "                           learning_rate=round(best['learning_rate'], 5)\n",
    "                          )\n",
    "\n",
    "\n",
    "# evaluation metric을 auc로, early stopping은 100 으로 설정하고 학습 수행. \n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, \n",
    "            eval_metric=\"auc\",eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_eval, lgbm_clf.predict_proba(X_eval)[:,1])\n",
    "print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
