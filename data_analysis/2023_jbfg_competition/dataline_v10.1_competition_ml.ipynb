{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JBFG Data Analysis Competition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 데이터 및 Null 건수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 데이터 포맷 함수\\n# ---------------\\ndef change_format(df, column, format):\\n    \\'\\'\\'\\n    데이터프레임의 지정된 컬럼에 컴마, 백분율로 변경하여 데이터프레임을 반환하는 함수\\n        Args:\\n            df (df) : DataFrame\\n            column (str) : column of DataFrame\\n            format (str) : \\'comma\\' | \\'percent\\'\\n        Return:\\n            DataFrame\\n    \\'\\'\\'\\n    if format == \\'comma\\':\\n        df[column] = df[column].apply(lambda x: f\"{x:,}\")\\n    elif format == \\'percent\\':\\n        df[column] = df[column].apply(lambda x: f\"{x:.2%}\")\\n        \\n    return df\\n\\n\\n# 데이터프레임의 특정 컬럼에 대한 건수, Null, Percent 표시\\n# -----------------------------------------------------\\ndef count_column_na_count(df, column):\\n    \\'\\'\\'\\n    데이터프레임의 특정 컬럼에 대한 건수, Null, Percent를 출력하는 함수\\n        Args:\\n            df (df) : DataFrame\\n            column (str) : column of DataFrame\\n        Return:\\n            None\\n    \\'\\'\\'\\n    column_na_counts = df[column].size, df[column].count(), df[column].isnull().sum()\\n    column_na_counts_df = pd.Series(column_na_counts).to_frame().T\\n    column_na_counts_df.columns = [\\'tot_counts\\', \\'data_counts\\', \\'null_counts\\']\\n    column_na_counts_df[\\'data_percents\\'] = column_na_counts_df[\\'data_counts\\'].values/column_na_counts_df[\\'tot_counts\\'].values\\n    column_na_counts_df[\\'null_percents\\'] = column_na_counts_df[\\'null_counts\\'].values/column_na_counts_df[\\'tot_counts\\'].values\\n\\n\\n    column_na_counts_df = change_format(column_na_counts_df, \\'tot_counts\\', \\'comma\\')\\n    column_na_counts_df = change_format(column_na_counts_df, \\'data_counts\\',\\'comma\\')\\n    column_na_counts_df = change_format(column_na_counts_df, \\'null_counts\\',\\'comma\\')\\n    column_na_counts_df = change_format(column_na_counts_df, \\'data_percents\\', \\'percent\\')\\n    column_na_counts_df = change_format(column_na_counts_df, \\'null_percents\\', \\'percent\\')\\n\\n    print(column_na_counts_df.to_string(index=False))\\n    print(\\'-\\'*70)\\n\\n\\ndef count_column_data_count(df, column):\\n    \\'\\'\\' \\n    \\'\\'\\'\\n    # column_data_countcounts = df.groupby(column)[\\'is_churned\\'].value_counts().unstack()\\n\\n\\n    column_counts = df.groupby(column)[\\'is_churned\\'].value_counts().unstack()\\n    column_counts = column_counts.rename(columns={0: \\'exist_counts\\', 1: \\'churned_counts\\'})\\n    column_counts[\\'total_counts\\'] =  column_counts[\\'exist_counts\\'] + column_counts[\\'churned_counts\\']\\n    column_counts = column_counts.fillna(0)\\n\\n    column_percents = df.groupby(column)[\\'is_churned\\'].value_counts(normalize=True).unstack()\\n    column_percents = column_percents.rename(columns={0: \\'exist_percents\\', 1: \\'churned_percents\\'})\\n    column_percents = column_percents.fillna(0)\\n\\n\\n    column_count_percent = pd.concat([column_counts, column_percents], axis=1)\\n    column_count_percent = column_count_percent.reset_index()\\n    column_count_percent = column_count_percent.sort_values(by=\\'churned_percents\\', ascending=False)\\n\\n    \\n    column_count_percent = change_format(column_count_percent, \\'exist_counts\\', \\'comma\\')\\n    column_count_percent = change_format(column_count_percent, \\'churned_counts\\', \\'comma\\')\\n    column_count_percent = change_format(column_count_percent, \\'total_counts\\', \\'comma\\')\\n    column_count_percent = change_format(column_count_percent, \\'exist_percents\\', \\'percent\\')\\n    column_count_percent = change_format(column_count_percent, \\'churned_percents\\', \\'percent\\')\\n    \\n\\n    print(column_count_percent.to_string(index=False))\\n\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 데이터 포맷 함수\n",
    "# ---------------\n",
    "def change_format(df, column, format):\n",
    "    '''\n",
    "    데이터프레임의 지정된 컬럼에 컴마, 백분율로 변경하여 데이터프레임을 반환하는 함수\n",
    "        Args:\n",
    "            df (df) : DataFrame\n",
    "            column (str) : column of DataFrame\n",
    "            format (str) : 'comma' | 'percent'\n",
    "        Return:\n",
    "            DataFrame\n",
    "    '''\n",
    "    if format == 'comma':\n",
    "        df[column] = df[column].apply(lambda x: f\"{x:,}\")\n",
    "    elif format == 'percent':\n",
    "        df[column] = df[column].apply(lambda x: f\"{x:.2%}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "# 데이터프레임의 특정 컬럼에 대한 건수, Null, Percent 표시\n",
    "# -----------------------------------------------------\n",
    "def count_column_na_count(df, column):\n",
    "    '''\n",
    "    데이터프레임의 특정 컬럼에 대한 건수, Null, Percent를 출력하는 함수\n",
    "        Args:\n",
    "            df (df) : DataFrame\n",
    "            column (str) : column of DataFrame\n",
    "        Return:\n",
    "            None\n",
    "    '''\n",
    "    column_na_counts = df[column].size, df[column].count(), df[column].isnull().sum()\n",
    "    column_na_counts_df = pd.Series(column_na_counts).to_frame().T\n",
    "    column_na_counts_df.columns = ['tot_counts', 'data_counts', 'null_counts']\n",
    "    column_na_counts_df['data_percents'] = column_na_counts_df['data_counts'].values/column_na_counts_df['tot_counts'].values\n",
    "    column_na_counts_df['null_percents'] = column_na_counts_df['null_counts'].values/column_na_counts_df['tot_counts'].values\n",
    "\n",
    "\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'tot_counts', 'comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'data_counts','comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'null_counts','comma')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'data_percents', 'percent')\n",
    "    column_na_counts_df = change_format(column_na_counts_df, 'null_percents', 'percent')\n",
    "\n",
    "    print(column_na_counts_df.to_string(index=False))\n",
    "    print('-'*70)\n",
    "\n",
    "\n",
    "def count_column_data_count(df, column):\n",
    "    ''' \n",
    "    '''\n",
    "    # column_data_countcounts = df.groupby(column)['is_churned'].value_counts().unstack()\n",
    "\n",
    "\n",
    "    column_counts = df.groupby(column)['is_churned'].value_counts().unstack()\n",
    "    column_counts = column_counts.rename(columns={0: 'exist_counts', 1: 'churned_counts'})\n",
    "    column_counts['total_counts'] =  column_counts['exist_counts'] + column_counts['churned_counts']\n",
    "    column_counts = column_counts.fillna(0)\n",
    "\n",
    "    column_percents = df.groupby(column)['is_churned'].value_counts(normalize=True).unstack()\n",
    "    column_percents = column_percents.rename(columns={0: 'exist_percents', 1: 'churned_percents'})\n",
    "    column_percents = column_percents.fillna(0)\n",
    "\n",
    "\n",
    "    column_count_percent = pd.concat([column_counts, column_percents], axis=1)\n",
    "    column_count_percent = column_count_percent.reset_index()\n",
    "    column_count_percent = column_count_percent.sort_values(by='churned_percents', ascending=False)\n",
    "\n",
    "    \n",
    "    column_count_percent = change_format(column_count_percent, 'exist_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'churned_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'total_counts', 'comma')\n",
    "    column_count_percent = change_format(column_count_percent, 'exist_percents', 'percent')\n",
    "    column_count_percent = change_format(column_count_percent, 'churned_percents', 'percent')\n",
    "    \n",
    "\n",
    "    print(column_count_percent.to_string(index=False))\n",
    "\n",
    "\"\"\"    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import time\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tot_column_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef tot_column_counts(df):\\n    ''' \\n    '''\\n    data_counts = df.count()\\n    null_counts = df.isnull().sum()\\n    tot_counts_df = pd.concat([data_counts, null_counts], axis=1)\\n    tot_counts_df = tot_counts_df.rename(columns={0: 'data_counts', 1: 'null_counts'})\\n    tot_counts_df.insert(0,'tot_counts', tot_counts_df['data_counts'] + tot_counts_df['null_counts'])\\n    tot_counts_df['data_percents'] = tot_counts_df['data_counts'].values / tot_counts_df['tot_counts'].values\\n    tot_counts_df['null_percents'] = tot_counts_df['null_counts'].values / tot_counts_df['tot_counts'].values\\n    tot_counts_df = tot_counts_df.sort_values(by='null_percents', ascending=False)\\n\\n    tot_counts_df = change_format(tot_counts_df, 'tot_counts', 'comma')\\n    tot_counts_df = change_format(tot_counts_df, 'data_counts','comma')\\n    tot_counts_df = change_format(tot_counts_df, 'null_counts','comma')\\n    tot_counts_df = change_format(tot_counts_df, 'data_percents', 'percent')\\n    tot_counts_df = change_format(tot_counts_df, 'null_percents', 'percent')\\n\\n    tot_counts_df = tot_counts_df.reset_index()\\n\\n    print(tot_counts_df.to_string(index=False))\\n    \\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def tot_column_counts(df):\n",
    "    ''' \n",
    "    '''\n",
    "    data_counts = df.count()\n",
    "    null_counts = df.isnull().sum()\n",
    "    tot_counts_df = pd.concat([data_counts, null_counts], axis=1)\n",
    "    tot_counts_df = tot_counts_df.rename(columns={0: 'data_counts', 1: 'null_counts'})\n",
    "    tot_counts_df.insert(0,'tot_counts', tot_counts_df['data_counts'] + tot_counts_df['null_counts'])\n",
    "    tot_counts_df['data_percents'] = tot_counts_df['data_counts'].values / tot_counts_df['tot_counts'].values\n",
    "    tot_counts_df['null_percents'] = tot_counts_df['null_counts'].values / tot_counts_df['tot_counts'].values\n",
    "    tot_counts_df = tot_counts_df.sort_values(by='null_percents', ascending=False)\n",
    "\n",
    "    tot_counts_df = change_format(tot_counts_df, 'tot_counts', 'comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'data_counts','comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'null_counts','comma')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'data_percents', 'percent')\n",
    "    tot_counts_df = change_format(tot_counts_df, 'null_percents', 'percent')\n",
    "\n",
    "    tot_counts_df = tot_counts_df.reset_index()\n",
    "\n",
    "    print(tot_counts_df.to_string(index=False))\n",
    "    \n",
    "\"\"\"    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop_null_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임의 특정컬럼을 리스트로 받아 삭제\n",
    "def drop_null_column(df, drop_list):\n",
    "    '''\n",
    "        데이터프레임의 특정컬럼을 리스트로 받아 삭제후 반환하는 함수\n",
    "        \n",
    "        Args:\n",
    "            df (df) : DataFrame\n",
    "            drop_list (list) : 삭제대상 컬럼의 List \n",
    "        Return:\n",
    "            DataFrame\n",
    "    '''\n",
    "    for col_name in drop_list:\n",
    "        # print(col_name, type(col_name))\n",
    "        df = df.drop(col_name, axis=1)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode_onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩 처리 \n",
    "# ----------------\n",
    "def encode_onehot(df):\n",
    "    '''\n",
    "        데이터프레임의 object type 컬럼을 원-핫 인코딩하는 함수\n",
    "        \n",
    "        Args:\n",
    "            df (df) : DataFrame\n",
    "        Return:\n",
    "            DataFrame\n",
    "    '''\n",
    "    catcols = df.select_dtypes(exclude = ['int64','float64']).columns\n",
    "    df = pd.get_dummies(df, columns = catcols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 Feature 식별\n",
    "# ----------------\n",
    "def select_feature(df, y, chosen_model):\n",
    "\n",
    "    np.random.seed(42)    \n",
    "    \n",
    "    available_models = {\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'RFE': RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=13),\n",
    "    'LGBMC': LGBMClassifier(),\n",
    "    'LGBMR': LGBMRegressor(),\n",
    "    'Xg Boost':XGBClassifier(booster='gbtree', importance_type='gain', eval_metric='auc'),\n",
    "    }\n",
    "\n",
    "    # Create the selected model\n",
    "    clf = available_models[chosen_model]\n",
    "\n",
    "    clf = clf.fit(df.values, y)                                     # Train\n",
    "\n",
    "    if chosen_model == 'LGBMC' or chosen_model == 'LGBMR': \n",
    "        feature_importances = clf.booster_.feature_importance(importance_type=\"gain\")\n",
    "    else:        \n",
    "        feature_importances = clf.feature_importances_\n",
    "\n",
    "\n",
    "    chosen_model = SelectFromModel(clf, prefit=True)\n",
    "    X_df = chosen_model.transform(df.values) \n",
    "    selected_feature_indices = chosen_model.get_support(indices=True)\n",
    "\n",
    "    selected_columns = df.columns[selected_feature_indices]         # Get the indices of the selected features\n",
    "    \n",
    "    return X_df, selected_columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proc_smote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_smote(X_new, y):\n",
    "    #Model Training\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X_new,y,test_size=0.25,stratify=y,random_state=0)\n",
    "\n",
    "    sm = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "    X_train, y_train=sm.fit_resample(X_train,y_train)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proc_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_normalization(X_train, X_test):\n",
    "    #Normalization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc=StandardScaler()\n",
    "    X_train=sc.fit_transform(X_train)\n",
    "    X_test=sc.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit_predict_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 평가\n",
    "# -----------\n",
    "def fit_predict_eval(proc_type, drop_no, model_comparison, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # 초기화\n",
    "    # ------\n",
    "    best_roc_auc = 0\n",
    "    \n",
    "    # Define Models\n",
    "    # ------------- \n",
    "    models = [\n",
    "        ('LogisticRegression', LogisticRegression()),\n",
    "        ('DecisionTree', DecisionTreeClassifier(criterion='entropy', random_state=0)),\n",
    "        ('KNN', KNeighborsClassifier(n_neighbors=5)),\n",
    "        ('NaiveBayes', GaussianNB()),\n",
    "        ('RandomForest', RandomForestClassifier(n_estimators=700, criterion='entropy', random_state=0)),\n",
    "        ('LightGBM', LGBMClassifier(n_estimators=700, random_state=42, boosting_type='GOSS')),\n",
    "        ('XgBoost', XGBClassifier(n_estimators=700, random_state=42, use_label_encoder=False,  eval_metric='auc')),\n",
    "        # ('Xg Boost', XGBClassifier(n_estimators=700, random_state=42, use_label_encoder=False, eval_metric='logloss')),        \n",
    "        ('ExtraTrees', ExtraTreesClassifier(n_estimators=700)),\n",
    "        # ('SVM', SVC(kernel='linear')),\n",
    "        # ('LASSO', Lasso(alpha=0.01)),\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Model Fit and Testing\n",
    "    # ---------------------\n",
    "    for model_name, classifier in models:\n",
    "        start_time = time.time()            \n",
    "        classifier.fit(X_train, y_train)            # Fit\n",
    "        \n",
    "        # 학습된 모델 저장\n",
    "        # ---------------\n",
    "        file_name = f'./models/{model_name}.pkl'\n",
    "        print\n",
    "        joblib.dump(classifier, file_name)\n",
    "\n",
    "        y_pred = classifier.predict(X_test)         # Test\n",
    "        pred_proba = classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate model metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5, scoring=\"roc_auc\")\n",
    "        # accuracies = cross_val_score(estimator=classifier, X=X_test, y=y_test, cv=5, scoring=\"recall\")\n",
    "        cv_auc = accuracies.mean()\n",
    "        cv_std = accuracies.std()\n",
    "        \n",
    "        accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "        accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1], )\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "        \n",
    "        \n",
    "        # Collect Result\n",
    "        # --------------\n",
    "        model_comparison[f'{model_name}_{proc_type}_{drop_no}'] = [accuracy, accuracy_class_0, accuracy_class_1, f1, cv_auc, cv_std, roc_auc]\n",
    "        \n",
    "        \n",
    "        # Best ROC_AUC Value Return\n",
    "        # -------------------------\n",
    "        if roc_auc > best_roc_auc:\n",
    "            best_roc_auc = roc_auc\n",
    "\n",
    "        cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_time = time.time()\n",
    "        delta_time = end_time - start_time\n",
    "        print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], Model Name: {model_name:<18}, BEST AUC: {best_roc_auc:0.6f}, AUC: {roc_auc:0.6f}')\n",
    "\n",
    "    return best_roc_auc\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print_eval_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_result(model_comparison):\n",
    "\n",
    "    # # MODEL COMPARISSON\n",
    "    # Model_com_df=pd.DataFrame(model_comparison).T\n",
    "    # Model_com_df.columns=['Model Accuracy','Model Accuracy-0','Model Accuracy-1','Model F1-Score','CV Accuracy','CV std', 'AUC']\n",
    "    # Model_com_df=Model_com_df.sort_values(by='AUC',ascending=False)\n",
    "    # # display(Model_com_df.style.format(\"{:.2%}\").background_gradient(cmap='magma'))\n",
    "\n",
    "    Model_com_df = pd.DataFrame(model_comparison).T\n",
    "    Model_com_df.columns = ['Accuracy', 'Accuracy-No', 'Accuracy-Yes', 'F1-Score', 'CV AUC', 'CV std', 'AUC']\n",
    "    Model_com_df = Model_com_df.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "    def highlight_below_75(s):\n",
    "        if s.name != 'CV std' and isinstance(s, pd.Series) and s.dtype == 'float64':\n",
    "            return ['color: red' if value < 0.75 else 'color: black' for value in s]\n",
    "        else:\n",
    "            return ['color: black'] * len(s)\n",
    "\n",
    "    styled_df = Model_com_df.iloc[:10,:].style.highlight_max(axis=0).apply(highlight_below_75, subset=pd.IndexSlice[:, :'CV AUC']).format(\"{:.2%}\", subset=pd.IndexSlice[:, :'CV AUC'])\n",
    "    display(styled_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(df):\n",
    "    \n",
    "    # 데이터 변환\n",
    "    # ------------------- \n",
    "    df = df.drop('cstno', axis=1)\n",
    "    df = df.drop('sex', axis=1)\n",
    "    # after_drop_cnt=len(df)\n",
    "    df['imcome_cat']=df['imcome_cat'].replace({'Less than $40K':40000, '$40K - $60K':50000, '$60K - $80K':70000, '$80K - $120K':100000, '$120K +':120000, 'Unknown':63000})\n",
    "\n",
    "    \n",
    "    # 결측치 처리\n",
    "    # ----------\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    # after_drop_cnt=len(df)\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    # ----------------\n",
    "    df = encode_onehot(df)  \n",
    "   \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 및 Test 단계"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_churner_df = pd.read_csv(\"./data/bank_churner.csv\")\n",
    "tot_cnt = len(ml_churner_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측 및 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[테스트] 2023-09-15 17:38:33, 0:00:00, [T_1], Model Name: LogisticRegression, BEST AUC: 0.871603, AUC: 0.871603\n",
      "[테스트] 2023-09-15 17:38:33, 0:00:00, [T_1], Model Name: DecisionTree, BEST AUC: 0.871603, AUC: 0.797741\n",
      "[테스트] 2023-09-15 17:38:36, 0:00:02, [T_1], Model Name: KNN, BEST AUC: 0.884643, AUC: 0.884643\n",
      "[테스트] 2023-09-15 17:38:36, 0:00:02, [T_1], Model Name: NaiveBayes, BEST AUC: 0.884643, AUC: 0.833868\n",
      "[테스트] 2023-09-15 17:40:08, 0:01:35, [T_1], Model Name: RandomForest, BEST AUC: 0.958165, AUC: 0.958165\n",
      "[테스트] 2023-09-15 17:40:11, 0:01:38, [T_1], Model Name: LightGBM, BEST AUC: 0.978527, AUC: 0.978527\n",
      "[테스트] 2023-09-15 17:40:22, 0:01:49, [T_1], Model Name: XgBoost, BEST AUC: 0.978527, AUC: 0.974055\n",
      "[테스트] 2023-09-15 17:40:51, 0:02:18, [T_1], Model Name: ExtraTrees, BEST AUC: 0.978527, AUC: 0.954647\n",
      "[테스트] 2023-09-15 17:40:51, 0:02:19, [T_1], AUC: 0.978527, tot_cnt: 8101  , after_drop_cnt : 8101  , after_smote_cnt: 10200 , X_train:(10200, 13), y_train:(10200,), X_test:(2026, 13), y_test:(2026,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b6cec_row0_col0, #T_b6cec_row0_col1, #T_b6cec_row0_col3, #T_b6cec_row1_col2, #T_b6cec_row3_col4 {\n",
       "  background-color: yellow;\n",
       "  color: black;\n",
       "}\n",
       "#T_b6cec_row0_col2, #T_b6cec_row0_col4, #T_b6cec_row1_col0, #T_b6cec_row1_col1, #T_b6cec_row1_col3, #T_b6cec_row1_col4, #T_b6cec_row2_col0, #T_b6cec_row2_col1, #T_b6cec_row2_col3, #T_b6cec_row2_col4, #T_b6cec_row3_col0, #T_b6cec_row3_col1, #T_b6cec_row3_col3, #T_b6cec_row4_col0, #T_b6cec_row4_col1, #T_b6cec_row4_col2, #T_b6cec_row4_col3, #T_b6cec_row4_col4, #T_b6cec_row5_col0, #T_b6cec_row5_col1, #T_b6cec_row5_col2, #T_b6cec_row5_col3, #T_b6cec_row5_col4, #T_b6cec_row6_col0, #T_b6cec_row6_col1, #T_b6cec_row6_col2, #T_b6cec_row6_col3, #T_b6cec_row6_col4, #T_b6cec_row7_col0, #T_b6cec_row7_col1, #T_b6cec_row7_col3, #T_b6cec_row7_col4 {\n",
       "  color: black;\n",
       "}\n",
       "#T_b6cec_row0_col6, #T_b6cec_row7_col5 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_b6cec_row2_col2, #T_b6cec_row3_col2, #T_b6cec_row7_col2 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b6cec_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th class=\"col_heading level0 col1\" >Accuracy-No</th>\n",
       "      <th class=\"col_heading level0 col2\" >Accuracy-Yes</th>\n",
       "      <th class=\"col_heading level0 col3\" >F1-Score</th>\n",
       "      <th class=\"col_heading level0 col4\" >CV AUC</th>\n",
       "      <th class=\"col_heading level0 col5\" >CV std</th>\n",
       "      <th class=\"col_heading level0 col6\" >AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row0\" class=\"row_heading level0 row0\" >LightGBM_T_1</th>\n",
       "      <td id=\"T_b6cec_row0_col0\" class=\"data row0 col0\" >94.87%</td>\n",
       "      <td id=\"T_b6cec_row0_col1\" class=\"data row0 col1\" >97.82%</td>\n",
       "      <td id=\"T_b6cec_row0_col2\" class=\"data row0 col2\" >79.38%</td>\n",
       "      <td id=\"T_b6cec_row0_col3\" class=\"data row0 col3\" >94.76%</td>\n",
       "      <td id=\"T_b6cec_row0_col4\" class=\"data row0 col4\" >99.39%</td>\n",
       "      <td id=\"T_b6cec_row0_col5\" class=\"data row0 col5\" >0.011514</td>\n",
       "      <td id=\"T_b6cec_row0_col6\" class=\"data row0 col6\" >0.978527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row1\" class=\"row_heading level0 row1\" >XgBoost_T_1</th>\n",
       "      <td id=\"T_b6cec_row1_col0\" class=\"data row1 col0\" >94.32%</td>\n",
       "      <td id=\"T_b6cec_row1_col1\" class=\"data row1 col1\" >97.00%</td>\n",
       "      <td id=\"T_b6cec_row1_col2\" class=\"data row1 col2\" >80.31%</td>\n",
       "      <td id=\"T_b6cec_row1_col3\" class=\"data row1 col3\" >94.28%</td>\n",
       "      <td id=\"T_b6cec_row1_col4\" class=\"data row1 col4\" >99.17%</td>\n",
       "      <td id=\"T_b6cec_row1_col5\" class=\"data row1 col5\" >0.015411</td>\n",
       "      <td id=\"T_b6cec_row1_col6\" class=\"data row1 col6\" >0.974055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row2\" class=\"row_heading level0 row2\" >RandomForest_T_1</th>\n",
       "      <td id=\"T_b6cec_row2_col0\" class=\"data row2 col0\" >92.65%</td>\n",
       "      <td id=\"T_b6cec_row2_col1\" class=\"data row2 col1\" >96.65%</td>\n",
       "      <td id=\"T_b6cec_row2_col2\" class=\"data row2 col2\" >71.69%</td>\n",
       "      <td id=\"T_b6cec_row2_col3\" class=\"data row2 col3\" >92.47%</td>\n",
       "      <td id=\"T_b6cec_row2_col4\" class=\"data row2 col4\" >99.28%</td>\n",
       "      <td id=\"T_b6cec_row2_col5\" class=\"data row2 col5\" >0.011294</td>\n",
       "      <td id=\"T_b6cec_row2_col6\" class=\"data row2 col6\" >0.958165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row3\" class=\"row_heading level0 row3\" >ExtraTrees_T_1</th>\n",
       "      <td id=\"T_b6cec_row3_col0\" class=\"data row3 col0\" >92.84%</td>\n",
       "      <td id=\"T_b6cec_row3_col1\" class=\"data row3 col1\" >97.41%</td>\n",
       "      <td id=\"T_b6cec_row3_col2\" class=\"data row3 col2\" >68.92%</td>\n",
       "      <td id=\"T_b6cec_row3_col3\" class=\"data row3 col3\" >92.56%</td>\n",
       "      <td id=\"T_b6cec_row3_col4\" class=\"data row3 col4\" >99.73%</td>\n",
       "      <td id=\"T_b6cec_row3_col5\" class=\"data row3 col5\" >0.003550</td>\n",
       "      <td id=\"T_b6cec_row3_col6\" class=\"data row3 col6\" >0.954647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row4\" class=\"row_heading level0 row4\" >KNN_T_1</th>\n",
       "      <td id=\"T_b6cec_row4_col0\" class=\"data row4 col0\" >84.35%</td>\n",
       "      <td id=\"T_b6cec_row4_col1\" class=\"data row4 col1\" >85.83%</td>\n",
       "      <td id=\"T_b6cec_row4_col2\" class=\"data row4 col2\" >76.62%</td>\n",
       "      <td id=\"T_b6cec_row4_col3\" class=\"data row4 col3\" >85.54%</td>\n",
       "      <td id=\"T_b6cec_row4_col4\" class=\"data row4 col4\" >96.80%</td>\n",
       "      <td id=\"T_b6cec_row4_col5\" class=\"data row4 col5\" >0.002265</td>\n",
       "      <td id=\"T_b6cec_row4_col6\" class=\"data row4 col6\" >0.884643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row5\" class=\"row_heading level0 row5\" >LogisticRegression_T_1</th>\n",
       "      <td id=\"T_b6cec_row5_col0\" class=\"data row5 col0\" >79.81%</td>\n",
       "      <td id=\"T_b6cec_row5_col1\" class=\"data row5 col1\" >80.54%</td>\n",
       "      <td id=\"T_b6cec_row5_col2\" class=\"data row5 col2\" >76.00%</td>\n",
       "      <td id=\"T_b6cec_row5_col3\" class=\"data row5 col3\" >81.83%</td>\n",
       "      <td id=\"T_b6cec_row5_col4\" class=\"data row5 col4\" >88.06%</td>\n",
       "      <td id=\"T_b6cec_row5_col5\" class=\"data row5 col5\" >0.008811</td>\n",
       "      <td id=\"T_b6cec_row5_col6\" class=\"data row5 col6\" >0.871603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row6\" class=\"row_heading level0 row6\" >NaiveBayes_T_1</th>\n",
       "      <td id=\"T_b6cec_row6_col0\" class=\"data row6 col0\" >75.27%</td>\n",
       "      <td id=\"T_b6cec_row6_col1\" class=\"data row6 col1\" >75.01%</td>\n",
       "      <td id=\"T_b6cec_row6_col2\" class=\"data row6 col2\" >76.62%</td>\n",
       "      <td id=\"T_b6cec_row6_col3\" class=\"data row6 col3\" >78.18%</td>\n",
       "      <td id=\"T_b6cec_row6_col4\" class=\"data row6 col4\" >86.94%</td>\n",
       "      <td id=\"T_b6cec_row6_col5\" class=\"data row6 col5\" >0.017155</td>\n",
       "      <td id=\"T_b6cec_row6_col6\" class=\"data row6 col6\" >0.833868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b6cec_level0_row7\" class=\"row_heading level0 row7\" >DecisionTree_T_1</th>\n",
       "      <td id=\"T_b6cec_row7_col0\" class=\"data row7 col0\" >87.56%</td>\n",
       "      <td id=\"T_b6cec_row7_col1\" class=\"data row7 col1\" >91.24%</td>\n",
       "      <td id=\"T_b6cec_row7_col2\" class=\"data row7 col2\" >68.31%</td>\n",
       "      <td id=\"T_b6cec_row7_col3\" class=\"data row7 col3\" >87.89%</td>\n",
       "      <td id=\"T_b6cec_row7_col4\" class=\"data row7 col4\" >89.80%</td>\n",
       "      <td id=\"T_b6cec_row7_col5\" class=\"data row7 col5\" >0.070841</td>\n",
       "      <td id=\"T_b6cec_row7_col6\" class=\"data row7 col6\" >0.797741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2191750c850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 저장소 초기화\n",
    "# -----------------\n",
    "model_comparison = {}  #Dictionary to store the comparison metrics of models\n",
    "model_eval_comparison = {}                        \n",
    "drop_no = 1\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "ml_churner_df = test_transform(ml_churner_df)\n",
    "after_drop_cnt = len(ml_churner_df)\n",
    "\n",
    "# ML 데이터 분리\n",
    "# --------------\n",
    "X=ml_churner_df.drop(['is_churned'],axis=1)\n",
    "y=ml_churner_df['is_churned']\n",
    "\n",
    "\n",
    "# 중요 Feature Column 선택\n",
    "# -----------------------\n",
    "# X_new, selected_columns = select_feature(X, y, 'Xg Boost')\n",
    "X_new, selected_columns = select_feature(X, y, 'ExtraTrees')\n",
    "\n",
    "\n",
    "# Train and Test 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, y_train, X_test, y_test = proc_smote(X_new, y)\n",
    "X_train_for_normalization = X_train.copy()\n",
    "after_smote_cnt = len(X_train)\n",
    "\n",
    "\n",
    "# Normalization\n",
    "# -------------\n",
    "X_train, X_test = proc_normalization(X_train, X_test)    \n",
    "\n",
    "\n",
    "# Pridict 및 Test 평가\n",
    "# --------------------\n",
    "proc_type='T'\n",
    "test_auc = fit_predict_eval(proc_type, drop_no, model_comparison, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "# 예측 및 테스트 로그 출력\n",
    "# ----------------------\n",
    "cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_time = time.time()\n",
    "delta_time = end_time - start_time\n",
    "print(f'[테스트] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], AUC: {test_auc:0.6f}, tot_cnt: {tot_cnt:<6}, after_drop_cnt : {after_drop_cnt:<6}, after_smote_cnt: {after_smote_cnt:<6}, X_train:{X_train.shape}, y_train:{y_train.shape}, X_test:{X_test.shape}, y_test:{y_test.shape}')\n",
    "\n",
    "\n",
    "print_eval_result(model_comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 단계 ~ 평가자가 Competition 평가를 위해 사용 하는 단계"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"./data/test_churner.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "# eval_df = pd.read_csv(\"./data/test_churner_kaggle_all.csv\") # 평가를 위한 데이터 로드 - 평가데이터 경로를 입력해 주세요!!!\n",
    "\n",
    "fit_df = pd.read_csv(\"./data/bank_churner.csv\") # 학습을 위한 데이터 로드\n",
    "tot_cnt = len(eval_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측 및 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[평  가] 2023-09-15 17:18:49, 0:02:17, [E_1], AUC: 0.989890, tot_cnt: 2026  , after_drop_cnt : 2026  , X_eval: (2026, 13), y_eval:(2026,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4e2d9_row0_col0, #T_4e2d9_row0_col1, #T_4e2d9_row0_col2, #T_4e2d9_row0_col3, #T_4e2d9_row2_col4 {\n",
       "  background-color: yellow;\n",
       "  color: black;\n",
       "}\n",
       "#T_4e2d9_row0_col4, #T_4e2d9_row1_col0, #T_4e2d9_row1_col1, #T_4e2d9_row1_col2, #T_4e2d9_row1_col3, #T_4e2d9_row1_col4, #T_4e2d9_row2_col0, #T_4e2d9_row2_col1, #T_4e2d9_row2_col2, #T_4e2d9_row2_col3, #T_4e2d9_row3_col0, #T_4e2d9_row3_col1, #T_4e2d9_row3_col2, #T_4e2d9_row3_col3, #T_4e2d9_row3_col4, #T_4e2d9_row4_col0, #T_4e2d9_row4_col1, #T_4e2d9_row4_col2, #T_4e2d9_row4_col3, #T_4e2d9_row4_col4, #T_4e2d9_row5_col0, #T_4e2d9_row5_col1, #T_4e2d9_row5_col2, #T_4e2d9_row5_col3, #T_4e2d9_row5_col4, #T_4e2d9_row6_col0, #T_4e2d9_row6_col1, #T_4e2d9_row6_col2, #T_4e2d9_row6_col3, #T_4e2d9_row6_col4, #T_4e2d9_row7_col0, #T_4e2d9_row7_col1, #T_4e2d9_row7_col3, #T_4e2d9_row7_col4 {\n",
       "  color: black;\n",
       "}\n",
       "#T_4e2d9_row0_col6, #T_4e2d9_row7_col5 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_4e2d9_row7_col2 {\n",
       "  color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4e2d9_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th class=\"col_heading level0 col1\" >Accuracy-No</th>\n",
       "      <th class=\"col_heading level0 col2\" >Accuracy-Yes</th>\n",
       "      <th class=\"col_heading level0 col3\" >F1-Score</th>\n",
       "      <th class=\"col_heading level0 col4\" >CV AUC</th>\n",
       "      <th class=\"col_heading level0 col5\" >CV std</th>\n",
       "      <th class=\"col_heading level0 col6\" >AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row0\" class=\"row_heading level0 row0\" >LightGBM_E_1</th>\n",
       "      <td id=\"T_4e2d9_row0_col0\" class=\"data row0 col0\" >96.25%</td>\n",
       "      <td id=\"T_4e2d9_row0_col1\" class=\"data row0 col1\" >97.53%</td>\n",
       "      <td id=\"T_4e2d9_row0_col2\" class=\"data row0 col2\" >89.60%</td>\n",
       "      <td id=\"T_4e2d9_row0_col3\" class=\"data row0 col3\" >96.27%</td>\n",
       "      <td id=\"T_4e2d9_row0_col4\" class=\"data row0 col4\" >99.39%</td>\n",
       "      <td id=\"T_4e2d9_row0_col5\" class=\"data row0 col5\" >0.011514</td>\n",
       "      <td id=\"T_4e2d9_row0_col6\" class=\"data row0 col6\" >0.989890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row1\" class=\"row_heading level0 row1\" >Xg Boost_E_1</th>\n",
       "      <td id=\"T_4e2d9_row1_col0\" class=\"data row1 col0\" >95.90%</td>\n",
       "      <td id=\"T_4e2d9_row1_col1\" class=\"data row1 col1\" >97.47%</td>\n",
       "      <td id=\"T_4e2d9_row1_col2\" class=\"data row1 col2\" >87.77%</td>\n",
       "      <td id=\"T_4e2d9_row1_col3\" class=\"data row1 col3\" >95.91%</td>\n",
       "      <td id=\"T_4e2d9_row1_col4\" class=\"data row1 col4\" >99.17%</td>\n",
       "      <td id=\"T_4e2d9_row1_col5\" class=\"data row1 col5\" >0.015411</td>\n",
       "      <td id=\"T_4e2d9_row1_col6\" class=\"data row1 col6\" >0.987490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row2\" class=\"row_heading level0 row2\" >ExtraTrees_E_1</th>\n",
       "      <td id=\"T_4e2d9_row2_col0\" class=\"data row2 col0\" >93.19%</td>\n",
       "      <td id=\"T_4e2d9_row2_col1\" class=\"data row2 col1\" >95.88%</td>\n",
       "      <td id=\"T_4e2d9_row2_col2\" class=\"data row2 col2\" >79.20%</td>\n",
       "      <td id=\"T_4e2d9_row2_col3\" class=\"data row2 col3\" >93.20%</td>\n",
       "      <td id=\"T_4e2d9_row2_col4\" class=\"data row2 col4\" >99.73%</td>\n",
       "      <td id=\"T_4e2d9_row2_col5\" class=\"data row2 col5\" >0.003550</td>\n",
       "      <td id=\"T_4e2d9_row2_col6\" class=\"data row2 col6\" >0.970017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row3\" class=\"row_heading level0 row3\" >Random Forest_E_1</th>\n",
       "      <td id=\"T_4e2d9_row3_col0\" class=\"data row3 col0\" >93.24%</td>\n",
       "      <td id=\"T_4e2d9_row3_col1\" class=\"data row3 col1\" >95.29%</td>\n",
       "      <td id=\"T_4e2d9_row3_col2\" class=\"data row3 col2\" >82.57%</td>\n",
       "      <td id=\"T_4e2d9_row3_col3\" class=\"data row3 col3\" >93.33%</td>\n",
       "      <td id=\"T_4e2d9_row3_col4\" class=\"data row3 col4\" >99.28%</td>\n",
       "      <td id=\"T_4e2d9_row3_col5\" class=\"data row3 col5\" >0.011294</td>\n",
       "      <td id=\"T_4e2d9_row3_col6\" class=\"data row3 col6\" >0.969373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row4\" class=\"row_heading level0 row4\" >KNN_E_1</th>\n",
       "      <td id=\"T_4e2d9_row4_col0\" class=\"data row4 col0\" >84.35%</td>\n",
       "      <td id=\"T_4e2d9_row4_col1\" class=\"data row4 col1\" >83.40%</td>\n",
       "      <td id=\"T_4e2d9_row4_col2\" class=\"data row4 col2\" >89.30%</td>\n",
       "      <td id=\"T_4e2d9_row4_col3\" class=\"data row4 col3\" >85.88%</td>\n",
       "      <td id=\"T_4e2d9_row4_col4\" class=\"data row4 col4\" >96.80%</td>\n",
       "      <td id=\"T_4e2d9_row4_col5\" class=\"data row4 col5\" >0.002265</td>\n",
       "      <td id=\"T_4e2d9_row4_col6\" class=\"data row4 col6\" >0.915765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row5\" class=\"row_heading level0 row5\" >Logistic Regression_E_1</th>\n",
       "      <td id=\"T_4e2d9_row5_col0\" class=\"data row5 col0\" >79.91%</td>\n",
       "      <td id=\"T_4e2d9_row5_col1\" class=\"data row5 col1\" >79.40%</td>\n",
       "      <td id=\"T_4e2d9_row5_col2\" class=\"data row5 col2\" >82.57%</td>\n",
       "      <td id=\"T_4e2d9_row5_col3\" class=\"data row5 col3\" >82.07%</td>\n",
       "      <td id=\"T_4e2d9_row5_col4\" class=\"data row5 col4\" >88.06%</td>\n",
       "      <td id=\"T_4e2d9_row5_col5\" class=\"data row5 col5\" >0.008811</td>\n",
       "      <td id=\"T_4e2d9_row5_col6\" class=\"data row5 col6\" >0.881188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row6\" class=\"row_heading level0 row6\" >Naive Bayes_E_1</th>\n",
       "      <td id=\"T_4e2d9_row6_col0\" class=\"data row6 col0\" >75.42%</td>\n",
       "      <td id=\"T_4e2d9_row6_col1\" class=\"data row6 col1\" >75.46%</td>\n",
       "      <td id=\"T_4e2d9_row6_col2\" class=\"data row6 col2\" >75.23%</td>\n",
       "      <td id=\"T_4e2d9_row6_col3\" class=\"data row6 col3\" >78.24%</td>\n",
       "      <td id=\"T_4e2d9_row6_col4\" class=\"data row6 col4\" >86.94%</td>\n",
       "      <td id=\"T_4e2d9_row6_col5\" class=\"data row6 col5\" >0.017155</td>\n",
       "      <td id=\"T_4e2d9_row6_col6\" class=\"data row6 col6\" >0.833593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4e2d9_level0_row7\" class=\"row_heading level0 row7\" >Decision Tree_E_1</th>\n",
       "      <td id=\"T_4e2d9_row7_col0\" class=\"data row7 col0\" >86.67%</td>\n",
       "      <td id=\"T_4e2d9_row7_col1\" class=\"data row7 col1\" >89.11%</td>\n",
       "      <td id=\"T_4e2d9_row7_col2\" class=\"data row7 col2\" >74.01%</td>\n",
       "      <td id=\"T_4e2d9_row7_col3\" class=\"data row7 col3\" >87.35%</td>\n",
       "      <td id=\"T_4e2d9_row7_col4\" class=\"data row7 col4\" >89.80%</td>\n",
       "      <td id=\"T_4e2d9_row7_col5\" class=\"data row7 col5\" >0.070841</td>\n",
       "      <td id=\"T_4e2d9_row7_col6\" class=\"data row7 col6\" >0.815587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2197598ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------------    \n",
    "# 평가 for Competition\n",
    "# -----------------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# 전처리 단계\n",
    "# -----------\n",
    "fit_df = test_transform(fit_df)\n",
    "eval_df = test_transform(eval_df)\n",
    "after_drop_cnt = len(eval_df)\n",
    "\n",
    "    \n",
    "# 평가를 위한 데이터 분리\n",
    "# ---------------------\n",
    "X_train=fit_df.drop(['is_churned'],axis=1)\n",
    "y_train=fit_df['is_churned']\n",
    "\n",
    "X_eval=eval_df.drop(['is_churned'],axis=1)\n",
    "y_eval=eval_df['is_churned']\n",
    "\n",
    "\n",
    "# 중요 Feature Column 선택\n",
    "# -----------------------\n",
    "X_new, selected_columns = select_feature(X_train, y_train, 'ExtraTrees')\n",
    "X_eval = X_eval[selected_columns]\n",
    "\n",
    "\n",
    "# Train and Test 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, y_train, X_test_temp, y_test_temp = proc_smote(X_new, y_train)\n",
    "\n",
    "\n",
    "# Evaluation 데이터 생성 및 가공\n",
    "# ---------------------------------\n",
    "X_train, X_eval = proc_normalization(X_train, X_eval.values)   \n",
    "\n",
    "\n",
    "# 최종 평가\n",
    "# --------\n",
    "proc_type='E'\n",
    "# eval_auc = fit_predict(proc_type, drop_no, model_eval_comparison, X_train_for_evaluation, y_train_for_evaluation, X_eval, y_eval)\n",
    "eval_auc = fit_predict_eval(proc_type, drop_no, model_eval_comparison, X_train, y_train, X_eval, y_eval)\n",
    "\n",
    "\n",
    "# 최종 평가 로그 출력\n",
    "# ------------------\n",
    "cur_datetime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_time = time.time()\n",
    "delta_time = end_time - start_time\n",
    "# print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, AUC: {test_auc:0.6f}, 처리 건수: {len(eval_df)}, 최종 평가 건수: {len(X_eval)}')\n",
    "print(f'[평  가] {cur_datetime}, {str(datetime.timedelta(seconds=delta_time)).split(\".\")[0]}, [{proc_type}_{drop_no}], AUC: {eval_auc:0.6f}, tot_cnt: {tot_cnt:<6}, after_drop_cnt : {after_drop_cnt:<6}, X_eval: {X_eval.shape}, y_eval:{y_eval.shape}')\n",
    "\n",
    "\n",
    "print_eval_result(model_eval_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
