# 최초 root / adminuser

# 파일럿 환경의 로그 확인
  - Hadoop 에코시스템 서버들의 로그 위치 : /var/log/디렉터리(cloudera, Hadoop, Oozie 등)
  - Redis 서버 로그 위치 : /var/log/redis_6379.log
  - Storm 서버 로그 위치 : /home/pilot-pjt/storm/logs
  - Zeppelin 서버 로그 위치 : /home/pilot-pjt/zeppelin-0.8.2-bin-all/logs
  

# 의존성 : 시작 순서
  - zookeeper -> hadoop -> yarn
### 2-5 빅데이터 클러스터 구성 02
## 설치 위치
# HDFS(하둡)
- 설치 위치
  - NameNode : server01.hadoop.com
  - SecondaryNameNode : server01.hadoop.com
  - Balancer : server01.hadoop.com
  - HttpFS : None
  - NFS Gateway : None
  - DataNode : server02.hadoop.com

# Cloudera Management Service
- 설치 위치
  - Service Monitor : server01.hadoop.com
  - Activity Monitor : None
  - Host Monitor : server01.hadoop.com
  - Event Server : server01.hadoop.com
  - Alert Publisher : server01.hadoop.com 
  
# YARN
- 설치 위치
  - ResourceManger : server01.hadoop.com
  - JobHistory Server : server01.hadoop.com
  - NodeManager : server02.hadoop.com

# 주키퍼
- 설치 위치
  - Server : server02.hadoop.com

## 환경 설정
# Hadoop 
  - 복제 계수 : 3 -> 1
  - 접근권한 해제(HDFS 권한 검사) : 체크박스를 해제
  - HDFS 블록 크기 : 128MiB -> 64MiB

# YARN
  - yarn.scheduler.max : 1GiB -> 1.5GiB
  - yarn.nodemanger.resource.memory-mb : 1GiB -> 5GiB
  - Scheduler 클래스(yarn.resourcemanager.scheduler.class : FifoScheduler로 변경 


# 실무에서 Hadoop 디렉토리 하위 경로의 복제계수를 10으로 변경하면 조회시 속도 향상. 물론 자원의 소비는 감안
$ hadoop fs -setrep 10 -R /user/hadoop/

### 2-5. 빅데이터 클러스터 구성
## HDFS 명령어 사용(server02.hadoop.com)
$ hdfs dfs -put Sample.txt /tmp
$ hdfs dfs -ls /tmp
$ hdfs dfs -cat /tmp/Sample.txt
$ hdfs dfs -mv /tmp/Sample.txt /tmp/Sample2.txt
$ hdfs dfs -get /tmp/Sample2.txt   # HDFS에 저장된 파일을 로컬 파일시스템으로 가져오기
$ hdfs dfs -rm /tmp/Sample2.txt    # HDFS에 저장된 파일 삭제(휴지통)
$ hdfs dfs -rm -skipTrash /tmp/Sample2.txt    # HDFS에 저장된 파일 삭제

$ hdfs dfs -stat '%b %o %r %n' /tmp/Sample.txt  # hdfs 저장된 파일 상태 확인
                                                # 파일크기(%b), 파일 블록크기(%o), 복제수(%r), 소유자명(%u), 파일명(%n)
$ hdfs fsck /         # HDFS 파일 시스템 상태 검사, 전체크기, 디렉러리 수, 파일수, 노드수 등 파일시스템의 전체 상태를 보여줌
$ hdfs dfsadmin -report  # 하둡 파일싯템의 기본 정보 및 통계를 보여줌

# HDFS 파일의 비정상 상태
$ hdfs dfsadmin -safemode leave    # 안전모드 상태로 전환됐다면 강제로 안전모드를 해제
$ hdfs fsck / -delete              # 손상된 파일을 강제로 삭제
$hdfs fsck / -move                 # 손상된 파일을 /lost+found 디렉터리로 옮김


## 주키퍼(zookeeper) 명령어 사용
$ zookeeper-client     # 주키퍼 클라이언트 실행

# 주키퍼 Z노드 등록/조회/삭제
[zk: localhost:2181(CONNECTED) 0] create /pilot-pjt bigdata # Z노드 생성
[zk: localhost:2181(CONNECTED) 0] ls /                      # Z노드 조회  
[zk: localhost:2181(CONNECTED) 0] get /pilot-pjt
[zk: localhost:2181(CONNECTED) 0] delete /pilot-pjt         # Z노드 삭제

### 스마트카 로그 시뮬레이터 - 설치 및 실행
# 1. 작업폴더 생성
  - $ cd /home
  - $ mkdir /home/pilot-pjt
  - $ mkdir /home/pilot-pjt/car-batch-log
  - $ mkdir /home/pilot-pjt/driver-realtime-log
  - chmod 777 -R /home/pilot-pjt

# 2. java 컴파일과 실행환경을 1.7에서 1.8로 변경
  - $ rm /usr/bin/java
  - $ rm /usr/bin/javac
  - $ ln -s /usr/java/jdk1.8.0_181-cloudera/bin/javac /usr/bin/javac
  - $ ln -s /usr/java/jdk1.8.0_181-cloudera/bin/java  /usr/bin/java
  - $ java -version
  
# 3. 시뮬레이터 실행
  - 실시간 로그(Driver Log)
    - $ java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20160101 3  
  - 배치 로그(Car Log)
    - $ java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20160101 3  
  
### 3.2 수집에 활용할 기술 : 플럼(flume)
# 플럼 설치
  - 설치 위치 : server02.hadoop.com

# 플럼 환경 설정
  - java heap : 50MiB -> 100MiB

# 플럼 동작 
# Source -> Channer -> Sink
# Source -> Interceptor -> Channel -> Sink(2개)

# 플럼 동작 
  - Flume [구성] - 구성파일 편집


### 3.3 수집에 활용할 기술 : 카프카(kafka)
# 설치 위치
  - kafka Broker : server02.hadoop.com
  - kafka MirrorMaker : None
  - Gateway : None

# 환경 설정
  - Data Retention Time : 7일 -> 10분
  
# 카프카 토픽 생성
  - $ kafka-topics --create --zookeeper server02.hadoop.com:2128 --replication-factor 1 --partitions 1 --topic SmartCar-Topic 

# 카프카 Producer 사용
  - kafka-console-producer --broker-list server02.hadoop.com:9092 -topic SmartCar-Topic
  
# 카프카 Consumer1 사용
  - kafka-console-consumer --bootstrap-server server02.hadoop.com:9092 --topic SmartCar-Topic --partition 0 --from-beginning

# 카프카 Consumer2 사용  
  - kafka-console-consumer --bootstrap-server server02.hadoop.com:9092 --topic SmartCar-Topic --partition 0 --from-beginning  
  
# Provider(N) -> 카프카 Broker(Topic1, Topic2) -> Consumer(N개)


3.7  수집 파일럿 실행 5단계
## SmartCar 로그 시뮬레이터 작동
$ cd /home/pilot-pjt/working

$ java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.CarLogMain 20160101 3 &

$ java -cp bigdata.smartcar.loggen-1.0.jar com.wikibook.bigdata.smartcar.loggen.DriverLogMain 20160101 3 &

$ mv /home/pilot-pjt/working/SmartCar/SmartCarStatusInfo_20160101.txt /home/pilot-pjt-working/car-batch-log/


### 5.4 실시간 적재 파일럿 실행 2단계 - 환경구성
## HBase
# HBase 설치
  - Master : server01.hadoop.com  
  - HBase REST Server : None
  - HBase Thrift Server : server01.hadoop.com  
  - RegionServer : server01.hadoop.com  


# HBase 구성
  - HBase Thrift Http 서버 설정  : "HBase(서비스 전체)" 항목의 체크박스에 Check
  

# HBase 사용
$ hbase shell
> create 'smartcar_test_table', 'cf'
> put 'smartcar_test_table', 'row-key1', 'cf:model', 'Z0001'
> put 'smartcar_test_table', 'row-key1', 'cf:no', 	'12345'
> get 'smartcar_test_table', 'row-key1'

> disable 'smart_test_table'
> drop 'smart_test_table'
> exit

# HBase 웹 관리자 화면
- http://server02.hadoop.com:16010

# HBase는 자원 소모가 높은 서버이므로 사용하지 않을때에는 일시정지.

# 간단한 HBase 명령어
$ hbase shell
  > create 'smartcar_test_table', 'cf'    # 테이블 및 컬럼패밀리 생성
  > put 'smartcar_test_table', 'row-ke1','cf:model','z0001'  # 데이터 insert
  > get 'smartcar_test_table', 'row-ke1'  # 데이터 조회
  > disable 'smartcar_test_table'            # 테이블 삭제전 disable
  > drop 'smartcar_test_table'               # 테이블 drop
  
  > list_snapshots
  > delete_snapshots 'snapshot 이름'
  > list                              # 테이블 리스트 
  > disable 'table_name'
  > drop 'table_name'
  > count 'table_name'
  > get 'table_name', 'key'
  > deleteall 'table_name','key'
  > scan 'table_name', {STARTROW=>'1234', ENDROW=>'1235'}
  > scan 'table_name', {STARTROW=>'1234', LIMIT=>10}
  > scan 'table 이름', {COLUMNS=>'패밀리이름:컬럼이름', LIMIT=>10, FILTER=>"ValueFilter(=, 'binary:조건값')"
  > scan 'table 이름', {LIMIT=>10, COLUMNS=>['패밀리네임'], FILTER => "(SingleColumnValueFilter('패밀리네임', '필드명',=,'binary:조건값1',true, true)) AND (SingleColumnValueFilter('패밀리네임', '필드명',=,'binary:조건값2',true,true))"}
  > scan 'table 이름', {LIMIT=>10, COLUMNS=>['패밀리네임:필드명'], TIMERANGE=> [start_timestamp, end_timestamp]} 


## Redis 설치
$ yum install -y gcc*
$ yum install -y tcl

$ cd /home/pilot-pjt
$ wget http://download.redis.io/releases/redis-5.0.7.tar.gz
$ tar -xvf redis-5.0.7.tar.gz

$ cd /home/pilot-pjt/redis-5.0.7
$ make
$ make install
$ cd /home/pilot-pjt/redis-5.0.7/utils
$ chmod 755 install_server.shell
$ ./install_server.sh

# Redis 설치 확인
$ vi /var/log/redis_6379.log

$ service redis_6379 status [start/stop]

# Redis 서버에 원격접근 가능토록 설정
$ vi /etc/redis/6379.conf

  -> bind 127.0.0.1 부분을 주석처리
  -> protected-mod yes -> no
$ service redis_6379 restart

# Redis CLI 로 데이터 저장/조회
$ redis-cli
> set key:1 Hello!BigData
> get key:1
> del key:1
> quit

## Storm 설치 (Server02)
$ cd /home/pilot-pjt
$ wget https://archive.apache.org/dist/storm/apache-storm-1.2.3/apache-storm-1.2.3.tar.gz
$ tar -xvf apache-storm-1.2.3-tar.gz
$ ln -s apache-storm-1.2.3 storm

# Storm 환경설정
$ cd /home/pilot-pjt/storm/conf

$ vi storm.yaml
  > storm.zookeeper.servers:                       # zookeeper 정보
  >  - "server02.hadoop.com"

  > storm.local.dir: "/home/pilot-pjt/storm/data"  # 스톰 작동을 위한 데이터 저장소

  > nimbus.seeds: ["server02.hadoop.com"]          # Nimbus 정보

  > supervisor.slots.ports:                        # Worker의 포트로써 포트의 갯수만큼 Worker가 만들어짐
     - 6700

  > ui.port: 8088                                  # Storm UI 접속 포트

# Storm 로그 레벨 조정 (info(INFO) -> error
$ cd /home/pilot-pjt/storm/log4j2
$ vi cluster.xml    # info 를 ERROR 로 변경
$ vi worker.xml     # INFO 를 ERROR 로 변경

# Storm을 편리하게 사용하기 위한 설정
$ vi /root/.bash_profile
  > PATH=$PATH:/home/pilot-pjt/storm/bin

$ source /root/.bash_profile

## Storm 설치
# Java 환경 설정(jdk1.8)
$ java -version
$ rm /usr/bin/java
$ rm /usr/bin/javac
$ ln -s /usr/java/jdk1.8.0_181-cloudera/bin/javac /usr/bin/javac
$ ln -s /usr/java/jdk1.8.0_181-cloudera/bin/java /usr/bin/java

# 자동 기동 스크립트 설정 (storm-nimbus, storm-supervisor, storm-ui)
https://gist.github.com/yulrizka 에서 스크립트 다운로드후 /etc/rc.d/init.d 에 저장

$ chmod 755 /etc/rc.d/init.d/storm-nimbus
$ chmod 755 /etc/rc.d/init.d/storm-supervisor
$ chmod 755 /etc/rc.d/init.d/storm-ui

# 서비스 등록 스크립트에 대한 Log 및 Pid 디렉터리를 만듬
$ mkdir /var/log/storm
$ mkdir /var/run/storm

$ service storm-nimbus [storm-supervisor/storm-ui] start/stop/status

# Storm UI 접속 및 상태 모니터링
- http://server02.hadoop.com:8088

# 스톰도 주키퍼 의존도가 높다. 주키퍼의 z노드인 /storm의 위치에 스톰의 주요 설정값이 관리되고 있음.

# Storm 기동 순서 : storm-nimbus -> storm-supervisor -> storm-ui (중지는 반대로)



# HBase Region 스플릿
- 저사양 파일럿 환경 : 리전 스플릿 수를 2로 설정(yarn 기반)
$ hbase org.apache.hadoop.hbase.util.RegionSplitter DriverCarInfo HexStringSplit -c 2 -f cf1



5.5 실시간 적재 파일럿 실행 3단계 - 적재 기능
# Storm Topology 배포
1. bigdata.smartcar.storm-1.0.jar 를 /home/pilot-pjt/working 에 업로드
2. storm 명령을 통해 DriverCarInfo라는 이름으로 배포. 배포하기 전 Storm 실행 확인
$ cd /home/pilot-pjt/working
$ storm jar bigdata.smartcar.storm-1.0.jar com.wikibook.bigdata.smartcar.storm.SmartCarDriverTopology DriverCarInfo


# Storm Topology 제거 
$ storm kill "배포시 사용했던 Topology 이름" -> $ storm kill "DriverCarInfo"

# storm topology 확인
- http://server02.hadoop.com:8088 로 확인
 

# 적재 테스트
1. 로그 시뮬레이터 작동

2. HBase 적재 확인
$ hbase shell
  > count 'DriverCarInfo'   # DriverCarInfo 테이블에 적재된 데이터 로우 수를 1000 단위로 출력
  > scan 'DriverCarInfo', {LIMIT=>20}  # 20개 데이터만 조회
  > scan 'DriverCarInfo', {STARTROW=>'00001030106102-Z0020', LIMIT=>1}
  > scan 'DriverCarInfo', {COLUMNS=>['cf1:car_number','cf1:area_numbr'], FILTER=>"RowFilter(=,'regexstring:30106102') AND SingleColumnValueFilter('cf1', 'area_numbr' ,=, 'regexstring:D04')"}  

3. Hbase 웹관리자에 접속해서 적재한 데이터가 분산 적재되는지 확인
- http://server02.hadoop.com:16010/


# Redis에 적재된 데이터 확인
$ redis-cli
$ 127.0.0.1:6379> smembers 20160103

5.6 실시가 적재 파일럿 실행 4단계 - 적재 테스트
# Redis 클라이언트 애플리케이션 작동
- bigdata.smartcar.redis-1.0.jar를 /home/pilot-pjt-working 으로 업로드

- 레디스 클라이언트 애플리케이션 실행 - redis에 있는 데이터를 가져와서 과속한 차량에 경고, 메시지를 보낼 수 있음
$ cd /home/pilot-pjt/working
$ java -cp bigdata.smartcar.redis-1.0.jar com.wikibook.bigdata.smartcar.redis.OverSpeedCarInfo 20160103

# 실시간 로그 시뮬레이터 중지
$ ps -ef | grep smartcar.log/redis_6379
$ kill -9 [pid]

# 적재 테스트가 종료되면 원할한 자원 관리를 위해 수집/적재 서비스를 정지 시킴.
# 아래 순서로 정지
- 플럼 서비스 : CM홈 -> [Flume] -> [정지]
- 카프카 서비스: CM홈 -> [Kafka] -> [정지]
- 스톰 서비스: server02 접속
$ service storm-ui stop
$ service storm-supervisor stop
$ service storm-nimbus stop
- 레디스 서비스: server02에 ssh 접속 
$ service redis_6379 stop
- HBase 서비스: CM홈 -> [HBase] -> [정지]


6. 빅데이터 탐색

## pig
- 실무로 배우는 빅데이터 기술(확장편): https://bit.ly/bigdata2nd


6.4 탐색 파일럿 실행 2단계 - 탐색 환경 구성

### 미사용 수집/적재 서비스 중지	
  - 플럼 서비스 : CM 홈 -> [Flume] -> [정지]
  - 카프카 서비스 : CM 홈 -> [Kafka] -> [정지]
  - 스톰 서비스 : server02.hadoop.com에 접속하여 정지
    $ service storm-ui stop
    $ service storm-supervisor stop
    $ service storm-ui stop	


### 탐색 소프트웨어 설치 및 설정
## Hive 설치
# 설치 방법
  - CM을 이용하여 Hive 서비스 추가
  - Hive 작동에 필요한 의존성 선택
  - Select Dependendencies(Hbase, HDFS, YARN(MR2 included), Zookeeper) 선택
  - 설치 프로그램 위치
    - Gateway : server02.hadoop.com
    - Hive MetaStore Server : server02.hadoop.com
    - WebHCat Server : None
    - HiveServer2 : server02.hadoop.com
  - Hive의 MetaStore로 사용되는 데이터베이스 설정 
    - 내장된 데이터베이스 사용 
    - PostgreSQL, server01.hadoop.com:7432, hive, hive (defalut)
  - Hive의 데이터웨어하우스 디렉토리와 메타스터어의 포트번호를 기본값으로 유지한 상태로 진행
  - Hive 설치 진행됨
  
  - HIVE 암호 : hive/lqvbwxaLFz 

## Oozie 설치
# 설치 방법
  - CM을 이용하여 Hive 서비스 추가
  - Oozie 작동에 필요한 의존성 선택
    - Select Dependendencies(Hbase, HDFS, Hive, YARN(MR2 included), Zookeeper) 선택
  - 설치 프로그램 위치
    - Oozie Server : server02.hadoop.com
  - Oozie에서 사용하는 데이터베이스 설정  
    - 내장된 데이터베이스 사용 
    - PostgreSQL, server01.hadoop.com:7432, oozie_oozie_server, oozie_oozie_server (defalut)
  - Oozie의 변경 내용 검토 : default로 진행 
  - Oozie 설치 진행됨

# Oozie 설정
  - Launcher Memory : 2GB -> 1GB

## Hue 설치 (admin/admin)

# 설치 방법
  - yum mirror list 설정	
    - $ echo "http://vault.centos.org/6.10/os/x86_64/" > /var/cache/yum/x86_64/6/base/mirrorlist.txt
	- $ echo "http://vault.centos.org/6.10/extras/x86_64/" > /var/cache/yum/x86_64/6/extras/mirrorlist.txt
    - $ echo "http://vault.centos.org/6.10/updates/x86_64/" > /var/cache/yum/x86_64/6/updates/mirrorlist.txt
    - $ echo "http://vault.centos.org/6.10/sclo/x86_64/rh" > /var/cache/yum/x86_64/6/centos-sclo-rh/mirrorlist.txt
    - $ echo "http://vault.centos.org/6.10/sclo/x86_64/sclo" > /var/cache/yum/x86_64/6/centos-sclo-sclo/mirrorlist.txt

  - 휴를 설치하기 위해서는 Python 2.7이 설치되어 있어야 함 - server02 서버에 설치
    - $ yum install centos-release-scl
	- $ yum install scl-utils
	- $ yum install python27
	- $ source /opt/rh/python27/enable  # 
	- $ python --version

  - 파이썬 패키지인 psycopg2 설치 
    - $ yum --enablerepo=extras install epel-release   # epel 저장소 활성화
    - $ yum install python-pip
	- $ yum install postgresql-devel
	- bash -c "source /opt/rh/python27/enable; pip install psycopg2==2.6.2 --ignore-installed"

  - CM을 이용하여 Hue 서비스 추가
  - Hue 작동에 필요한 의존성 선택
    - Select Dependendencies(Hbase, HDFS, Hive, Oozie, YARN(MR2 included), Zookeeper) 선택
  - 설치 프로그램 위치
    - Hue Server : server02.hadoop.com
	- Load Balancer : None
  - 데이터베이스 설정 : default로 계속 진행	
  - Hue 설치 진행됨

# Hue 설정
  - Time Zone 변경 
    - "시간대" : America/Los_Angeles -> Asia/Seoul
  - HBase 브라우저를 사용하기 위한 옵션 설정 
    - Hbase Thrift 서버 : HBase Thrift Server(server02) 선택  
	 
## Spark 설치
# 설치 방법
  - CM을 이용하여 Hue 서비스 추가
  - Spark 작동에 필요한 의존성 선택
    - Select Dependendencies(Hbase, HDFS, YARN(MR2 included), Zookeeper) 선택
  - 설치 프로그램 위치
    - History Server : server02.hadoop.com
	- Gateway : server02.hadoop.com
  - Spark 설치 진행됨
  - 스파크를 YARN에서 작동하도록 구성했으므로 YARN 서비스와 스파크를 재시작
    - YARN 재시작 : CM 홈 [YARN] 선택후 재시작
	- Spark 클라이언트 구성 배포 : [클라이언트 구성 배포] 아이콘을 선택/실행 
	- Spark 시작 : CM홈 에서 Spark 시작
  - 스파크 정상 설치 확인위해 Spark history 서버에 접속하여
    - 스파크 히스토리 서버 : http://server02.hadoop.com:18088/
	

6.5 탐색 파일럿 실행 3단계 - 데이터 탐색 & 처리1

### HDFS에 적재된 데이터 확인 
 - 휴 접속 : http://server02.hadoop.com:8888
         : CM 홈 -> [Hue] -> 상단의 Hue 웹 UI
 - 좌측 상단의 드롭다운 메뉴 클릭 -> [파일] 
 
### HBase에 적재된 데이터 확인 
 - 좌측 상단의 드롭다운 메뉴 클릭 -> [HBase] 

6.5 탐색 파일럿 실행 3단계 - 데이터 탐색 & 처리2
  - Hive는 MapReduce를 통해서 쿼리 수행
  
### Hive 를 이용한 External 데이터 탐색(1/5)
  - Hive는 Hadoop에 있는 데이터를 탐색하는 도구
  - Hive 쿼리를 이용해 스마트카 상태 정보를 탐색하기 위한 하이브의 External 테이블을 생성하고 실행
    - c:/bigdata-master/CH06/HiveQL/그림-6.44.hql

### Hive 를 이용한 External 데이터 탐색(2/5)
  - "스마트카 상태 정보"를 위한 하이브 테이블을 생성했으면 이제 작업일자를 기준으로 파티션 정보를 생성
  - 플럼을 통해 HDFS로 적재한 데이터를 Alter table 명령을 실행해 작업 일자 기준으로 파티션 정보를 추가 
  - 작업했던 날짜로 입력(wrk_date=20210508)
  - ALTER TABLE SmartCar_Status_Info ADD PARTITION(wrk_date='20210508');
  
### Hive 를 이용한 External 데이터 탐색(3/5)
  - Hive의 External 테이블로 만들어진 SmartCar_Status_Info 테이블에서 단순 SELECT 쿼리 실행 
    (참고로, Ctrl + Enter 단축키를 이용하면 편집기 상에서 커서가 위치한 곳의 쿼리가 실행됨)
  - select * from SmartCar_Status_Info limit 5;

### Hive 를 이용한 External 데이터 탐색(4/5)
  - 조건절을 포함하는 하이브 쿼리 실행. 
  - select car_number, avg(battery) as battery_avg
      from SmartCar_Status_Info
	 where battery < 60
	 group by car_number;

### Hive 를 이용한 External 데이터 탐색(5/5)
  - 실행된 하이브 쿼리 모니터링은 출력되는 로그창을 통해서도 확인할 수 있지만 [Job Browser] 메뉴를 통해
    맵리듀스로 변환되어 실행된 Job 정보도 확인할 수 있음


6.5 탐색 파일럿 실행 3단계 - 데이터 탐색 & 처리3
### 하이브 이용한 HBase 데이터 탐색(1/4)
  - HBase 서비스를 시작 - CM 홈 -> [HBase] -> [시작]
  - HBase 데이터 조회 방법
    - HBase Shell을 이용하는 방법
	- Hue에서 HBase 조회하는 방법
	- 두 방법다 뭔가 불편함. HBase를 SQL처럼 사용하면 좋을 것 같다. Hive에서 HBaseStorageHandler를 이용하여 조회하면 유용 
	
### 하이브 이용한 HBase 데이터 탐색(2/4)
  - 휴 상단 메뉴의 쿼리 콤보박스에서 [편집기] -> [Hive]를 선택해 하이브의 쿼리 편집기로 이동.
  - 하이브의 External 테이블을 생성하는데, HBaseStorageHandler를 이용해 SmartCar_Drive_Info 테이블 생성 
  - create external table smartcar_drive_info(
      r_key string,
	  r_date string,
	  car_number string,
	  speed_pedal string,
	  break_pedal string,
	  steer_angle string,
	  direct_light string,
	  speed string,
	  area_number string)
	stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
	with serdeproperties (
	  "hbase.columns.mapping" = "cf1:date, cf1:car_number,
	                             cf1:speed_pedal,
								 cf1:break_pedal,
								 cf1:steer_angel,
								 cf1:direct_light,
								 cf1:speed,
								 cf1:area_number")
	tableproperties(
	  "hbase.table.name" = "DriverCarInfo");
	
### 하이브 이용한 HBase 데이터 탐색(3/4)
  - 위의 쿼리가 실행되면 SmartCar_Drive_Info 테이블의 정보가 메타스토어에 등록되고, 하이브 쿼리를 통해 HBase에 적재된 테이블을 탐색할 수 있다.
  
### 하이브 이용한 HBase 데이터 탐색(4/4)
  - HBase 서비스를 정지시킨다

6.5 탐색 파일럿 실행 3단게 - 데이터 탐색 & 처리 4
### 데이터셋 추가
  - 스마트카 마스터 데이터 와 스마트카 차량용품 구매 이력 데이터를 추가 적재한다.
    외부 파일(CarMaster.txt, CarItemBuyList_202003.txt)을 휴 파일 브라우저 업로드 기능을 이용해
	두 개의 파일을 적재하고 하이브 External Table로 정의한다.
  - 휴를 통해서 파일을 HDFS에 직접 저장하는 방식을 실행

  1. 휴의 파일브라우저 기능을 실행. 디렉터리 위치를 /pilot-pjt/collect로 이동
  2. /pilot-pjt/collect 위치에서 "car-master" 디렉터리를 생성한후 "car-master" 로 이동
  3. CH06/CarMaster.txt 파일을 HDFS의 /pilot-pjt/collect/car-master/ 경로에 업로드 
  4. 휴의 파일 브라우저에서 /pilot-pjt/collect/car-master/CarMaster.txt를 열어본다
  5. CH06/CarItemBuyList_202003.txt 파일을 /pilot-pjt/collect/buy-list/ 경로에 업로드 
  6. 추가한 두개의 데이터 셋인 "스마트카 마스터"와 "스마트카 상품 구매 이력"을 하이브의 External 테이블로
     정의한다. 휴 상단메뉴의 [Query Editor] -> [Hive]를 선택해 하이브의 쿼리 편집기로 이동한후 
	 스마트카 마스터 테이블인 smartcar_master를 생성한다.
  7. "스마트카 마스터" 테이블의 데이터를 하이브 쿼리를 이용해 확인한다.
     - select * from smartcar_master;
  8. "스마트카 차량용품 구매이력"에 대해 하이브 테이블인 smartcar_item_buylist를 생성한다.
  9. 하이브 쿼리를 이용해 확인
     - select * from smartcar_item_buylist limit 10;
  
	 
6.5 탐색 파일럿 실행 3단계 - 데이터 탐색 & 처리 5
### Spark를 이용한 추가 데이터셋 탐색
  1. 스파크가 설치된 server02에 접속하여 스파크 쉘을 실행. 
     정상적인 스파크 쉘 기동이 완료되면 "scala>" 프롬포트가 나타난다.
     - $ spark-shell  
	 
  2. Spark-SQL 컨텍스트를 이용해 하이브에서 생성한 "스마트카 마스터 데이터"인 samrtcar_master 테이블을 조회할 수 있다.
     "age >= 18" 조건으로 spark-SQL 컨텍스트를 정의해 스파크 DataFrame 변수인 smartcar_master_df에 할당한다.
     - scala> val smartcar_master_df = spark.sqlContext.sql("select * from smartcar_master where age >= 18")
	 
  3. 스파크-SQL로 age가 18 으로 만들어진 DataFrame을 출력한다. 상위 20개의 항목이 표시되고, age 필드를 보면
     18 미만인 데이터는 보이지 않는다.
     - scala> smartcar_master_df.show()
	 
  4. age가 18세 이상으로 정제된 데이터셋을 하이브의 Managed 테이블인 SmartCar_Master_Over18에 별도로 저장한다.
     - scala> smartcar_master_df.write.saveAsTable("SmartCar_Master_Over18")
	   -> Hive DataWareHouse(DW)의 Managed 영역으로 저장하는 거임.
	   
  5. 휴의 [Query 편집기] -> [Hive]로 이동해서 스파크-SQL에서 만든 테이블인 smartcar_master_over18이 생성되었는지
     확인하고 smartcar_master_over18에 "age > 18"에 해당하는 데이터만 존재하는지 하이브 QL로 조회해 본다.
     - select * from smartcar_master_over18 where age > 18 limit 10;
	 
  6. 동일 쿼리를 파일럿 환경의 하이브와 스파크에서 각각 실행했을 때 수행시간을 비교.
     - 실행 쿼리 : select * from smartcar_master_over18 where age > 18 and sex = '남';
       - Hive : 62초, Spark : 20초


6.6 탐색 파일럿 실행 4단계 - 01. 탐색 주제영역
### 탐색 주제영역
  - 실제 환경에서는 데이터웨어하우스를 구축하고 데이터 마트를 구축해야 하나 파일럿 환경에서는 lake영역에서 바로 데이터 마트로 구축 함.
  - 1. 스마트카 상태 모니터링 정보
       - 이용할 테이블 
	     - 스마트카 마스터 데이터 : smartcar_master_over18)
		 - 스마트카 상태 정보 데이터 :(smartcar_status_info)
	   - 워크플로 이름 : subject1-workflow
	   - 스케줄러 이름 : subject1-coordinator
	   - 수행 주기 : 01:00 / 1day
	   - 생성할 마트 테이블 : managed_smartcar_status_info
  
  - 2. 스마트카 운전자 운행기록 정보
       - 이용할 테이블 
	     - 스마트카 마스터 데이터(smartcar_master_over18)
		 - 스마트카 운전자 운행 데이터(smartcar_drive_info_2)
	   - 워크플로 이름 : subject2-workflow
	   - 스케줄러 이름 : subject2-coordinator
	   - 수행 주기 : 02:00 / 1day
	   - 생성할 마트 테이블 : managed_smartcar_drive_info

  - 3. 이상 운전 패턴 스마트카 정보 
       - 이용할 테이블 
	     - 스마트카 운전자 운행기록 정보(managed_smartcar_drive_info)
	   - 워크플로 이름 : subject3-workflow
	   - 스케줄러 이름 : subject3-coordinator
	   - 수행 주기 : 03:00 / 1day
	   - 생성할 마트 테이블 : managed_smartcar_symptom_info

  - 4. 긴급 점검이 필요한 스마트카 정보 
       - 이용할 테이블 
	     - 스마트카 상태 모니터링 정보(managed_smartcar_status_info)
	   - 워크플로 이름 : subject4-workflow
	   - 스케줄러 이름 : subject4-coordinator
	   - 수행 주기 : 04:00 / 1day
	   - 생성할 마트 테이블 : managed_smartcar_emergency_check_info

  - 5. 운전자의 차량용품 구매 이력 정보 
       - 이용할 테이블 
	     - 스마트카 마스터 데이터(smartcar_master_over18)
		 - 스마트카 차량용품 구매 이력 데이터(smartcar_item_buylist)
	   - 워크플로 이름 : subject5-workflow
	   - 스케줄러 이름 : subject5-coordinator
	   - 수행 주기 : 05:00 / 1day
	   - 생성할 마트 테이블 : managed_smartcar_item_buylist_info
	   
	   

6.6 탐색 파일럿 실행 4단계 - 데이터 준비
### 스마트카 상태정보 데이터 생성(1/2)
  - 로그 시뮬레이터를 이용해 오늘 날짜의 "스마트카 상태 정보 데이터"를 생성한다.
  - 플럼 서비스 시작 : CM 홈 -> [Flume] -> [시작]
  
### 스마트카 상태정보 데이터 생성(2/2)
  - 스마트카 상태 정보 배치 수행 - 현재 날짜를 기준으로 수행 

### 스마트카 상태정보 데이터 적재 및 확인(1/2)
  - 스마트카 상태 정보 데이터를 플럼의 수집 디렉터리로 옮긴다.

### 스마트카 상태정보 데이터 적재 및 확인(2/2)
  - 오늘 날짜의 스마트카 상태 정보가 HDFS에 정상적으로 적재되었는지 확인 
    - $ hdfs dfs -ls /pilot-pjt/collect/car-batch-log/
	

### 스마트카 운전자 운행 로그 생성 및 확인(1/6)
  - 수집 및 적재 서비스 기동
    - 플럼 서비스 : CM 홈 -> [Flume] -> [시작]
    - 카프카 서비스 : CM 홈 -> [Kafka] -> [시작]
    - 스톰 서비스 : server02.hadoop.com에 ssh로 접속 
      - service storm-nimbus start 
      - service storm-supervisor start 
      - service storm-ui start 
    - redis 서비스 : server02.hadoop.com에 ssh로 접속 
      - service redis_6379 start 
    - HBase 서비스 : CM 홈 -> [HBase] -> [시작]

### 스마트카 운전자 운행 로그 생성 및 확인(2/6)
  - 현재 날짜로 10의 운전자 운행 로그 생성 

### 스마트카 운전자 운행 로그 생성 및 확인(3/6)
  - tail -f /home/pilot-pjt/working/driver-realtime-log/SmartCarDriverInfo.log

### 스마트카 운전자 운행 로그 생성 및 확인(4/6)
  - 휴에 접속해 [메뉴] -> [HBase] -> [DriverCarInfo] 테이블을 선택해 실행일자의 운행데이터가 생성되었는지 확인 
  - HBase 브라우저 검색에 실행일자를 역변환한 로우키의 prefix "00000070111202"를 입력하면 등록된 로우키 목록이
    자동 완성되어 나타난다. 이 가운데 아무 로우키나 선택하고 뒤에 콤마(",")를 입력하고 [검색] 버튼을 클릭하면 해당 로우키
	스마트카 운전자의 실시간 운행 정보가 HBase에서 조회된다. 
	
### 스마트카 운전자 운행 로그 생성 및 확인(5/6)
  - 레디스 cli를 실행해 오늘 날짜로 과속한 스마트카 차량 정보를 확인한다.
    - $ redis-cli
    - 127.0.0.1:6379> smembers 실행일자(20211107)
  - 과속 차량이 3대 이상 발견되면 스마트카 운전자에 대한 운행 로그 시뮬레이터를 종료한다.

### 스마트카 운전자 운행 로그 생성 및 확인(6/6)
# 적재 테스트가 종료되면 원할한 자원 관리를 위해 수집/적재 서비스를 정지 시킴.
# 아래 순서로 정지
  - 플럼 서비스 : CM홈 -> [Flume] -> [정지]
  - 카프카 서비스: CM홈 -> [Kafka] -> [정지]
  - 스톰 서비스: server02 접속
    - $ service storm-ui stop
    - $ service storm-supervisor stop
    - $ service storm-nimbus stop
  - 레디스 서비스: server02에 ssh 접속 
    - $ service redis_6379 stop
  - HBase 서비스: CM홈 -> [HBase] -> [정지]  


6.6 탐색 파일럿 실행 4단계 - 03. 주제1-스마트카 상태정보 모니터링1
### 스마트카 상태 정보 모니터링 
  - 현재 하이브의 External에 '스마트카 상태 정보 데이터'가 적재되어 있음
  - 이 데이터를 우지 워크플로를 이용해 Managed 영역으로 매일 옮기고, 이때 '스마트카 마스터 데이터'와
    Join을 통해 데이터를 통합하는 작업을 한다.
  - 워크플로의 하이브 작업에 사용될 하이브 QL은 CH6/HiveQL/경로에서 확인

  - 우지 서비스 시작
    - CM 홈 -> [Oozie] -> [시작]

## 스마트카 상태 정보 모니터링 - 실습
# 주제 영역1 에서 사용할 작업 디렉터리를 만든다
  - 휴의 좌측 드롭박스 메뉴에서 [문서] 메뉴을 선택해 내 문서 기능을 실행
  - 하이브 스크립트 파일을 저장하기 위한 작업 폴더를 "workflow"라는 이름으로 생성
  - workflow 폴더 밑에 하위 작업 폴더로 "hive_script"를 추가로 생성
  - hive_script 폴더 밑에 하위 작업 폴더로 "subject1"을 생성 
  - hive_script 폴더 밑에 나머지 주제영역에서도 사용할 subjct2, subject3, subject4, subject5 폴더를
    각각 생성 
# 주제 영역1 에서 사용할 하이브 스크립트 파일 3개를 만든다. 먼저 내 문서에서 /workflow/hive_script/subject1
  위치로 이동 
  - [새문서] -> [Hive 쿼리] 선택
# 하이브 쿼리를 작성할 수 있는 에디터가 활성화 됨 
# 하이브 에디트 창에 DDL 문장을 입력하고 상단의 [저장] 버튼을 클릭. 파일 입력창이 나타나면 "create_table_managed_smartcar_status_info.hql"
  로 입력하고 [Save] 버튼을 클릭. 하이브 QL은 CH06/HiveQL/그림-6.69.hql 참조
# 계속해서 내 문서 /workflow/hive_script/subject1의 위치에 두 번째 하이브 스크립트 파일을 만듬
  - subject1 디렉터리에서 [새문서] -> [Hive 쿼리 선택]  
# 하이브 에디트 창에 External의 SmartCar_Status_Info 테이블에 작업일자(오늘날짜)를 기준으로 파티션 정보
  를 추가하는 스크립트를 작성한 후 저장한다. 파일 입력창이 나타나면 이름을 "alter_partition_smartcar_status_info.hql"
  로 입력하고 [Save] 버튼을 클릭 
# 내문서의 /worker/hive_script/subject1 위치에 세번째 하이브 스크립트 파일을 만든다
  - [새문서] -> [Hive 쿼리] 선택 

# 하이브 에디트 창이 활성화되면 먼저 동적 파티션을 생성하기 위한 하이브 환경변수 값을 설정해야 한다. 동적 파티션은
  지정된 특정 필드값을 기준으로 자동 파티션되는 기능인데, 다음의 옵션을 반드시 지정해야 한다.
  - set hive.exec.dynamic.partition = true;
  - set hive.exec.dynamic.partition.mode = nonstrict;
  
  동적 파티션 설정에 이어서 External 영역에 생성돼 있는 두 테이블인 SmartCar_Master_Over18과 
  SmartCar_Status_Info를 조인해서 조회된 데이터를 앞서 만든 Managed 테이블인 Managed_SmartCar_Status_Info에
  삽입하는 하이브 스크립트를 작성하고 [저장] 버튼을 클릭한다. 파일명은 "insert_table_managed_smartcar_status_info.hql"
  로 지정한다.

# 상단의 쿼리 콤보박스에서 [스케줄러] -> [Workflow]를 선택해서 실행할 워크플로를 만든다

# 첫번째 작업으로 주제 영역1의 데이터를 관리하기 위해 Managed 영역에 하이브 테이블을 만드는 작업을 추가한다.
  워크플로의 작업 툴 박스에서 "Hive 쿼리(HiveServer2 스크립트)" 작업을 워크플로의 첫번째 작업 노드에 
  드래그 앤드 드롭한다.

# 사용할 Hive 쿼리 스크립트 파일을 선택한다. 앞 단계에서 만든 내문서의 /workflow/hive_script/subject1에 
  만들어 놓은 create_table_managed_smartcar_status_info.hql을 선택한 후 [추가] 버튼을 만든다.
  
# 두 번째 작업으로 데이터를 가져올 External 영역의 SmartCar_Status_Info 테이블에 오늘 날짜로 파티션 정보를
  설정하는 하이브 작업을 만든다. 워크플로의 작업 툴박스에서 "Hive 쿼리" 작업을 워크플로우의 두 번째 작업노드에
  드래그 앤드 드롭한다.  
  
# 그림 6.71의 하이브 스크립트 안에서 정의한 작업일자(수집일자) 매개변수인 ${working_day}의 값을 그림 6.77의
  워크플로 매개변수와 연결한다. 이때 working_day 매개변수에 ${today}를 설정하는데, 이 값은 잠시후 우지의 
  예약 스케줄러에서 정의해 등록한다.
  - working_day=${today}

# 마지막 작업으로 첫번째 작업에서 만든 Managed_SmartCar_Status_Info 테이블에 데이터를 저장하기 위한 하이브
  작업을 만든다. 워크플로의 작업 툴박스에서 "Hive 쿼리" 작업을 워크플로의 네번재 작업 노드에 드래그 앤드 드롭한다.

# 사용할 Hive 쿠리 스크립트 파일을 선택한다. 앞 단계에서 만든 내 문서의 /workflow/hive_script/subject1에
  있는 insert_table_managed_smartcar_status_info.hql을 선택한후 [추가] 버튼을 클릭한다.
  
# 이번에도 하이브 스크립트 안에서 정의한 작업일자(수집일자)를 설정하는데 이 값은 잠시 후 우지의 예약 작업의 스케줄러
  설정 단계에서 정의한다.
  - working_day=${today}

# 워크플로의 이름을 지정한다. 워크플로 상단의 "My Workflow"를 클릭하고 "Subject1-Workflow" 변경한 후
  [확인] 버튼을 클릭한다.

# 워크플로 작성을 완료한다. 우측 상단의 [저장] 버튼을 누른다.

# 이제 작성한 워크플로를 작동하기 위한 예약 작업을 생성한다. 그림6.83처럼 상단의 쿼리 콤보박스에서 [스케줄러] -> [예약]을
  선택한다.

# 예약 작업의 이름을 "Subject1-예약"으로 입력한다.

# 예약 작업에서 사용할 워크플로를 선택한다. 앞서 만든 주제 영역1의 워크플로인 "Subject1-Workflow"를 선택한다.

# 예약 작업의 워크플로를 주기적으로 실행하기 위한 일종의 배치 잡 스케줄러다. 그림 6.86처럼 스케줄러 실행 주기와
  옵션 값을 설정한다. 시작일자와 종료일자는 파일럿 프로젝트 실습일자에 맞춰 입력한다.
  - 실행 간격 : 매일, 01시
  - 시작 일자 : 2021년 11월 21일, 00시 00분
  - 종료 일자 : 2021년 11월 30일, 23시 59분
  - 시간대 : Asia/Seoul

# 워크플로에서 사용할 매개변수인 today 값을 설정한다.
  - 앞서 워크플로의 하이브 작업에서 작업일자(수집일자) 매개변수를 "working_day=${today}로 등록했다. today의
    값을 예약 작업의 내장 함수를 통해 다음과 같이 설정한다.
	- ${coord:formatTime(coord:dateTzOffset(coord:nominalTime(),"Asia/Seoul"),'yyyyMMdd')}
	
# 우지의 예약 작업 설정이 모두 끝났다. [저장] 버튼을 누르고 작성을 완료한다.	
	
# 제출된 예약 작업 상태를 확인해 본다. 우측 상단의 [Job] 버튼을 클릭하고 잡 브라우저에서 [일정]을 선택한다. 앞서
  등록한 "Subject1-예약"이 설정한 스케줄러 시간에 따라 "PREP(준비), "Running(실행)" 상태 등으로 표기되며,
  매일 새벽 01시가 되면 등록된 워크플로가 작동하게 된다.
 
# 잡 브라우저에서는 등록된 잡의 현황, 진행 상태, 처리 이력과 결과 등을 확인할 수 있다.

# 특정 워크플로가 실행 중일 때는 그림 6.93처럼 작업 진행 상태를 프로그레스바로 곧바로 확인할 수 있다.

# 예약된 작업들이 활성화되어 실행될 때는 우측 상단에 [Job Browser]의 버튼 우측에 실행 중인 잡의 개수가 표시된다.
  [작업 미리보기]를 클릭하면 실행 중이거나 종료된 잡의 상세 정보를 확인할 수 있다.

# "Subject1-Workflow"가 정상적으로 작동했으면, 휴의 Hive Editor로 이동해서 그림 66.59와 같이 하이브 QL을
  작성해 실행한다. "biz_date=20200322"의 날짜는 독자들의 파일럿 환경의 실행 일자에 맞춰 입력한다.
  참고로 워크플로를 통해 만들어지는 managed_smartcar_status_info 같은 테이블이 하이브 편집기에서 곧바로
  표기되지 않아 하이브 편집기에서 해당 테이블 관련 쿼리를 실행할 때 에러가 발생할 수 있다. 이때는 하이브 테이블 
  목록 상단의 [새로고침] 버튼을 클릭해 테이블 목록을 새롭게 갱신한다.
  - select * from managed_smartcar_status_info where biz_date = '20200322' limit 10
 
# 실행 결과는 다음과 같다. 가장 우측 열의 biz_date 컬럼에 날짜가 "20200322"인 것을 확인할 수 있다.

# 한번 등록한 예약 작업은 종료 기간까지 계속 활성화된 상태로 남아 있게 되어 파일럿 환경의 리소스를 차지하게 된다.
  파일럿 환경에서는 매일 워크플로가 실행될 필요가 없으니 실행 및 테스트가 끝난 작업은 휴 좌측 드롭박스 메뉴의
  [Job] -> [일정]을 선택해 실행 중인 예약 작업들을 모두 중지시킨다.

# Tip
  - 테스트를 위해 워크플로를 즉시 실행할 수 있는 좀 더 쉬운 방법이 있다.
  - [내 문서]에서 실행할 워크플로를 선택하고 들어가 상단의 [수정] 버튼을 클릭하고 매개변수의 
    작업일자(매개변수: working_dat=YYYYMMDD)를 그림 6.87처럼 독자의 파일럿 상황에 맞춰 직접 입력한 후 상단의
	[저장] -> [제출] 버튼을 차례로 누르면 워크플로가 즉시 실행된다. 매개변수 설정이 필요없는 워크플로는 곱바로 
	[저장] -> [제출]을 선택하면 된다.
	- working_day=20200322

# [주제 영역별 매개변수 설정]
  - 주제영역1 : 플럼이 하둡에 적재한 날짜(개인별 실행중인 파일럿의 현재 작업일자 값으로, 모를경우 하둡 적재경로인
              "/pilot-pjt/collect/car-batch-log/wrk_date=YYYYMMDD"에 디렉터리의 날짜값을 이용한다.
              Server02에 접속해서 $ hdfs dfs -ls -R /pilot-pjt/collect/car-batch-log/
  - 주제영역2 : 스마트카 시뮬레이션 날짜(로그시뮬레이터 실행시 설정한 날짜로, 책기준 "20200322" 입력)		
  - 주제영역3 : 스마트카 시뮬레이션 날짜(로그시뮬레이터 실행시 설정한 날짜로, 책기준 "20200322" 입력)			  
  - 주제영역4 : 스마트카 시뮬레이션 날짜(로그시뮬레이터 실행시 설정한 날짜로, 책기준 "20200322" 입력)			  
  - 주제영역5 : 미사용

6.6 탐색 파일럿 실행 4단계 - 03. 주제1-스마트카 상태정보 모니터링2
# 실습 임.

  
6.6 탐색 파일럿 실행 4단계 - 03. 주제2-스마트카 운전자 운행기록 정보



