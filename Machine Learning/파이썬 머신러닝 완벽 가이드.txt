##### 파이썬 머신러닝 완벽 가이드 - 권철민

1장 파이썬 기반의 머신러닝과 생태계 이해
## 머신러닝 개념
  - 머신러닝이란 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 추론하는 알고리즘 기법을 통칭합니다.
  - 현신 세계의 매우 복잡한 조건으로 인행 기존의 소프트웨어 코드만으로는 해결하기 어려웠던 문제점을 이제 머신러닝을 이용해
    해결
  - 인간만이 가지는 인지능력만이 해결 가능하다고 여겨졌던 다양한 분야에서 머신러닝의 응용이 두드러지고 있으며 데이터 마이닝,
    영상인식, 음성 인식, 자연어 처리에서 머신러닝을 적용하면서 급속하게 발전.

# 머신러닝, 왜 필요한가?
  - 현실 세계의 복잡한 업무와 규칙을 구현하기 위한 매우 복잡하고 방대한 코드 
  - 수시로 변하는 업무 환경, 정책, 사용자 성향에 따른 애플리케이션 구현이 어려움
  - 많은 자원과 비용을 통해서 구현된 애플리케이션의 예측 정확성 문제 
  - 동일한 숫자라 하더라도 여러 변형으로 인행 숫자 인식에 필요한 여러 특징(feature)들을 if else와 같은 조건으로 구분하여
    숫자를 인식하기 어렵다.
  - 머신러닝은 이러한 복잡한 문제를 데이터를 기반으로 숨겨진 패턴을 인지해 해결 
  - 머신러닝 알고리즘은 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해
    데이터 내의 패턴을 스스로 인지하고 신로도 있는 예측 결과를 도출

# 머신러닝의 분류
  - 지도학습(Supervised Learning), 비지도 학습(Un-supervised Learning),
    강화학습(Reinforcement Learning)
  - 지도학습 - 명확한 결정값이 주어진 데이터를 학습 - 분류, 회귀, 시각/음성 감지/인지
  - 비지도 학습 - 결정값이 주어지지 않는 데이터를 학습 - 군집화(클러스터링), 차원 축소 

# 머신러닝 기반의 예측 분석(Predictive Analysis)
  - 머신러닝은 데이터를 관통하는 패턴을 학습하고, 이에 기반한 예측을 수행하면서 데이터 분석 영역에 새로운 혁신을 가져옴.
  - 데이터 분석 영역은 재빠르게 머신러닝 기반의 예측 분석으로 재편되고 있음
  - 많은 데이터 분석가와 데이터 과학자 머신러닝 알고리즘 기반의 새로운 예측 모델을 이용해 더욱 정확한 예측 및 의사 결정을
    도출하고 있으며, 데이터에 감춰진 새로운 의미와 인사이트를 발굴해 놀랄만한 이익으로 연결

# 모신러닝 알고리즘 유형
  - (책) 마스터 알고리즘(THe Master Algorithm)
  - 기호주의:결정 트리 등, 연결주의:신경망/딥러닝, 유전알고리즘, 베이지안 통계, 유추주의:KNN, 서포트 벡터 머신
  
# 머신러닝의 단점
  - 데이터에 너무 의존적(Garbage In, Garbage Out)
  - 학습시에 최적의 결과를 도출하기 위해 수립된 머신러닝 모델은 실제 환경 데이터 적용시 과접합 되기 쉬움.
  - 복잡한 머신러닝 알고리즘으로 인해 도출된 결과에 대한 논리적인 이해가 어려울 수 있음(머신러닝은 블랙박스)
  - 데이터만 집어 넣어도 자동으로 최적화된 결과를 도출할 것이라는 것은 환상이다.(특정 경우에는 개발자가 직접 만든 코드보다
    정확도가 더 떨어질 수 있음.) 끊임없이 모델을 개선하기 위한 노력이 필요하기 때문에 데이터의 특성을 파악하고 최적의
	알고리즘과 파라미터를 구성할 수 있는 고급 능력이 필요 

# 왜 데이터 수집에 열광하는가?
  - 다양하고 광대한 데이터를 기반으로 만들어진 머신러닝 모델은 더 좋은 품질을 약속합니다. 앞으로 많은 회사의 경쟁력은
    어떠한 품질의 머신러닝 모델을 가지고 있느냐에 결정 될 수 있음.

## 파이썬 기반 머신러닝의 특징및 장점과 구성요소 
# R vs python
  - 파이썬도 굉장히 직관적인 언어지만, R의 경우 통계분석을 위해 특화된 언어이며 무엇보다도 오랜 기간동안 많은 R 사용자들이
    생성하고 검증해온 다양한 많은 통계 패키지를 보유하고 있는 것이 가장 큰 장점.

# R Vs. Python Machine Learning - Google trends
  - 이제 머신러닝을 시작하려는 사람이라면, 특히 개발자라면 R보다 파이썬을 권장.

# LM + Python 강점 - Python의 놀라운 인기
  - Python은 소리없이 프로그래밍 세계를 점령하고 있는 Language:
    - Academy나 타 영역의 인재들도 Python 선호
	- Google, Facebook 등 유수의 IT 업계에서도 높은 생산성으로 인해 활용도가 높음(특히 Google)
	- 오픈 소스 계열의 전폭적인 지원을 받고 있음
	- 놀라울 정도의 많은 라이브러리 지원은 어떠한 유형의 개발도 쉽게 가능(역으로 선택의 자유가 많아서 오히려 머리가 아플 정도)
	- Interpreter Language의 특성상 속도는 느리지만 쉽고 유연한 특징으로 인해 데스크탑, 서버, 네트워크, 시스템,
	  IOT 등 다양한 영역에서 사용되고 있음.

# LM + Python 강점 - 뛰어난 확장성, 연계, 호환성
  - 많은 라이브러리, 뛰어난 생산성을 가지는 Python 언어 + Machine Learning
    - 분석 영역을 넘어서 ML 기반의 다양한 Application 개발이 쉽게 가능 
	- 기존 Application과의 연계도 쉬움(서로 다른 언어로 개발된 Application의 경우 Rest API)
	- Enterprise 아키텍처에도 연계, 확장 가능. Microservice 실시간 연계 등. 

# LM + Python 강점 - Deep Learning 으로의 진격
  - 유수의 Deep Learning Framework 이 Python 기반으로 작성(tensorflow Backend는 성능 때문에 C/C++로 작성)
  - 대부분의 Deep Learning 관련 Tutorial, 설명자료들이 Python으로 작성되어 제공
  - 현 시점에서 Deep Learning을 활용하기에 가장 좋은 시작점은 Python 
  
# 파이썬 머신러닝 생태계를 구성하는 주요 패키지 
  - 머신러닝 패키지 - Scikit learn - 딥러닝 기능은 없음
  - 배열/선형대수/통계패키지 - Numpy, SciPy
  - 데이터 핸들링 - pandas
  - 시각화 - matplotlib, seaborn
  - 대화형 파이썬 툴 - jupyter notebook
  
## 파이썬 기반 머신러닝을 위한 SW의 설치
# Anaconda vs pip
  - 파이썬 머신러닝을 위한 패키지를 설치하는 가장 쉬운 방법은 anaconda를 이용하는 것.
  - https://www.anaconda.com/download/


## 주피터 노트북 사용법과 넘파이/판다스이 필요성
# 주피터 노트북은 대표적인 대화형 파이썬 툴, 학생들이 필기하듯이 중요 코드 단위로 설명을 적고 코드를 수행해 그
  결과를 볼 수 있게 만들어서 직관적으로 어떤 코드가 어떤 역할을 하는지 확인 가능 

# 머신러닝을 위한 넘파이와 판다스의 중요성
  - 머신러닝 애플리케이션 구현에서 다양한 데이터의 추출/가공/변환이 상당한 영역을 차지하고 데이터 처리 부분은
    대부분 넘파이와 판다스의 몫.
  - 사이킷런이 넘파이 기반에서 작성됐기 때문에 넘파이의 기본 프레임워크를 이해하지 못하면 사이킷런 역시 실제
    구현에서 많은 벽에 부딪힐 수 있음.
  - 사이킷런은 API 구성이 매우 간결하고 직관적이어서 이를 이용한 개발 또한 상대적으로 쉽지만 넘파이와 판다스
    API는 더 방대하기 때문에 이를 익히는 데 시간이 많이 소모될 수 있음. 하지만 머신러닝을 위해서 이들을 많은
	시간을 들여 전문적으로 공부하는 것은 효율적이지 못함.
  - 넘파이와 판다스에 대한 기본 프레임워크와 중요 API만 습득하고, 일단 코드와 부딪쳐 가면서 모르는 API에 대해서는
    인터넷 자료를 통해 체득하는 것이 머신러닝 뿐만아니라 넘파이와 판다스에 관한 이해를 넓히는 더 빠른 방법임.

## 강의에 사용될 예제 소스코드 다운로드 받기 
  - https://github.com/chulminkw/PerfectGuide

## 넘파이 배열 ndarray 소개
  - ndarray : N차원(Dimesion) 배열(Array) 객체

# ndarray 생성
  - import numpy as np
    array1 = np.array([1,2,3])

# ndarray 형태(Shape)와 차원
  - 1차원 (3,0)
  - 2차원 (2,3)
  - ndarray의 shape은 ndarray.shape 속성으로, 차원은 ndarray.ndim 속성으로 알 수 있음.

# ndarray 타입(type)
  - ndarray내의 데이터값은 숫자 값, 문자열 값, 불 값 등이 모두 가능
  - 숫자형 - int형(8bit, 16bit, 32bit), unsigned int형(8bit, 16bit, 32bit), float형(16bit, 64bit, 128bit)
           그리고 이보다 더 큰 숫자 값이나 정밀도를 위해 complex 타입도 제공 
  - ndarray내의 데이터 타입은 그 연산의 특성상 같은 데이터 타입만 가능. 즉 한개의 ndarray객체에 int와 float가 함께 있을 수 없음.
  - ndarray내의 데이터 타입은 ndarray.dtype으로 확인할 수 있음.

# ndarray 타입(type) 변환 
  - astype()을 이용하여 변환 
    - 변경을 원하는 타입을 astype()에 인자로 입력
    - 대용량 데이터를 ndarray로 만들 때 메모리를 절약하기 위해 자주 사용
      - 0, 1, 2와 같이 크지 않는 숫자를 64bit float형 보다 8bit 또는 16bit의 integer형으로 변환하는 것이 
        메모리를 많이 절약.
  - 대용량 데이터를 다룰 시 메모리 절약을 위해서 형변환을 특히 고려해야 함.

# 넘파이 ndarray의 axis 축
  - ndarray는 shape는 행, 열, 높이 단위로 부여되는 것이 아니라 axis0, axis1, axis2와 같이 axis 단위로 부여됨 
  
  
## 넘파이 배열 ndarray 초기화 방법과 ndarray차원과 크기를 변경하는 reshape()의 이해 
# ndarray를 편리하게 생성하기 - arange, zeros, ones
  - 특정 크기와 차원을 가진 ndarray를 연속값이나 0 또는 1로 초기화 생성해야 할 경우 arange(), zeros(), ones()를
    이용해 쉽게 ndarray를 생성할 수 있음. 
	- np.arange(10) -> [0,1,2,3,4,5,6,7,8,9]
	- np.zeros((3,2), dtype='int32')
	- np.ones((3,2))

# ndarray의 차원과 크기를 변경하는 reshape()
  - reshape()는 ndarray를 특정 차원 및 형태로 변환. 변환 형태를 함수 인자로 부여하면 됨.
    - array1 -> reshape(2,5) -> 변환된 array 

# ndarray의 차원과 크기를 변경하는 reshape()
  - reshape(-1, 5)와 같이 인자에 -1을 부여하면, -1에 해당하는 axis의 크기는 가변적이 되고 -1이 아닌 인자값
    에 해당하는 axis 크기는 인자값으로 고정하여 ndarray의 shape를 변환.
    - array1, (10,) -> reshape(-1,5) -> shape는 (2,5)
  - reshape()는 reshape(-1,1), reshape(-1,)과 같은 형식으로 변환이 요구되는 경우가 많음. 
    주로 머신러닝 API의 인자로 1차원 array를 명확하게 2차원 ndarray로 변환하여 입력하기를 원하거나, 또는 반대의
    경우가 있을 경우 reshape()를 이용하여 ndarray의 형태을 변환시키는데 사용.
    - array1d, [0 1 2 3 4] -> reshape(-1,1) -> shape는 (5,1) [[0] [1] [2] [3] [4]]
    - array2d, [[0] [1] [2] [3] [4]] -> reshape(-1,) -> shape(5,) [0 1 2 3 4]

## 넘파이 ndarray의 인덱싱(Indexing)을 통한 데이터 세트 선택하기
# 인덱싱
  - 특정위치의 단일값 추출
  - 슬라이싱(slicing)
  - 팬시 인덱싱(Fancy Indexing)
  - 블린 인덱싱(Boolean Indexing)

# 단일값 추출 - 1차원 ndarray 
  - ndarray는 axis를 기준으로 0부터 시작하는 위치 인덱스값을 가지고 있음. 해당 인덱스 값을 []에 명시하여 단일값을
    추출함. 마이너스가 인덱스로 사용되면 맨 뒤에서 부터 위치를 지정함.
	- array1[3], array1[-2], array1[-1]

# 단일 값 추출 - 2차원 ndarray 
  - array2d[0,0], array2d[0,1], array2d[1,0], array2d[2,2]

# 슬라이싱(Slicing) - 1차원 ndarray 
  - 슬라이싱은 : 을 이용하여 연속된 값을 선택.
    - array1[:], array1[:3], array1[3:], array1[0:3]

# 슬라이싱(Slicing) - 2차원 ndarray 
  - array2d[0:2, 0:2], array2d[1:3, 0:3], array2d[1:3, :]
  - array2d[:, :], array2d[:2, 1:], array2d[:2, 0]

# 팬시 인덱싱 - 1차원 ndarray
  - 팬시 인덱싱(Fancy Indexing)은 리스트나 ndarray로 인덱스 집합을 지정하면 해당 위치의 인덱스에
    해당하는 ndarray를 반환하는 인덱싱 방식.
    - array1[[2,4,7]]

# 팬시 인덱싱 - 2차원 ndarray
  - array2d[[0,1], 2], array2d[[0,1], 0:2], array2d[[0,1]]

# 블린 인덱싱
  - 불린 인덱싱(Boolean Indexing)은 조건 필터링과 검색을 동시에 할 수 있기 때문에 매우 자주 사용되는 인덱싱 방식.
    - ndarray 내의 값이 5보다 큰 ndarray를 추출하고자 한다면?
      - 불린 인덱싱을 사용하지 않은 경우
        - array1d = np.arange(start=1, end=10)
          target=[]
          for i in range(0, 9):
              if array1d[i] > 5:
                 target.append(array1d[i])
          array_selected = np.array(target)
          print(array_selected)
      - 블린 인덱싱 사용
        - array1[array1>5]
		
## 넘파이 ndarray의 정렬과 선형대수 연산 
# 배열의 정렬 - sort()와 argsort()
  - sort()
    - np.sort() : 원 행렬은 그대로 유지한 채 원 행렬의 정렬된 행렬을 반환 
    - ndarray.sort() : 원 행렬 자체를 정렬한 형태로 반환하며 반환 값은 None
    - np.sort()나 ndarray.sort() 모두 기본적으로 오름차순으로 정렬. 내림차순으로 정렬하기위해서는 [::-1]을
      적용 함. np.sort()[::-1]
  - argsort()
    - 원본 행렬 정렬 시 정렬된 행렬의 원래 인덱스를 필요로 할 때 np.argsort()를 이용. np.argsort()는 
	  정렬 행렬의 원본 행렬 익덱스를 ndarray형으로 반환합니다.
	  

# 2차원 배열에서 axis 기반 sort()
  - np.sort(A, axis=0)
  - np.sort(A, axis=1)
  
# 선형대수 연산 - 행렬 내적
  - np.dot(A, B)
  
# 선형대수 연산 - 전치 행렬 
  - np.transpose(A) 

# 넘파이 Summary
  - 넘파이는 파이썬 머신러닝을 구성하는 핵심 기반으로 반드시 이해가 필요
  - 넘파이 API는 매우 넓은 범위를 다루고 있으므로 머신러닝 애플리케이션을 작성 할 때 중요하게 활용될 수 있는
    핵심 개념 위조로 숙지하는 것이 좋음
  - 넘파이는 판다스에 비해서 친절한 API를 제공하지 않음. 2차원 데이터라면 데이터 가공을 위해서 넘파이 보다는
    판다스를 이용하는 것이 효율적임.

## 판다스(Pandas) 개요와 기본 API - 01
# 판다스(Pandas) - 웰스 매키니
  - 판다스는 파이썬에서 데이터 처리를 위해 존재하는 가장 인기 있는 라이브러리.

# 판다스의 주요 구성 요소 - DataFrame, Series, Index 
  - DataFrame - 2차원 데이터 셋
  - Series - 1개의 컬럼갑승로만 구성된 1차원 데이터 셋 
  
# 기본 API
  - read_csv(), head(), shape, info(), describe(), value_counts(), sort_values()


## 판다스 DataFrame의 변환, 컬럼 세트 생성/수정, 삭제 및 Index 객체 소개 
# DataFrame과 리스트, 딕셔너리, 넘파이 ndarray 상호 변환
  - 리스트(list)를 DataFrame으로 변환
    - df_list1 = pd.DataFrame(list, columns=col_name1)
  - ndarray를 DataFrame으로 변환 
    - df_array2 = pd.DataFrame(array2, columns=col_name2)
  - 딕셔너리(dict)를 DataFrame으로 변환 
    - dict = {'col1':[1,11], 'col2':[2,22], 'col3':[3,33]}
	- df_dict = pd.DataFrame(dict)
  - DataFrame을 ndarray로 변환 
    - DataFrame 객체의 values 속성을 이용하여 ndarray 변환 
  - DataFrame을 리스트로 변환
    - DataFrame 객체의 values 속성을 이용하여 먼저 ndarray로 변환 후 tolist()를 이용하여 list로 변환 
  - DataFrame을 딕셔너리로 변환   
    - DataFrame 객체의 to_dict()를 이용하여 변환 

# DataFrame 데이터 삭제 
  - DataFrame의 drop()
    - DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, error='raise')
      - axis : DataFrame의 로우를 삭제할 때는 axis=0, 컬럼을 삭제할 때는 axis=1로 설정 
	  - 원본 DataFrame은 유지하고 드롭된 DataFrame을 새롭게 객체 변수로 받고 싶다면 inplace=False로
	    설정(디폴트 값이 False 임)
		- Ex: titanic_drop.df = titanic_df.drop('Age_0', axis=1, inplace=False)
	  - 원본 DataFrame에 드롭된 결과를 적용할 경우에는 inplace=True를 적용 
	    - Ex: titanic_df.drop('Age_0', axis=1, inplace=True)
	  - 원본 DataFrame에서 드롭된 DataFrame을 다시 원본 DataFrame 객체 변수로 할당하면 원본 DataFrame에서 드롭된 
	    결과를 적용할 경우와 같음(단, 기존 원본 DataFrame 객체 변수는 메모리에서 추후 제거됨)
		- Ex: titanic_df = titanic_df.drop('Age_0', axis=1, inplace=False) 

# INdex 객체 
  - 판다스의 Index 객체는 DataFrame, Series의 레코드를 고유하게 식별하는 객체 
  - DataFrame, Series에서 Index 객체만 추출하려면 DataFrame.index 또는 Series.index 속성을 통해 가능.
  - Series 객체는 Index 객체를 포함하지만 Series 객체에 연산 함수를 적용할 때 Index는 연산에서 제외됨.
    Index는 오직 식별용으로만 사용됨.
  - DataFrame 및 Series에 reset.index() 메소드를 수행하면 새롭게 인덱스를 연속된 숫자형으로 할당하며 기존의
    인덱스는 'index'라는 새로운 컬럼명으로 추가됨.
	

## 판다스 데이터 셀렉션과 필터링 
  - 데이터 셀렉션 및 필터링
    - [] - 컬럼 기반 필터링 또는 불린 인덱싱 필터링 제공 
    - ix[], loc[], iloc[] - 명칭/위치 기반 인덱싱 제공 
	- 불린 인덱싱(Boolean Indexing) - 조건식에 따른 필터링 제공 
  - 명칭(Label) 기반 인덱싱은 컬럼의 명칭을 기반으로 위치를 지정하는 방식. 
  - 위치(Position) 기반 인덱싱은 0을 출발점으로 하는 가로축, 세로축 좌표 기반의 행과 열 위치를 기반으로 데이터를 지정.
    - ix[] - 명칭 기반과 위치 기반 인덱싱을 함께 제공하나 없어질 예정.
	- loc[] - 명칭 기반 인덱싱 
	- iloc[] - 위치 기반 인덱싱

# 명칭 기반과 위치 기반 인덱싱 구분 

# 불린 인덱싱(Boolean indexing)
  - 위치기반, 명칭기반 인덱싱 모두 사용할 필요없이 조건식을 [] 안에 기입하여 간편하게 필터링 수행 
    - titanic_boolean = titanic_df[titanic_df['Age']>60]


## 판다스 Aggregation 함수와 Group by 수행 
# Aggregation 함수 
  - sum(), max(), min(), count() 등의 함수는 DataFrame/Series에서 집합(Aggregation) 연산을 수행.
  - DataFrame의 경우 DataFrame에서 바로 aggregation을 호출할 경우 모든 컬럼에 해당 aggregation을 적용.
  
# axis에 따른 Aggregation 함수 결과 
  - Aggregation 함수 호출시 axis 값을 명시하지 않으면 axis=0으로 결과 출력
    - titanic_df[['Age','Fare']].sum(axis=0) vs titanic_df[['Age','Fare']].sum(axis=1)
  	
# DataFrame Group By
  - DataFrame은 Group by 연산을 위해 groupby() 메소드를 제공
  - groupby() 메소드는 by 인자로 group by 하려는 컬럼명을 입력 받으면 DataFrameGroupBy 객체를 반환 
  - 이렇게 반환된 DataFrameGroupBy 객체에 aggregation 함수를 수행.


## 판다스 결손 데이터 처리하기
# 결손 데이터(Missing Data) 처리하기 
  - isna() - DataFrame의 isna() 메소드는 주어진 컬럼값들이 NaN인지 True/False 값을 반환함.
  - fillna() - Missing 데이터를 인자로 주어진 값으로 대체함.

## 판다스 람다식 적용하여 데이터 가공하기 
# 판다스 apply lambda - 파이썬 lambda 식 이해 
  - def get_square(a):
        retrun a**2
  - lambda x: x ** 2		

# apply lambda 식으로 데이터 가공 
  - 판다스는 apply 함수에 lambda 식을 결합해 DataFrame이나 Series의 레코드 별로 데이터를 가공하는 기능을
    제공함. 판다스의 경우 컬럼에 일괄적으로 데이터 가공을 하는 것이 속도 면에서 더 빠르거나 복잡한 데이터 가공이 
	필요할 경우 어쩔 수 없이 apply lambda 를 이용함.
	- titanic_df['Name_len'] = titanic_df['Name'].apply(lambda x: len(x))

# 판다스 Summary 
  - 2차원 데이터 핸들링을 위해서는 판다스를 사용 
  - 판다스는 매우 편리하고 다양한 데이터 처리 API를 제공하지만(조인, 피벗/언피벗, SQL like API 등), 이를
    다 알기에는 많은 시간과 노력이 필요 
  - 지금까지 언급된 핵심 사항에 집중하고, 데이터 처리를 직접 수행해 보면서 문제에 부딛칠 때마다 판다스이 다양한 API를
    찾아서 해결해 가면 판다스에 대한 실력을 더욱 향상 시킬 수 있음.
	

## 파이썬 기반의 머신러닝과 생태계 이해 Summary 
# 1장 Summary 
  - 머신러닝이란 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 추론하는 알고리즘 기법을 통칭 
  - 직관적인 문법, 많은 라이브러리, 뛰어난 생산성을 가지는 파이썬 언어를 기반으로 한 머신러닝 애플리케이션은 유연성, 
    통합성 등의 많은 장점을 사용자에게 제공해 줌.
  - 파이썬 기반의 머신러닝을 익히기 위해서는 머신러닝 패키지 뿐만아니라, 넘파이, 판다스, 시각화 등의 다양한 지원 패키지들도
    같이 학습되어야 함. 머신러닝 애플리케이션을 작성하면서 인터넷 등을 통해 찾아가면서 구현하는 것이 이들 지원 패키지를 
	빨리 익히는데 도움이 더 될 것이다.


2장 사이킷런으로 시작하는 머신러닝 
## 사이킷런 소개와 첫번째 머신러닝 애플리케이션 만들어 보기 - 붓꽃(Iris) 품종 예측 
# 사이킷런 소개 
  - 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬 스러운 API를 제공함.
  - 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공함
  - 오랜 기간 실전 환경에서 검증됐으며, 매우 많은 환경에서 사용되는 성숙한 라이브러리임.
  - 주로 Numpy와 Scipy 기반 위에서 구축된 라이브러리임.

# 사이킷런을 이용한 붓꽃 데이터 분류 - 분류 예측, 지도학습 
  - 붓꽃 데이터 세트는 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 피처(Feature)를 기반으로 꽃의 품종을 예측하기 위한 것.
  - 붓꽃 데이터 피쳐
    - Sepal length - 꽃받침의 길이 
    - Sepal width  - 꽃받침의 넓이  
    - Petal length - 꽃잎의 길이 
    - Petal width  - 꽃잎의 넓이 	
    - 붓꽃 데이터 품종(레이블) - Setosa, Vesicolor, Virginica
	
# 머신러닝을 위한 용어 정리 
  - 피처(Feature)? 속성?
    - 피처는 데이터 세스의 일반 속성임.
	- 머신러닝은 2차원 이상의 다차원 데이터에서도 많이 사용되므로 타겟값을 제외한 나머지 속성을 모두 피처로 지칭 
  - 레이블, 클래스, 타겟(값), 결정(값) 
    - 타겟값 또는 결정값은 지도학습시 데이터의 학습을 위해 주어지는 정답 데이터.
	- 지도학습 중 분류의 경우에는 이 결정값을 레이블 또는 클래스로 지칭
  	
# 지도학습 - 분류 
  - 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법의 하나임. 
  - 지도학습은 학습을 위한 다양한 피처와 분류 결정값인 레이블(Label) 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터
    세트에서 미지의 레이블을 예측함
  - 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식임. 이 때 학습을 위해 주어진
    데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세스틀 테스트
    데이터 세트로 지칭함. 

# 붓꽃 데이터 분류 예측 프로세스 
  - 데이터 세트 분리 - 데이터를 학습 데이터와 테스트 데이터로 분리함.
  - 모델 학습 - 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킴.
  - 예측 수행 - 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 함.
  - 평가     - 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가
  
## 사이킷런의 기반 프레임 워크 익히기 - 주요 API/모듈 및 내장 예제 데이터 세트 소개
# 붓꽃 데이터 분류 예측 프로세스
  - 학습데이터 -> 모델학습 -> 테스트데이터 -> 학습된 모델로 테스트 데이터 예측 -> 평가 

# 사이킷런 기반 프레임워크 - Estimator 와 fit(), predict()
  - Esstimator - 학습(fit()), 예측(predict())
    - 분류(Classifier) - 분류구현 클래스
      - DecisionTreeClassifier
      - RandomForestClassifier
      - GradientBoostingClassifier
      - GaussianNB
      - SVC
    - 회귀(Regressor) - 회귀구현 클래스 
      - LinearRegression
	  - Ridge
	  - Lasso
	  - RandomForestRegressor
	  - GradientBoostingRegressor 
	
# 사이킷런의 주요 모듈
  - 예제 데이터 
    - sklearn.datasets - 사이킷런에 내장되어 예제로 제공하는 데이터 세트 
  - 데이터 분리, 검증 & 파라미터 튜닝 
    - sklearn.model_selection - 교차검증을 위한 학습용/테스트용 분리, 그리드 서치(Grid Search)로 최적 파라미터 추출 등이 API 제공 
  - 피처 처리 
    - sklearn.preprocessing - 데이터 전처리에 필요한 다양한 가공 기능 제공(문자열을 숫자형 코드 값으로 인코딩, 정규화, 스케일링 등)
    - sklearn.feature_selection - 알고리즘에 큰 영향을 미치는 피처를 우선순위 대로 셀렉션 작업을 수행하는 다양한 기능 제공 
    - sklearn.feature_extraction - 텍스트 데이터나 이미지 데이터의 벡터화된 피처를 추출하는데 사용됨. 예를 들어 텍스트 데이터에서 
	                               Count Vectorizer 나 Tf-Idf Vectorizer 등을 생성하는 기능 제공 
                                 - 텍스트 데이터의 피처 추출은 sklearn.feature_extraction.text 모듈에, 이미지 데이터의 피처 추출은 
                                   sklearn.feature_extraction.image 모듈에 지원 API가 있음.
  - 피처 처리 & 차원 축소 
    - sklearn.decomposition - 차원 축소와 관련한 알고리즘을 지원하는 모듈임. PCA, NMF, Truncated SVD 등을 통해 차원 축소 기능을 수행할 수 있음.

  - 평가 
    - sklearn.metrics - 분류, 회귀, 클러스터링, 페어와이즈(Pairwise)에 대한 다양한 성능 측정 방법 제공 
                      - Accuracy, Precision, Recall, ROC-AUC, RMSE 등 제공 
  - ML 알고리즘 - 다양한 알고리즘을 가지고 있음. 아래 예시보다 더 많음.
    - sklearn.ensemble - 앙상블 알고리즘 제공
                       - 랜덤 포레스트, 에이다 부스트, 그래디언트 부스팅 등을 제공 
    - sklearn.linear_model - 주로 선형회귀, 릿지(Ridge), 라쏘(Lasso) 및 로지스틱 회귀 등 회귀 관려 알고리즘을 지원. 또한 SGD(Stochastic Gradient Descent) 관련
                             알고리즘도 제공 
    - sklearn.naive_bayes - 나이브 베이즈 알고리즘 제공. 가우시안 NB, 다항분포 NB 등.
    - sklearn.neighboors - 최근접 아웃 아로고리즘 제공. K-NN 등 
    - sklearn.svm - 서포트 벡터 머신 알고리즘 제공 
    - sklearn.tree - 의사 결정 트리 알고리즘 제공 
    - sklearn.cluster - 비지도 클러스터링 알고리즘 제공(K-평균, 계층형, DBSCAN 등)
  - 유틸리티 
    - sklearn.pipeline - 피처 처리 등의 변환과 ML 알고리즘 학습, 예측 등을 함께 묶어서 실행할 수 있는 유틸리티 제공   

# 사이킷런 내장 예제 데이터 셋 - 분류 및 회귀용 
  - datasets.load_boston() - 회귀 용도이며, 미국 보스턴의 집 피처들과 가격에 대한 데이터세트 
  - datasets.load_breast_cancer() - 분류 용도이며, 위스콘신 유방암 피처들과 악성/음성 레이블 데이터 세트 
  - datasets.load_diabetes() - 회귀 용도이며, 당뇨 데이터 세트 
  - datasets.load_digits() - 분류 용도이며, 0에서 9까지 숫자의 이미지 픽셀 데이터 세트 
  - datasets.load_iris() - 분류 용도이며, 붓꽃에 대한 피처를 가진 데이터 세트 

# 내장 예제 데이터 셋 구성 
  - iris_data 의 데이터 셋 확인

## 학습과 테스트 데이터 세트의 분리
# Model Selection(sklearn.model_selection) 소개 - 학습 데이터와 테스트 데이터 
  - 학습 데이터 세트 
    - 머신러닝 알고리즘 학습을 위해 사용
	- 데이터의 속성들과 결정값(레이블)값 모두를 가지고 있음 
	- 학습 데이터를 기반으로 머신러닝 알고리즘이 데이터 속성과 결정값의 패턴을 인지하고 학습 
  - 테스트 데이터 세트  
    - 테스트 데이터 세트에서 학습된 머신러닝 알고리즘을 테스트 
	- 테스트 데이터는 속성 데이터만 머신러닝 알고리즘에 제공하며, 머신러닝 알고리즘은 제공된 데이터를 기반으로 결정값을 예측 
	- 테스트 데이터는 학습 데이터와 별도로 데이터 세트로 제공되어야 함.

# 학습 데이터와 테스트 데이터 분리 - train_test_split()
  - sklearn.model_selection의 train_test_split() 함수
    : X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121)
    - test_size : 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정. 디폴트는 0.25, 즉 25%임.
    - train_size : 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정. test_size parameter를 사용하므로 잘 사용하지 않음.
	- shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정. 디폴트는 True. 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를
	            만드는데 사용.
	- random_state : random_state는 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값입니다. 
	                 train_test_split()는 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트용 
					 데이터를 생성함.


## 교차검증 - K-Fold와 Stratified K-Fold 
# 교차 검증
  - 학습 데이터를 다시 분할하여 학습 데이터와 학습된 모델의 성능을 일차 평가하는 검증 데이터로 나눔
    - 학습데이터 세트 -> 분할 -> 학습데이터 세트 + 검증 데이터 세트
  - 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위한 데이터 세트 - 테스트 데이터 세트 
  - 교차 검증은 필수 요소임. 데이터를 섞어서 다시 검증데이터를 생성하여 여러번 수행.  

# K 폴드 교차 검증 
  - K=5일때 총 5개의 폴드 세트에 5번의 학습과 검증 평가 반복 수행 
    - 학습 데이터를 5개로 나누어 1개를 검증 데이터로 사용. 돌아가면서 5번의 학습 및 검증을 수행

# K 폴드 교차 검증
  - 일반 K 폴드
  - Stratified K 폴드
    - 불균형한(imbalanced) 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식 
    - 학습데이터와 검증 데이터 세트가 가지는 레이블 분포도가 유사하도록 검증 데이터 추출 	

## 교차검증 성능평가 cross_val_score()와 하이퍼 파라미터 튜닝을 위한 GridSearchCV
# 교차 검증을 보다 간편하게 - cross_val_score()
  - KFold 클래스를 이용한 교차 검증 방법
    1. 폴드 세트 설정
    2. for 루프에서 반복적으로 학습/검증 데이터 추출 및 학습과 예측 수행 
    3. 폴드 세트별로 예측 성능을 평균하여 최종 성능 평가 
  - cross_val_score() 함수로 폴드 세트 추출, 학습/예측, 평가를 한번에 수행 
    - cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')

# GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한번에 
  - 사이킷런은 GridSearchCV를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 
    순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안을 제공   
	- grid_parameters = {'max_depth':[1,2,3], 'min_samples_split': [2,3]}
	- cv 세트가 3이라면 6(파라미터 순차 적용 횟수) X 3(CV 세트수) -> 학습/검증 총 수행횟수(18)

## 데이터 전처리 - 인코딩과 스케일링
# 데이터 전처리(Preprocessing)
  - 데이터 클린징 - 잘못된 오류 데이터 처리
  - 결손값 처리(Null/NaN 처리)
  - 데이터 인코딩(레이블, 원-핫 인코딩)
  - 데이터 스케일링
  - 이상치(Outlier) 제거
  - Feature 선택, 추출 및 가공 

# 데이터 인코딩
  - 머신러닝 알고리즘은 문자열 속성을 입력 받지 않으며 모든 데이터는 숫자형으로 표현되어야 함. 문자형 카테고리형 속성은 모두 숫자값으로 변환/인코딩 되어야 함.
    - 레이블(Label) 인코딩
	- 원-핫(One-Hot) 인코딩 

# 레이블(Label) 인코딩 
  - [TV, 냉장고, 전자레인지, 컴퓨터, 선풍기, 믹서] -> [0, 1, 4, 5, 3, 2]
    - 상품명은 연관성이 없으나 숫자로 변경됨으로써 의미가 부여 되는 부분이 있음. 숫자는 크기 및 순서가 있어 머신러닝이 관련성을 인지할 수 있음.
      그래서 나온게 원-핫 인코딩.

# 원-핫(One-Hot) 인코딩	  
  - 원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 컬럼만을 1을 표시하고 나머지 컬럼에는 0을 표시하는 방식.
    - TV (100000), 냉장고 (010000), 전자렌지 (001000), 컴퓨터 (000010), 믹서 (000001)

# 사이킷런 - 원-핫 인코딩 
  - 원본데이터 -> 숫자로 인코딩 -> 원-핫 인코딩 

# 판다스 get_dummies()를 이용한 원-핫 인코딩 
  - pd.get_dummies(DataFrame) 
  
# 피처 스케일링 
  - 표준화는 데이터의 피처 각각이 평균이 0이고 분산이 1인 가우시안 정규분포를 가진 값으로 변환하는 것을 의미함.
    - xi_new = (xi - mean(x))/stdev(x)
  - 정규화는 서로 다른 퍼처의 크기를 통일하기 위해 크기를 변환해주는 개념  
    - xi_new = (xi - min(x))/(max(x) - min(x)

# 사이킷런 피처 스케일링 지원 
  - StandardScaler : 평균이 0이고, 분산이 1인 정규 분포 형태로 변환 
  - MinMaxScaler : 데이터값을 0과 1사이의 범위 값으로 변환(음수 값이 있으면 -1에서 1값으로 변환)
 
## 사이킷런으로 수행하는 타이타닉 생존자 예측 
# 타이타닉 생존자 ML 예측 구현 
  - 데이터 전처리
    - Null 처리 
    - 불필요한 속성 제거 
    - 인코딩 수행 
  - 모델 학습 및 검증/예측/평가 
    - 결정트리, 랜덤포레스트, 로지스틱 회귀 학습 비교 
    - K폴드 교차 검증 
    - cross_val_score() 와 GridSearchCV() 수행 	

## 2장 Summary 
# 머신러닝 지도학습 프로세스 
  - 데이터 전처리
    - 데이터 클린징 
    - 결손값 처리(Null/NaN 처리) 
    - 데이터 인코딩(레이블, 원-핫 인코딩)
    - 데이터 스케일링 
    - 이상치 제거 
    - Feature 선택, 추출 및 가공 	
  - 데이터 세트 분리 
    - 학습 데이터/테스트 데이터 분리 
  - 모델학습 및 검증 평가 - 교차 검증(K 폴드), cross_val_score(), GridSearchCV
    - 알고리즘 학습 
  - 예측 수행 
    - 테스트 데이터로 예측 수행 
  - 평가   
    - 예측 평가 

3. 평가(Evaluation) 
## 분류(Classification) 성능 평가지표 개요와 정확도(Accuracy) 소개 
# 분류(Classification) 성능 평가 지표 
  - 정확도(Accuracy) 
  - 오차행렬(Confusion Matrix)
  - 정밀도(Precision)
  - 재현율(Recall) 
  - F1 스코어 
  - ROC AUC - 이진분류시 많이 사용 

# 정확도(Accuracy)
  - 정확도(Accuracy) = 예측 결과가 동일한 데이터 건수 / 전체 예측 데이터 건수 
  - 정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표. 하지만 이진 분류의 경우 데이터의 구성에 따라
    ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하지 않음.
  - 특히 정확도는 불균형한(imbalanced) 레이블 값 분포에서 ML 모델의 성능을 판단할 경우, 적합한 평가 지표가 아님.

# 정확도의 문제점 
  - 타이타닉 생존자 예측에서 여성은 모두 생존으로 판별 
    - if sex = '여성' 생존   
  - MNIST 데이터셋을 multi classification에서 binary classification으로 변경 


## 오차행렬(Confusion Matrix), 정밀도(Precision), 재현율(Recall) 소개  
# 오차 행렬(Confusion Matrix)
  - 오차 행렬은 이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표 

                                             예측 클래스(Predicted Class)
											 
                                       | Negative(0)        | Positive(1)
	                       ------------|--------------------|--------------------			
                           Negative(0) | TN(True Negative)  | FP(False Positive)
    실제 클래스(Actual Class) ------------|--------------------|--------------------	
	                       Positive(1) | FN(False Negative) | TP(True Positive) 
	                                   |                    |

# 정밀도(Precision)과 재현율(Recall) - 이진분류에서 많이 사용
  - 정밀도 = TP / (FP + TP)
    - 정밀도는 예측을 Positive로 한 대상 중에 예측과 실제값이 Positive로 일치한 데이터의 비율을 말함. 
  - 재현율 = TP / (FN + TP) 
    - 재현율은 실제 값이 Positive인 대상 중에 예측과 실제값이 Positive로 일치한 데이터의 비율을 말함.
	
# 사이킷런의 정밀도, 재현율 
  - 정밀도는 precision_sore()
  - 재현율은 recall_score()  


## 정밀도와 재현율의 트레이드 오프(Trade off)
# 업무에 따른 재현율과 정밀도의 상대적 중요도 
  - 재현율이 상대적으로 더 중요한 지표인 경우는 실제 Positive 양성인 데이터 예측을 Negative로 잘못 판단하게 되면
    업무상 큰 영향이 발생하는 경우 : 암 진단, 금융사기 판별 
  - 정밀도는 상대적으로 더 중요한 지표인 경우는 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게
    되면 업무상 큰 영향이 발생하는 경우 : 스팸 메일 

  - 불균형한 레이블 클래스를 가지는 이진 분류 모델에서는 많은 데이터 층에서 중점적으로 찾아야 하는 매우 적은 수의
    결괏값에 Positive를 설정해 1값을 부여하고, 그렇지 않은 경우는 negative 0값을 일반적으로 부여함.  

# 정밀도/재현율 트레이드 오프 
  - 분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold)을 조정해
    정밀도 또는 재현율의 수치를 높일 수 있음.
  - 하지만 정밀도와 재현율은 상호 보완적인 평가 지표이기 때문에 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지기
    쉬움. 이를 정밀도/재현율의 트레이드오프(Trade-off)라고 함.

# 분류 결정 임계값에 따른 Positive 예측 확률 변화 
  - 정밀도 = TP / (FP + TP)
  - 재현율 = TP / (FN + TP)

  - 디폴트 임계값(0.5) -> 새로운 임계값(0.4) 
    - 분류 결정 임계값이 낮아질 수록 Positive로 예측할 확률이 높아짐. 재현율 증가하고 정밀도는 감소함.
      - 사이킷런 Estimator 객체의 predict_proba() 메소드는 분류 결정 예측 확률을 반환
      - 이를 이용하면 임의로 분류 결정 임계값을 조정하면서 예측 확률을 변경할 수 있음.	  

# 분류 결정 임계값에 따른 정밀도, 재현율 곡선 
  - 사이킷런은 precision_recall_curve() 함수를 통해 임계값에 따른 정밀도, 재현율의 변화값을 제공함.


## F1 Score와 ROC-AUC 소개   
# 정밀도와 재현율의 맹점 
  - 정밀도를 100%로 만드는 법 
    - 확실히 기준이 되는 경우만 Positive로 예측하고 나머지는 모두 Negative로 예측함. 정밀도 = TP / (TP+FP)임.
      전체 환자 1000명 중 확실한 Positive 징후만 가진 환자는 단 1명이라고 하면 이 한 명만 Positive로 예측하고 
      나머지는 모두 Negative로 예측하더라도 FP는 0, TP는 1이 되므로 정밀도는 1/(1+0)으로 100%가 됨.
  - 재현율을 100%로 만드는 법 
    - 모든 환자를 Positive로 예측하면 됨. 재현율 = TP / (TP+FN) 이므로 전체 환자 1000명을 다 Positive로 예측.
      이중 실제 양성인 사람이 30명 정도라도 TN이 수치에 포함되지 않고 FN은 아예 0이므로 30/(30+0)으로 100%가 됨.

# F1 Score 
  - F1 Score는 정밀도와 재현율을 결합한 지표. F1 스코어는 정밀도와 재현율이 어느 한쪽에 치우치지 않는 수치를 나타낼 때 
    상대적으로 높은 값을 가짐. 
  - F1 스코어 공식 
    - F1 = 2 / ((1/recall) + (1/precision)) = 2 * (precision * recall) / (precision + recall)
  
  - 만일 A 예측 모델의 경우 정밀도가 0.9 재현율이 0.1로 극단적 차이가 나고, B 예측 모델은 정밀도가 0.5, 재현율이 0.5로
    정밀도와 재현율이 큰 차이가 없다면 A 예측 모델의 F1 스코어는 0.18이고, B 예측 모델의 F1 스코어는 0.5로 B 모델이
  - 사이킷런은 F1 score를 위해 f1_score() 함수를 제공함. 

# ROC 곡선과 AUC 
  - ROC 곡선(Receiver Operation Characteristic Curve)과 이에 기반한 AUC 스코어는 이진 분류의 예측 성능 측정에서
    중요하게 사용되는 지표임. 일반적으로 의학 분야에서 많이 사용되지만, 머신러닝의 이진 분류 모델의 예측 성능을 판단하는
    중요한 평가 지표 이기도 함.
  - ROC 곡선은 FPR(False Positive Rate)이 변할때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선임.
    FPR을 X축으로, TPR을 Y 축으로 잡으면 FPR의 변화에 따른 TPR의 변화가 곡선 형태로 나타남.
	
  - 분류의 성능 지표로 사용되는 것은 ROC 곡선 면적에 기반한 AUC값으로 결정함. AUC(Area Under Curve) 값은 ROC 곡선 밑의
    면적을 구한 것으로서 일반적으로 1에 가까울수록 좋은 수치임.  
	
# ROC 곡선 
  - FPR의 변화에 따른 TPR의 변화 곡선 
    - TPR은 True Positive Rate의 약자이며, 이는 재현율을 나타냄. 따라서 TPR은 TP / (FN + TP) 입니다. 
      TPR, 즉 재현율은 민감도로도 불림. 
  - FPR은 실제 Negative(음성)을 잘못 예측한 비율을 나타냄. 즉 실제 Negative인데 Positive 또는 Negative로 
    예측한 것 중 Positive로 잘못 예측한 비율입니다. FPR = FP / (FP + TN) 임.

# 사이킷런 ROC 곡선 및 AUC 스코어 
  - 사이킷런은 임계값에 따른 ROC 곡선 데이터를 roc_curve()로, AUC 스코어를 roc_auc_score() 함수로 제공 	
	  

## 평가 실습 - 피마 인디언 당뇨병 예측 
# 피마 인디언 당뇨병 예측 
  - 북아메리카 피마 인디언 당뇨병(Pima Indian Diabetes) 데이터 세트를 이용해 당뇨병 여부를 판단하는 머신러닝 예측 모델을 
    수립하고, 지금까지 평가 지표를 적용해 봄.
  - 고립된 환경에서 서구화된 식습관에 따른 당뇨병 점검	

# 평가 Summary 
  - 이진 분류에서 정밀도, 재현율, F1 스코어, AUC 스코어가 주로 성능 평가 지표가 활용됨.
  - 오차 행렬(Confusion Matrix)은 실제 클래스 값과 예측 클래스 값의 True, False에 따라 TN, FP, FN, TP로 매핑되는
    4분면 행령을 제공함.
  - 정밀도와 재현율은 Positive 데이터 세트의 예측 성능에 좀 더 초점을 맞춘 지표이며, 분류 결정 임계값을 조정해 
    정밀도 또는 재현율은 수치를 높이거나 낮출 수 있음   
  - F1 스코어는 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 좋은 값을 가짐. 
  - AUC 스코어는 ROC 곡선 밑의 면적을 구한 것으로 1에 가까울 수록 좋은 수치임. 
  

4장 분류(Classification) 
## 분류(Classification) 개요와 결정트리(Decision Tree) 소개 
# 분류 알고리즘 
  - 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로
    학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것.
  - 대표적인 분류 알고리즘 
    - 베이즈(Bayes) 통계화 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
    - 독립변수 와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression) 
    - 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree) 
    - 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine) 
    - 근접 거리를 기준으로 하는 최소 근접(Nearest Neighborr) 알고리즘 
    - 심층 연결 기반의 신경망(Neural Network) - 딥러닝 
    - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble) - 기반 알고리즘으로 Decision Tree 사용 
	
# 결정 트리와 앙상블 
  - 결정 트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘. 또한 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이
    매우 적음. 하지만 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적합(overfiting)이 
    발생해 반대로 예측 성능이 저하될 수도 있다는 단점이 있음. 
  - 하지만, 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용함. 앙상블은 매우 많은 여러개의 약한 학습(즉, 예측 성능이
    상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서
    예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기가 되기 때문임.(GBM, XGBoost, LightGBM 등) 

# 결정 트리(Decision Tree) 
  - 결정 트리 알고리즘은 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만듬(If-Else 기반규칙) 
  - 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만즐어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성느을 크게 좌우함.  
    - 브랜치/서브 트리 - 새로운 규칙 조건마다 규칙 노드 기반의 서브 트리 생성.

# 트리 분할을 위한 데이터의 균일도	
  - 다음중 가장 균일한 데이터 셋은? 3번 데이터
    - 1번 데이터셋 - 흰돌 50개, 검은돌 50개 
	- 2번 데이터셋 - 흰돌 10개, 검은돌 90개
	- 3번 데이터셋 - 검은돌 100개

# 균일도 기반 규칙 조건 
  - 노랑색 블록의 경우 모두 동그라미로 구성되고, 빨강과 파랑 블록의 경우는 동그라미, 네모, 세모가 골구루 섞여 있다고 한다면 
    각 레고 블록을 분류하고자 할 때 가장 첫 번째로 만들어져 하는 규칙 조건은?
	- if 색깔 == '노란색'

# 정보 균일도 측정 방법 
  - 정보 이득(Information Gain)
    - 정보 이득은 엔트로피라는 개념을 기반으로 함. 엔트로피는 주어진 데이터 집합의 혼잡도를 의미하는데, 서로 다른 값이 섞여
      있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮습니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값임.
      즉, 1 - 엔트로피 지수임.  결정 트리는 이 정보 이득 지수로 분할 기준을 정함. 즉, 정보 이득이 높은 속성을 기준으로 
      분할 함.
  - 지니 계수 - 경제학에서 불평등한 지수(0이 가장 평등하고 1로 갈수록 불평등)
    - 지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수임. 경제학자인 코라도 지니(Corrado Gini)의 
      이름에서 딴 계수로서 0이 가장 평등하고 1로 갈수록 불평등함. 머신러닝에 적용될 때는 지니계수가 낮을 수록 데이터
      균일도가 높은 것으로 해석되어 계수가 낮은 속성을 기준으로 분할함.

# 결정 트리의 규칙 노드 생성 프로세스 
  1) 데이터 집합의 모든 아이템이 같은 분류에 속하는지 확인 
     2-1) If True 리프 노드를 만들어서 분류 결정 
     2-2) Else 데이터를 분할하는데 가장 좋은 속성과 분할 기준을 찾음.(정보이득 or 지니계수 이용)
          3) 해당 속성과 분할 기준으로 데이터 분할하여 규칙 브랜치 노드 생성 
  4) Recursive 하게 모든 데이터 집합의 분류가 결정될 때까지 수행	
	 
# 결정 트리의 특징 
  - 결정 트리 장점 
    - 쉽다, 직관적이다 
    - 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음
  - 결정 트리 단점
    - 과적합(Overfiting)으로 알고리즘 성능이 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요.


## 결정트리의 주요 하이퍼 파라미터와 결정 트리 모델의 시각화 
# 결정 트리 주요 하이퍼 파라미터 
  - max_depth 
    - 트리의 최대 깊이를 규정 
    - 디폴트는 None, None으로 설정하면 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는
      데이터 개수가 min_sample_split보다 작아질 때까지 계속 깊이를 증가시킴.
    - 깊이가 깊어지면 min_sample_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요.
  - max_features 
    - 최적의 분할을 위해 고려할 최대 피처 개수, 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행.
    - int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트 임.
    - 'sqrt'는 전체 피처 중 sqrt(전체 피처 개수), sqrt(전체 피처 개수) 만큼 선정 
    - 'auto'로 지정하면 sqrt와 동일 
    - 'log'는 전체 피처 중 log2(전체 피처 개수) 선정
    - 'None'은 전체 피처 선정 
  - min_samples_split
    - 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용됨.
    - 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성 증가 
    - 과적합율 제어. 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가 
  - min_sample_leaf
    - 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수 
    - Min_sample_split와 유사하게 과적합 제어 용도. 그러나 비대칭(imbalanced) 데이터의 경우 특정 클래스의
      데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정 필요 
  - max_leaf_nodes
    - 말단 노드(Leaf)의 최대 개수   
	
# Graphviz를 이용한 결정트리 모델의 시각화-설치 
  - Graphviz 실행 파일을 설치 
    - graphviz.org 에서 윈도우 설치 파일 다운로드 및 설치 
  - Graphviz 파이썬 래퍼 모듈을 설치 
    - 아나콘다 프롬포트를 관리자권한으로 실행후 "pip intall graphviz"로 설치 
  - OS 환경변수 구성   
    - 고급 시스템 설정 -> 환경변수 
	  - 사용자 변수 : PATH 추가 - C:\Program Files\Graphviz\bin
	  - 시스템 변수 : PATH 추가 - C:\Program Files\Graphviz\bin\dot.exe
	  - dot -version
	    - cmd.exe를 관리자 권한으로 수행 
	    - 호깃 doc -c 를 실행하라고 하면 "doc -c"를 실행후 "dot -version" 재수행

# Graphviz의 시각화 노드 

# max_depth 에 따른 결정 트리 구조 

# min_samples_split에 따른 결정 트리 구조 

# min_sample_leaf에 따른 결정 트리 구조 

	
## 결정트리의 과적합 이해 및 사용자 행동 인식데이터를 이용한 결정트리 실습 
# 결정 트리의 Feature 선택 중요도 
  - 사이킷런의 DecisionClassifier 객체는 feature_importances_을 통해 학습/예측을 위해서 중요한 Feature들을
    선택할 수 있게 정보를 제공함. 

# 결정 트리 과적합 
  - 2개의 Feature로 된 3개의 결정 클래스를 가지도록 make_classification()함수를 이용하여 임의 데이터를 생성한 후
    트리 생성 제약이 없는 경우와 min_samples_leaf=6으로 제약을 주었을 때 분류 기준선의 변화   


## 앙상블 학습의 개요와 보팅(Voting)의 이해 
# 앙상블 학습 - 앙상블 학습 개요 
  - 앙상블 학습(Ensemble Learning)을 통한 분류는 여래개의 분류기(Classifier)를 생성하고 그 예측을
    결합함으로써 보다 정확한 최종 예측을 도출하는 기법을 말함.
  - 어려운 문제의 결론을 내기 위해 여러 명의 전문가로 위원회를 구성해 다양한 의견을 수렴하고 결정하듯이 앙상블 
    학습의 목표는 댜양한 분류기의 예측 결과를 결합함으로써 단일 분류기보다 신뢰성이 높은 예측값을 얻는 것임.

# 앙상블의 유형 
  - 앙상블의 유형은 일반적으로는 보팅(Voting), 배깅(Bagging), 부스팅(Boosting)으로 구분할 수 있으며, 이외에
    스태킹(Stacking) 등의 기법이 있음. 
  - 대표적이 배깅(Bagging)은 랜덤 포레스트(Random Forest) 알고리즘이 있으며, 부스팅(Boosting)은 에이다 부스팅,
    그래디언트 부스팅, XGBoost, LightGBM 등이 있음. 정형 데이터의 분류나 회귀에서는 GBM 부스팅 계열의 앙상블이
    전반적으로 높은 예측 성능을 나타냄.
  - 넓은 의미로는 서로 다른 모델을 결합한 것들을 앙상블로 지칭하기도 함.

# 앙상블의 특징 
  - 단일 모델의 약점을 다수의 모델들을 결합하여 보완 
  - 뛰어난 성능을 가진 모델들로만 구성하는 것보다 성능이 떨어지더라도 서로 다른 유형의 모델을 섞는 것이 오히려 전체 성능이
    도움이 될 수 있음.
  - 랜덤 포레스트 및 뛰어난 부스팅 알고리즘들은 모두 결정트리 알고리즘을 기반 알고리즘으로 적용함.
  - 결정트리의 단점인 과적합(오버 피팅)을 수십~수천개의 많은 분류기를 결합해 보완하고 장점인 직관적인 분류 기준은 강화됨.

# 보팅(Voting)과 배깅(Bagging) 개요
  - 보팅과 배깅은 여러개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식 
  - 보팅과 배깅의 다른 점은 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합하는 것이고, 배깅의 경우 각각의 
    분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것임.

# 보팅 유형 - 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting) 
  - Hard Voting은 다수의 classifier 간 다수결로 최종 class 결정	
    - classifier 1,3,4는 클래스값 1로 예측, classifier 2는 클래스값 2로 예측 -> 다수결로 클래스 값 1로 예측 
  - Soft Voting은 다수의 classifier들의 class 확률을 평균하여 결정 
    - class 값 1일 확률:0.65, 클래스값 2일 확률 : 0.35 -> 클래스값 1로 예측   
    - predict_proba() 메소드를 이용하여 class 별 확률 결정 
  - 일반적으로 하드 보팅보다는 소프트 보팅 예측 성능이 상대적으로 우수하여 주로 사용됨.
  - 사이킷런은 VotingClassifier 클래스를 통해 보팅(Voting)을 지원.


## 배깅(Bagging)의 이해와 랜덤 포레스트(Random Forest) 소개 및 실습 
# 배깅(Bagging) - 랜덤 포레스트(Random Forest) 
  - 배깅의 대표적인 알고리즘은 랜덤 포레스트임. 
  - 랜덤 포레스트는 다재 다능한 알고리즘임. 앙상블 알고리즘중 비교적 빠른 수행 속도를 가지고 있으며, 다양한 영역에서 
    높은 예측 성능을 보이고 있음.
  - 랜덤 포레스트는 여러 개의 결정트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 
    학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 하게 됨. 

# 랜덤 포레스트의 부트스트래핑 분할 
  - 랜덤 포레스트는 개별적인 분류기의 기반 알고리즘은 결정 트리이지만 개별 트리가 학습하는 데이터 세트는 전체 데이터에서
    일부가 중첩되게 샘플링된 데이터 세트임. 이렇게 여러 개의 데이터 세트를 중첩하게 분리하는 것을 부트스트래핑(bootstrapping)
    분할 방식이라고 함.(그래서 배깅(Bagging)이 bootstrap aggregating의 줄임말임.)
  - 원본 데이터의 건수가 10개인 학습 데이터 세트에 랜덤 포레스트를 2개의 결정 트리 기반으로 학습하려고 n_estimators=3으로 
    하이퍼 파라미터를 부여하면 다음과 같이 데이터 서브세트가 만들어 짐. 
    - (1,2,3,4,5,6,7,8,9,10) ->(부트스트래핑 분할) - 서브세트 #1 (1,2,3,4,5,6,7,8,9)
                                              - 서브세트 #2 (1,3,4,5,6,8,8,9,9,10)
                                              - 서브세트 #3 (1,1,3,4,4,5,6,6,9,9)		

# 사이킷런 랜덤 포레스트 하이퍼 파라미터 
  - 사이킷런은 랜덤 포레스트 분류를 위해 RandomForestClassifier 클래스를 제공함. 
  - RandomForestClassifier 하이퍼 파라미터 
    - n_estimators : 랜덤포레스트에서 결정트리의 개수를 지정함. 디폴트는 10개임. 많이 설정할 수록 좋은 성능을 기대할 수 있지만 
                     계속 증가시킨다고 성능이 무조건 향상되는 것은 아님. 또한 늘릴수록 학습 수행 시간이 오래 걸린다는 것도 감안.
    - max_features : 결정 트리에 사용된 max_features 파라미터와 같음. 하지만 RandomForestClassifier의 기본
                     max_features는 'None'이 아니라 'auto', 즉 'sqrt'와 같음. 따라서 랜덤 포레스트의 트리를 분할하는 
                     피처를 참조할 때 전체 피처가 아니라 sqrt(전체 피처 개수)만큼 참조함(전체 피처가 16개 라면 분할을 위해 4개 참조) 
    - max_depth나 min_sample_leaf와 같이 결정트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에도 똑깥이 적용될 수 있음. 

## 부스팅(Boosting)의 이해와 그래디언트 부스팅 소개 및 실습
# 부스팅(Boosting) 
  - 부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 
    오류를 개선해 나가면서 학습하는 방식.
    - 순차적으로 하다보니 수행시간이 많이 걸림	
  - 부스티의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있음. 

# 에이다 부스팅의 학습/예측 프로세스 

# GBM(Gradient Boost Machine) 개요 
  - GBM(Gradient Boost Machine)도 에이다부스트와 유사하나, 가중치 업데이트를 경사하강법(Gradient Descent)을
    이용하는 것이 큰 차이임. 오류 값은 "실제값 - 예측값". 분류의 실제값을 y, 피처를 X1, X2, ..., Xn 그리고 이 피처에 
    기반한 예측 함수를 F(x)함수라고 하면 오류식은 h(x) = y - F(x)가 됨. h(x)를 최소화하는 방향성을 가지고 반복적으로 
    가중치 값을 업데이트하는 것이 경사 하강법임. 경사 하강법은 반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트
    값을 도출하는 기법으로 머신러닝에서 중요한 기법 중 하나임.

# 사이킷런 GBM 주요 하이퍼 파라미터 및 튜닝 
  - 사이킷런은 GBM 분류를 위해 GradientBoostingClassifier 클래스를 제공.
    - loss
    - learning_rate
    - n_estimators 
    - subsample 
	