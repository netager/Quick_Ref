##### 파이썬 머신러닝 완벽 가이드 - 권철민

1장 파이썬 기반의 머신러닝과 생태계 이해
## 머신러닝 개념
  - 머신러닝이란 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 추론하는 알고리즘 기법을 통칭합니다.
  - 현신 세계의 매우 복잡한 조건으로 인행 기존의 소프트웨어 코드만으로는 해결하기 어려웠던 문제점을 이제 머신러닝을 이용해
    해결
  - 인간만이 가지는 인지능력만이 해결 가능하다고 여겨졌던 다양한 분야에서 머신러닝의 응용이 두드러지고 있으며 데이터 마이닝,
    영상인식, 음성 인식, 자연어 처리에서 머신러닝을 적용하면서 급속하게 발전.

# 머신러닝, 왜 필요한가?
  - 현실 세계의 복잡한 업무와 규칙을 구현하기 위한 매우 복잡하고 방대한 코드 
  - 수시로 변하는 업무 환경, 정책, 사용자 성향에 따른 애플리케이션 구현이 어려움
  - 많은 자원과 비용을 통해서 구현된 애플리케이션의 예측 정확성 문제 
  - 동일한 숫자라 하더라도 여러 변형으로 인행 숫자 인식에 필요한 여러 특징(feature)들을 if else와 같은 조건으로 구분하여
    숫자를 인식하기 어렵다.
  - 머신러닝은 이러한 복잡한 문제를 데이터를 기반으로 숨겨진 패턴을 인지해 해결 
  - 머신러닝 알고리즘은 데이터를 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해
    데이터 내의 패턴을 스스로 인지하고 신로도 있는 예측 결과를 도출

# 머신러닝의 분류
  - 지도학습(Supervised Learning), 비지도 학습(Un-supervised Learning),
    강화학습(Reinforcement Learning)
  - 지도학습 - 명확한 결정값이 주어진 데이터를 학습 - 분류, 회귀, 시각/음성 감지/인지
  - 비지도 학습 - 결정값이 주어지지 않는 데이터를 학습 - 군집화(클러스터링), 차원 축소 

# 머신러닝 기반의 예측 분석(Predictive Analysis)
  - 머신러닝은 데이터를 관통하는 패턴을 학습하고, 이에 기반한 예측을 수행하면서 데이터 분석 영역에 새로운 혁신을 가져옴.
  - 데이터 분석 영역은 재빠르게 머신러닝 기반의 예측 분석으로 재편되고 있음
  - 많은 데이터 분석가와 데이터 과학자 머신러닝 알고리즘 기반의 새로운 예측 모델을 이용해 더욱 정확한 예측 및 의사 결정을
    도출하고 있으며, 데이터에 감춰진 새로운 의미와 인사이트를 발굴해 놀랄만한 이익으로 연결

# 모신러닝 알고리즘 유형
  - (책) 마스터 알고리즘(THe Master Algorithm)
  - 기호주의:결정 트리 등, 연결주의:신경망/딥러닝, 유전알고리즘, 베이지안 통계, 유추주의:KNN, 서포트 벡터 머신
  
# 머신러닝의 단점
  - 데이터에 너무 의존적(Garbage In, Garbage Out)
  - 학습시에 최적의 결과를 도출하기 위해 수립된 머신러닝 모델은 실제 환경 데이터 적용시 과접합 되기 쉬움.
  - 복잡한 머신러닝 알고리즘으로 인해 도출된 결과에 대한 논리적인 이해가 어려울 수 있음(머신러닝은 블랙박스)
  - 데이터만 집어 넣어도 자동으로 최적화된 결과를 도출할 것이라는 것은 환상이다.(특정 경우에는 개발자가 직접 만든 코드보다
    정확도가 더 떨어질 수 있음.) 끊임없이 모델을 개선하기 위한 노력이 필요하기 때문에 데이터의 특성을 파악하고 최적의
	알고리즘과 파라미터를 구성할 수 있는 고급 능력이 필요 

# 왜 데이터 수집에 열광하는가?
  - 다양하고 광대한 데이터를 기반으로 만들어진 머신러닝 모델은 더 좋은 품질을 약속합니다. 앞으로 많은 회사의 경쟁력은
    어떠한 품질의 머신러닝 모델을 가지고 있느냐에 결정 될 수 있음.

## 파이썬 기반 머신러닝의 특징및 장점과 구성요소 
# R vs python
  - 파이썬도 굉장히 직관적인 언어지만, R의 경우 통계분석을 위해 특화된 언어이며 무엇보다도 오랜 기간동안 많은 R 사용자들이
    생성하고 검증해온 다양한 많은 통계 패키지를 보유하고 있는 것이 가장 큰 장점.

# R Vs. Python Machine Learning - Google trends
  - 이제 머신러닝을 시작하려는 사람이라면, 특히 개발자라면 R보다 파이썬을 권장.

# LM + Python 강점 - Python의 놀라운 인기
  - Python은 소리없이 프로그래밍 세계를 점령하고 있는 Language:
    - Academy나 타 영역의 인재들도 Python 선호
	- Google, Facebook 등 유수의 IT 업계에서도 높은 생산성으로 인해 활용도가 높음(특히 Google)
	- 오픈 소스 계열의 전폭적인 지원을 받고 있음
	- 놀라울 정도의 많은 라이브러리 지원은 어떠한 유형의 개발도 쉽게 가능(역으로 선택의 자유가 많아서 오히려 머리가 아플 정도)
	- Interpreter Language의 특성상 속도는 느리지만 쉽고 유연한 특징으로 인해 데스크탑, 서버, 네트워크, 시스템,
	  IOT 등 다양한 영역에서 사용되고 있음.

# LM + Python 강점 - 뛰어난 확장성, 연계, 호환성
  - 많은 라이브러리, 뛰어난 생산성을 가지는 Python 언어 + Machine Learning
    - 분석 영역을 넘어서 ML 기반의 다양한 Application 개발이 쉽게 가능 
	- 기존 Application과의 연계도 쉬움(서로 다른 언어로 개발된 Application의 경우 Rest API)
	- Enterprise 아키텍처에도 연계, 확장 가능. Microservice 실시간 연계 등. 

# LM + Python 강점 - Deep Learning 으로의 진격
  - 유수의 Deep Learning Framework 이 Python 기반으로 작성(tensorflow Backend는 성능 때문에 C/C++로 작성)
  - 대부분의 Deep Learning 관련 Tutorial, 설명자료들이 Python으로 작성되어 제공
  - 현 시점에서 Deep Learning을 활용하기에 가장 좋은 시작점은 Python 
  
# 파이썬 머신러닝 생태계를 구성하는 주요 패키지 
  - 머신러닝 패키지 - Scikit learn - 딥러닝 기능은 없음
  - 배열/선형대수/통계패키지 - Numpy, SciPy
  - 데이터 핸들링 - pandas
  - 시각화 - matplotlib, seaborn
  - 대화형 파이썬 툴 - jupyter notebook
  
## 파이썬 기반 머신러닝을 위한 SW의 설치
# Anaconda vs pip
  - 파이썬 머신러닝을 위한 패키지를 설치하는 가장 쉬운 방법은 anaconda를 이용하는 것.
  - https://www.anaconda.com/download/


## 주피터 노트북 사용법과 넘파이/판다스이 필요성
# 주피터 노트북은 대표적인 대화형 파이썬 툴, 학생들이 필기하듯이 중요 코드 단위로 설명을 적고 코드를 수행해 그
  결과를 볼 수 있게 만들어서 직관적으로 어떤 코드가 어떤 역할을 하는지 확인 가능 

# 머신러닝을 위한 넘파이와 판다스의 중요성
  - 머신러닝 애플리케이션 구현에서 다양한 데이터의 추출/가공/변환이 상당한 영역을 차지하고 데이터 처리 부분은
    대부분 넘파이와 판다스의 몫.
  - 사이킷런이 넘파이 기반에서 작성됐기 때문에 넘파이의 기본 프레임워크를 이해하지 못하면 사이킷런 역시 실제
    구현에서 많은 벽에 부딪힐 수 있음.
  - 사이킷런은 API 구성이 매우 간결하고 직관적이어서 이를 이용한 개발 또한 상대적으로 쉽지만 넘파이와 판다스
    API는 더 방대하기 때문에 이를 익히는 데 시간이 많이 소모될 수 있음. 하지만 머신러닝을 위해서 이들을 많은
	시간을 들여 전문적으로 공부하는 것은 효율적이지 못함.
  - 넘파이와 판다스에 대한 기본 프레임워크와 중요 API만 습득하고, 일단 코드와 부딪쳐 가면서 모르는 API에 대해서는
    인터넷 자료를 통해 체득하는 것이 머신러닝 뿐만아니라 넘파이와 판다스에 관한 이해를 넓히는 더 빠른 방법임.

## 강의에 사용될 예제 소스코드 다운로드 받기 
  - https://github.com/chulminkw/PerfectGuide

## 넘파이 배열 ndarray 소개
  - ndarray : N차원(Dimesion) 배열(Array) 객체

# ndarray 생성
  - import numpy as np
    array1 = np.array([1,2,3])

# ndarray 형태(Shape)와 차원
  - 1차원 (3,0)
  - 2차원 (2,3)
  - ndarray의 shape은 ndarray.shape 속성으로, 차원은 ndarray.ndim 속성으로 알 수 있음.

# ndarray 타입(type)
  - ndarray내의 데이터값은 숫자 값, 문자열 값, 불 값 등이 모두 가능
  - 숫자형 - int형(8bit, 16bit, 32bit), unsigned int형(8bit, 16bit, 32bit), float형(16bit, 64bit, 128bit)
           그리고 이보다 더 큰 숫자 값이나 정밀도를 위해 complex 타입도 제공 
  - ndarray내의 데이터 타입은 그 연산의 특성상 같은 데이터 타입만 가능. 즉 한개의 ndarray객체에 int와 float가 함께 있을 수 없음.
  - ndarray내의 데이터 타입은 ndarray.dtype으로 확인할 수 있음.

# ndarray 타입(type) 변환 
  - astype()을 이용하여 변환 
    - 변경을 원하는 타입을 astype()에 인자로 입력
    - 대용량 데이터를 ndarray로 만들 때 메모리를 절약하기 위해 자주 사용
      - 0, 1, 2와 같이 크지 않는 숫자를 64bit float형 보다 8bit 또는 16bit의 integer형으로 변환하는 것이 
        메모리를 많이 절약.
  - 대용량 데이터를 다룰 시 메모리 절약을 위해서 형변환을 특히 고려해야 함.

# 넘파이 ndarray의 axis 축
  - ndarray는 shape는 행, 열, 높이 단위로 부여되는 것이 아니라 axis0, axis1, axis2와 같이 axis 단위로 부여됨 
  
  
## 넘파이 배열 ndarray 초기화 방법과 ndarray차원과 크기를 변경하는 reshape()의 이해 
# ndarray를 편리하게 생성하기 - arange, zeros, ones
  - 특정 크기와 차원을 가진 ndarray를 연속값이나 0 또는 1로 초기화 생성해야 할 경우 arange(), zeros(), ones()를
    이용해 쉽게 ndarray를 생성할 수 있음. 
	- np.arange(10) -> [0,1,2,3,4,5,6,7,8,9]
	- np.zeros((3,2), dtype='int32')
	- np.ones((3,2))

# ndarray의 차원과 크기를 변경하는 reshape()
  - reshape()는 ndarray를 특정 차원 및 형태로 변환. 변환 형태를 함수 인자로 부여하면 됨.
    - array1 -> reshape(2,5) -> 변환된 array 

# ndarray의 차원과 크기를 변경하는 reshape()
  - reshape(-1, 5)와 같이 인자에 -1을 부여하면, -1에 해당하는 axis의 크기는 가변적이 되고 -1이 아닌 인자값
    에 해당하는 axis 크기는 인자값으로 고정하여 ndarray의 shape를 변환.
    - array1, (10,) -> reshape(-1,5) -> shape는 (2,5)
  - reshape()는 reshape(-1,1), reshape(-1,)과 같은 형식으로 변환이 요구되는 경우가 많음. 
    주로 머신러닝 API의 인자로 1차원 array를 명확하게 2차원 ndarray로 변환하여 입력하기를 원하거나, 또는 반대의
    경우가 있을 경우 reshape()를 이용하여 ndarray의 형태을 변환시키는데 사용.
    - array1d, [0 1 2 3 4] -> reshape(-1,1) -> shape는 (5,1) [[0] [1] [2] [3] [4]]
    - array2d, [[0] [1] [2] [3] [4]] -> reshape(-1,) -> shape(5,) [0 1 2 3 4]

## 넘파이 ndarray의 인덱싱(Indexing)을 통한 데이터 세트 선택하기
# 인덱싱
  - 특정위치의 단일값 추출
  - 슬라이싱(slicing)
  - 팬시 인덱싱(Fancy Indexing)
  - 블린 인덱싱(Boolean Indexing)

# 단일값 추출 - 1차원 ndarray 
  - ndarray는 axis를 기준으로 0부터 시작하는 위치 인덱스값을 가지고 있음. 해당 인덱스 값을 []에 명시하여 단일값을
    추출함. 마이너스가 인덱스로 사용되면 맨 뒤에서 부터 위치를 지정함.
	- array1[3], array1[-2], array1[-1]

# 단일 값 추출 - 2차원 ndarray 
  - array2d[0,0], array2d[0,1], array2d[1,0], array2d[2,2]

# 슬라이싱(Slicing) - 1차원 ndarray 
  - 슬라이싱은 : 을 이용하여 연속된 값을 선택.
    - array1[:], array1[:3], array1[3:], array1[0:3]

# 슬라이싱(Slicing) - 2차원 ndarray 
  - array2d[0:2, 0:2], array2d[1:3, 0:3], array2d[1:3, :]
  - array2d[:, :], array2d[:2, 1:], array2d[:2, 0]

# 팬시 인덱싱 - 1차원 ndarray
  - 팬시 인덱싱(Fancy Indexing)은 리스트나 ndarray로 인덱스 집합을 지정하면 해당 위치의 인덱스에
    해당하는 ndarray를 반환하는 인덱싱 방식.
    - array1[[2,4,7]]

# 팬시 인덱싱 - 2차원 ndarray
  - array2d[[0,1], 2], array2d[[0,1], 0:2], array2d[[0,1]]

# 블린 인덱싱
  - 불린 인덱싱(Boolean Indexing)은 조건 필터링과 검색을 동시에 할 수 있기 때문에 매우 자주 사용되는 인덱싱 방식.
    - ndarray 내의 값이 5보다 큰 ndarray를 추출하고자 한다면?
      - 불린 인덱싱을 사용하지 않은 경우
        - array1d = np.arange(start=1, end=10)
          target=[]
          for i in range(0, 9):
              if array1d[i] > 5:
                 target.append(array1d[i])
          array_selected = np.array(target)
          print(array_selected)
      - 블린 인덱싱 사용
        - array1[array1>5]
		
## 넘파이 ndarray의 정렬과 선형대수 연산 
# 배열의 정렬 - sort()와 argsort()
  - sort()
    - np.sort() : 원 행렬은 그대로 유지한 채 원 행렬의 정렬된 행렬을 반환 
    - ndarray.sort() : 원 행렬 자체를 정렬한 형태로 반환하며 반환 값은 None
    - np.sort()나 ndarray.sort() 모두 기본적으로 오름차순으로 정렬. 내림차순으로 정렬하기위해서는 [::-1]을
      적용 함. np.sort()[::-1]
  - argsort()
    - 원본 행렬 정렬 시 정렬된 행렬의 원래 인덱스를 필요로 할 때 np.argsort()를 이용. np.argsort()는 
	  정렬 행렬의 원본 행렬 익덱스를 ndarray형으로 반환합니다.
	  

# 2차원 배열에서 axis 기반 sort()
  - np.sort(A, axis=0)
  - np.sort(A, axis=1)
  
# 선형대수 연산 - 행렬 내적
  - np.dot(A, B)
  
# 선형대수 연산 - 전치 행렬 
  - np.transpose(A) 

# 넘파이 Summary
  - 넘파이는 파이썬 머신러닝을 구성하는 핵심 기반으로 반드시 이해가 필요
  - 넘파이 API는 매우 넓은 범위를 다루고 있으므로 머신러닝 애플리케이션을 작성 할 때 중요하게 활용될 수 있는
    핵심 개념 위조로 숙지하는 것이 좋음
  - 넘파이는 판다스에 비해서 친절한 API를 제공하지 않음. 2차원 데이터라면 데이터 가공을 위해서 넘파이 보다는
    판다스를 이용하는 것이 효율적임.

## 판다스(Pandas) 개요와 기본 API - 01
# 판다스(Pandas) - 웰스 매키니
  - 판다스는 파이썬에서 데이터 처리를 위해 존재하는 가장 인기 있는 라이브러리.

# 판다스의 주요 구성 요소 - DataFrame, Series, Index 
  - DataFrame - 2차원 데이터 셋
  - Series - 1개의 컬럼갑승로만 구성된 1차원 데이터 셋 
  
# 기본 API
  - read_csv(), head(), shape, info(), describe(), value_counts(), sort_values()


## 판다스 DataFrame의 변환, 컬럼 세트 생성/수정, 삭제 및 Index 객체 소개 
# DataFrame과 리스트, 딕셔너리, 넘파이 ndarray 상호 변환
  - 리스트(list)를 DataFrame으로 변환
    - df_list1 = pd.DataFrame(list, columns=col_name1)
  - ndarray를 DataFrame으로 변환 
    - df_array2 = pd.DataFrame(array2, columns=col_name2)
  - 딕셔너리(dict)를 DataFrame으로 변환 
    - dict = {'col1':[1,11], 'col2':[2,22], 'col3':[3,33]}
	- df_dict = pd.DataFrame(dict)
  - DataFrame을 ndarray로 변환 
    - DataFrame 객체의 values 속성을 이용하여 ndarray 변환 
  - DataFrame을 리스트로 변환
    - DataFrame 객체의 values 속성을 이용하여 먼저 ndarray로 변환 후 tolist()를 이용하여 list로 변환 
  - DataFrame을 딕셔너리로 변환   
    - DataFrame 객체의 to_dict()를 이용하여 변환 

# DataFrame 데이터 삭제 
  - DataFrame의 drop()
    - DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, error='raise')
      - axis : DataFrame의 로우를 삭제할 때는 axis=0, 컬럼을 삭제할 때는 axis=1로 설정 
	  - 원본 DataFrame은 유지하고 드롭된 DataFrame을 새롭게 객체 변수로 받고 싶다면 inplace=False로
	    설정(디폴트 값이 False 임)
		- Ex: titanic_drop.df = titanic_df.drop('Age_0', axis=1, inplace=False)
	  - 원본 DataFrame에 드롭된 결과를 적용할 경우에는 inplace=True를 적용 
	    - Ex: titanic_df.drop('Age_0', axis=1, inplace=True)
	  - 원본 DataFrame에서 드롭된 DataFrame을 다시 원본 DataFrame 객체 변수로 할당하면 원본 DataFrame에서 드롭된 
	    결과를 적용할 경우와 같음(단, 기존 원본 DataFrame 객체 변수는 메모리에서 추후 제거됨)
		- Ex: titanic_df = titanic_df.drop('Age_0', axis=1, inplace=False) 

# INdex 객체 
  - 판다스의 Index 객체는 DataFrame, Series의 레코드를 고유하게 식별하는 객체 
  - DataFrame, Series에서 Index 객체만 추출하려면 DataFrame.index 또는 Series.index 속성을 통해 가능.
  - Series 객체는 Index 객체를 포함하지만 Series 객체에 연산 함수를 적용할 때 Index는 연산에서 제외됨.
    Index는 오직 식별용으로만 사용됨.
  - DataFrame 및 Series에 reset.index() 메소드를 수행하면 새롭게 인덱스를 연속된 숫자형으로 할당하며 기존의
    인덱스는 'index'라는 새로운 컬럼명으로 추가됨.
	

## 판다스 데이터 셀렉션과 필터링 
  - 데이터 셀렉션 및 필터링
    - [] - 컬럼 기반 필터링 또는 불린 인덱싱 필터링 제공 
    - ix[], loc[], iloc[] - 명칭/위치 기반 인덱싱 제공 
	- 불린 인덱싱(Boolean Indexing) - 조건식에 따른 필터링 제공 
  - 명칭(Label) 기반 인덱싱은 컬럼의 명칭을 기반으로 위치를 지정하는 방식. 
  - 위치(Position) 기반 인덱싱은 0을 출발점으로 하는 가로축, 세로축 좌표 기반의 행과 열 위치를 기반으로 데이터를 지정.
    - ix[] - 명칭 기반과 위치 기반 인덱싱을 함께 제공하나 없어질 예정.
	- loc[] - 명칭 기반 인덱싱 
	- iloc[] - 위치 기반 인덱싱

# 명칭 기반과 위치 기반 인덱싱 구분 

# 불린 인덱싱(Boolean indexing)
  - 위치기반, 명칭기반 인덱싱 모두 사용할 필요없이 조건식을 [] 안에 기입하여 간편하게 필터링 수행 
    - titanic_boolean = titanic_df[titanic_df['Age']>60]


## 판다스 Aggregation 함수와 Group by 수행 
# Aggregation 함수 
  - sum(), max(), min(), count() 등의 함수는 DataFrame/Series에서 집합(Aggregation) 연산을 수행.
  - DataFrame의 경우 DataFrame에서 바로 aggregation을 호출할 경우 모든 컬럼에 해당 aggregation을 적용.
  
# axis에 따른 Aggregation 함수 결과 
  - Aggregation 함수 호출시 axis 값을 명시하지 않으면 axis=0으로 결과 출력
    - titanic_df[['Age','Fare']].sum(axis=0) vs titanic_df[['Age','Fare']].sum(axis=1)
  	
# DataFrame Group By
  - DataFrame은 Group by 연산을 위해 groupby() 메소드를 제공
  - groupby() 메소드는 by 인자로 group by 하려는 컬럼명을 입력 받으면 DataFrameGroupBy 객체를 반환 
  - 이렇게 반환된 DataFrameGroupBy 객체에 aggregation 함수를 수행.


## 판다스 결손 데이터 처리하기
# 결손 데이터(Missing Data) 처리하기 
  - isna() - DataFrame의 isna() 메소드는 주어진 컬럼값들이 NaN인지 True/False 값을 반환함.
  - fillna() - Missing 데이터를 인자로 주어진 값으로 대체함.

## 판다스 람다식 적용하여 데이터 가공하기 
# 판다스 apply lambda - 파이썬 lambda 식 이해 
  - def get_square(a):
        retrun a**2
  - lambda x: x ** 2		

# apply lambda 식으로 데이터 가공 
  - 판다스는 apply 함수에 lambda 식을 결합해 DataFrame이나 Series의 레코드 별로 데이터를 가공하는 기능을
    제공함. 판다스의 경우 컬럼에 일괄적으로 데이터 가공을 하는 것이 속도 면에서 더 빠르거나 복잡한 데이터 가공이 
	필요할 경우 어쩔 수 없이 apply lambda 를 이용함.
	- titanic_df['Name_len'] = titanic_df['Name'].apply(lambda x: len(x))

# 판다스 Summary 
  - 2차원 데이터 핸들링을 위해서는 판다스를 사용 
  - 판다스는 매우 편리하고 다양한 데이터 처리 API를 제공하지만(조인, 피벗/언피벗, SQL like API 등), 이를
    다 알기에는 많은 시간과 노력이 필요 
  - 지금까지 언급된 핵심 사항에 집중하고, 데이터 처리를 직접 수행해 보면서 문제에 부딛칠 때마다 판다스이 다양한 API를
    찾아서 해결해 가면 판다스에 대한 실력을 더욱 향상 시킬 수 있음.
	

## 파이썬 기반의 머신러닝과 생태계 이해 Summary 
# 1장 Summary 
  - 머신러닝이란 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 추론하는 알고리즘 기법을 통칭 
  - 직관적인 문법, 많은 라이브러리, 뛰어난 생산성을 가지는 파이썬 언어를 기반으로 한 머신러닝 애플리케이션은 유연성, 
    통합성 등의 많은 장점을 사용자에게 제공해 줌.
  - 파이썬 기반의 머신러닝을 익히기 위해서는 머신러닝 패키지 뿐만아니라, 넘파이, 판다스, 시각화 등의 다양한 지원 패키지들도
    같이 학습되어야 함. 머신러닝 애플리케이션을 작성하면서 인터넷 등을 통해 찾아가면서 구현하는 것이 이들 지원 패키지를 
	빨리 익히는데 도움이 더 될 것이다.


2장 사이킷런으로 시작하는 머신러닝 
## 사이킷런 소개와 첫번째 머신러닝 애플리케이션 만들어 보기 - 붓꽃(Iris) 품종 예측 
# 사이킷런 소개 
  - 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬 스러운 API를 제공함.
  - 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공함
  - 오랜 기간 실전 환경에서 검증됐으며, 매우 많은 환경에서 사용되는 성숙한 라이브러리임.
  - 주로 Numpy와 Scipy 기반 위에서 구축된 라이브러리임.

# 사이킷런을 이용한 붓꽃 데이터 분류 - 분류 예측, 지도학습 
  - 붓꽃 데이터 세트는 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 피처(Feature)를 기반으로 꽃의 품종을 예측하기 위한 것.
  - 붓꽃 데이터 피쳐
    - Sepal length - 꽃받침의 길이 
    - Sepal width  - 꽃받침의 넓이  
    - Petal length - 꽃잎의 길이 
    - Petal width  - 꽃잎의 넓이 	
    - 붓꽃 데이터 품종(레이블) - Setosa, Vesicolor, Virginica
	
# 머신러닝을 위한 용어 정리 
  - 피처(Feature)? 속성?
    - 피처는 데이터 세스의 일반 속성임.
	- 머신러닝은 2차원 이상의 다차원 데이터에서도 많이 사용되므로 타겟값을 제외한 나머지 속성을 모두 피처로 지칭 
  - 레이블, 클래스, 타겟(값), 결정(값) 
    - 타겟값 또는 결정값은 지도학습시 데이터의 학습을 위해 주어지는 정답 데이터.
	- 지도학습 중 분류의 경우에는 이 결정값을 레이블 또는 클래스로 지칭
  	
# 지도학습 - 분류 
  - 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법의 하나임. 
  - 지도학습은 학습을 위한 다양한 피처와 분류 결정값인 레이블(Label) 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터
    세트에서 미지의 레이블을 예측함
  - 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식임. 이 때 학습을 위해 주어진
    데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세스틀 테스트
    데이터 세트로 지칭함. 

# 붓꽃 데이터 분류 예측 프로세스 
  - 데이터 세트 분리 - 데이터를 학습 데이터와 테스트 데이터로 분리함.
  - 모델 학습 - 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킴.
  - 예측 수행 - 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 함.
  - 평가     - 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가
  
## 사이킷런의 기반 프레임 워크 익히기 - 주요 API/모듈 및 내장 예제 데이터 세트 소개
# 붓꽃 데이터 분류 예측 프로세스
  - 학습데이터 -> 모델학습 -> 테스트데이터 -> 학습된 모델로 테스트 데이터 예측 -> 평가 

# 사이킷런 기반 프레임워크 - Estimator 와 fit(), predict()
  - Esstimator - 학습(fit()), 예측(predict())
    - 분류(Classifier) - 분류구현 클래스
      - DecisionTreeClassifier
      - RandomForestClassifier
      - GradientBoostingClassifier
      - GaussianNB
      - SVC
    - 회귀(Regressor) - 회귀구현 클래스 
      - LinearRegression
	  - Ridge
	  - Lasso
	  - RandomForestRegressor
	  - GradientBoostingRegressor 
	
# 사이킷런의 주요 모듈
  - 예제 데이터 
    - sklearn.datasets - 사이킷런에 내장되어 예제로 제공하는 데이터 세트 
  - 데이터 분리, 검증 & 파라미터 튜닝 
    - sklearn.model_selection - 교차검증을 위한 학습용/테스트용 분리, 그리드 서치(Grid Search)로 최적 파라미터 추출 등이 API 제공 
  - 피처 처리 
    - sklearn.preprocessing - 데이터 전처리에 필요한 다양한 가공 기능 제공(문자열을 숫자형 코드 값으로 인코딩, 정규화, 스케일링 등)
    - sklearn.feature_selection - 알고리즘에 큰 영향을 미치는 피처를 우선순위 대로 셀렉션 작업을 수행하는 다양한 기능 제공 
    - sklearn.feature_extraction - 텍스트 데이터나 이미지 데이터의 벡터화된 피처를 추출하는데 사용됨. 예를 들어 텍스트 데이터에서 
	                               Count Vectorizer 나 Tf-Idf Vectorizer 등을 생성하는 기능 제공 
                                 - 텍스트 데이터의 피처 추출은 sklearn.feature_extraction.text 모듈에, 이미지 데이터의 피처 추출은 
                                   sklearn.feature_extraction.image 모듈에 지원 API가 있음.
  - 피처 처리 & 차원 축소 
    - sklearn.decomposition - 차원 축소와 관련한 알고리즘을 지원하는 모듈임. PCA, NMF, Truncated SVD 등을 통해 차원 축소 기능을 수행할 수 있음.

  - 평가 
    - sklearn.metrics - 분류, 회귀, 클러스터링, 페어와이즈(Pairwise)에 대한 다양한 성능 측정 방법 제공 
                      - Accuracy, Precision, Recall, ROC-AUC, RMSE 등 제공 
  - ML 알고리즘 - 다양한 알고리즘을 가지고 있음. 아래 예시보다 더 많음.
    - sklearn.ensemble - 앙상블 알고리즘 제공
                       - 랜덤 포레스트, 에이다 부스트, 그래디언트 부스팅 등을 제공 
    - sklearn.linear_model - 주로 선형회귀, 릿지(Ridge), 라쏘(Lasso) 및 로지스틱 회귀 등 회귀 관려 알고리즘을 지원. 또한 SGD(Stochastic Gradient Descent) 관련
                             알고리즘도 제공 
    - sklearn.naive_bayes - 나이브 베이즈 알고리즘 제공. 가우시안 NB, 다항분포 NB 등.
    - sklearn.neighboors - 최근접 아웃 아로고리즘 제공. K-NN 등 
    - sklearn.svm - 서포트 벡터 머신 알고리즘 제공 
    - sklearn.tree - 의사 결정 트리 알고리즘 제공 
    - sklearn.cluster - 비지도 클러스터링 알고리즘 제공(K-평균, 계층형, DBSCAN 등)
  - 유틸리티 
    - sklearn.pipeline - 피처 처리 등의 변환과 ML 알고리즘 학습, 예측 등을 함께 묶어서 실행할 수 있는 유틸리티 제공   

# 사이킷런 내장 예제 데이터 셋 - 분류 및 회귀용 
  - datasets.load_boston() - 회귀 용도이며, 미국 보스턴의 집 피처들과 가격에 대한 데이터세트 
  - datasets.load_breast_cancer() - 분류 용도이며, 위스콘신 유방암 피처들과 악성/음성 레이블 데이터 세트 
  - datasets.load_diabetes() - 회귀 용도이며, 당뇨 데이터 세트 
  - datasets.load_digits() - 분류 용도이며, 0에서 9까지 숫자의 이미지 픽셀 데이터 세트 
  - datasets.load_iris() - 분류 용도이며, 붓꽃에 대한 피처를 가진 데이터 세트 

# 내장 예제 데이터 셋 구성 
  - iris_data 의 데이터 셋 확인

## 학습과 테스트 데이터 세트의 분리
# Model Selection(sklearn.model_selection) 소개 - 학습 데이터와 테스트 데이터 
  - 학습 데이터 세트 
    - 머신러닝 알고리즘 학습을 위해 사용
	- 데이터의 속성들과 결정값(레이블)값 모두를 가지고 있음 
	- 학습 데이터를 기반으로 머신러닝 알고리즘이 데이터 속성과 결정값의 패턴을 인지하고 학습 
  - 테스트 데이터 세트  
    - 테스트 데이터 세트에서 학습된 머신러닝 알고리즘을 테스트 
	- 테스트 데이터는 속성 데이터만 머신러닝 알고리즘에 제공하며, 머신러닝 알고리즘은 제공된 데이터를 기반으로 결정값을 예측 
	- 테스트 데이터는 학습 데이터와 별도로 데이터 세트로 제공되어야 함.

# 학습 데이터와 테스트 데이터 분리 - train_test_split()
  - sklearn.model_selection의 train_test_split() 함수
    : X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121)
    - test_size : 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정. 디폴트는 0.25, 즉 25%임.
    - train_size : 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정. test_size parameter를 사용하므로 잘 사용하지 않음.
	- shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정. 디폴트는 True. 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를
	            만드는데 사용.
	- random_state : random_state는 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값입니다. 
	                 train_test_split()는 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트용 
					 데이터를 생성함.


## 교차검증 - K-Fold와 Stratified K-Fold 
# 교차 검증
  - 학습 데이터를 다시 분할하여 학습 데이터와 학습된 모델의 성능을 일차 평가하는 검증 데이터로 나눔
    - 학습데이터 세트 -> 분할 -> 학습데이터 세트 + 검증 데이터 세트
  - 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위한 데이터 세트 - 테스트 데이터 세트 
  - 교차 검증은 필수 요소임. 데이터를 섞어서 다시 검증데이터를 생성하여 여러번 수행.  

# K 폴드 교차 검증 
  - K=5일때 총 5개의 폴드 세트에 5번의 학습과 검증 평가 반복 수행 
    - 학습 데이터를 5개로 나누어 1개를 검증 데이터로 사용. 돌아가면서 5번의 학습 및 검증을 수행

# K 폴드 교차 검증
  - 일반 K 폴드
  - Stratified K 폴드
    - 불균형한(imbalanced) 분포도를 가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식 
    - 학습데이터와 검증 데이터 세트가 가지는 레이블 분포도가 유사하도록 검증 데이터 추출 	

## 교차검증 성능평가 cross_val_score()와 하이퍼 파라미터 튜닝을 위한 GridSearchCV
# 교차 검증을 보다 간편하게 - cross_val_score()
  - KFold 클래스를 이용한 교차 검증 방법
    1. 폴드 세트 설정
    2. for 루프에서 반복적으로 학습/검증 데이터 추출 및 학습과 예측 수행 
    3. 폴드 세트별로 예측 성능을 평균하여 최종 성능 평가 
  - cross_val_score() 함수로 폴드 세트 추출, 학습/예측, 평가를 한번에 수행 
    - cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')

# GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한번에 
  - 사이킷런은 GridSearchCV를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 
    순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안을 제공   
	- grid_parameters = {'max_depth':[1,2,3], 'min_samples_split': [2,3]}
	- cv 세트가 3이라면 6(파라미터 순차 적용 횟수) X 3(CV 세트수) -> 학습/검증 총 수행횟수(18)

## 데이터 전처리 - 인코딩과 스케일링
# 데이터 전처리(Preprocessing)
  - 데이터 클린징 - 잘못된 오류 데이터 처리
  - 결손값 처리(Null/NaN 처리)
  - 데이터 인코딩(레이블, 원-핫 인코딩)
  - 데이터 스케일링
  - 이상치(Outlier) 제거
  - Feature 선택, 추출 및 가공 

# 데이터 인코딩
  - 머신러닝 알고리즘은 문자열 속성을 입력 받지 않으며 모든 데이터는 숫자형으로 표현되어야 함. 문자형 카테고리형 속성은 모두 숫자값으로 변환/인코딩 되어야 함.
    - 레이블(Label) 인코딩
	- 원-핫(One-Hot) 인코딩 

# 레이블(Label) 인코딩 
  - [TV, 냉장고, 전자레인지, 컴퓨터, 선풍기, 믹서] -> [0, 1, 4, 5, 3, 2]
    - 상품명은 연관성이 없으나 숫자로 변경됨으로써 의미가 부여 되는 부분이 있음. 숫자는 크기 및 순서가 있어 머신러닝이 관련성을 인지할 수 있음.
      그래서 나온게 원-핫 인코딩.

# 원-핫(One-Hot) 인코딩	  
  - 원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 컬럼만을 1을 표시하고 나머지 컬럼에는 0을 표시하는 방식.
    - TV (100000), 냉장고 (010000), 전자렌지 (001000), 컴퓨터 (000010), 믹서 (000001)

# 사이킷런 - 원-핫 인코딩 
  - 원본데이터 -> 숫자로 인코딩 -> 원-핫 인코딩 

# 판다스 get_dummies()를 이용한 원-핫 인코딩 
  - pd.get_dummies(DataFrame) 
  
# 피처 스케일링 
  - 표준화는 데이터의 피처 각각이 평균이 0이고 분산이 1인 가우시안 정규분포를 가진 값으로 변환하는 것을 의미함.
    - xi_new = (xi - mean(x))/stdev(x)
  - 정규화는 서로 다른 퍼처의 크기를 통일하기 위해 크기를 변환해주는 개념  
    - xi_new = (xi - min(x))/(max(x) - min(x)

# 사이킷런 피처 스케일링 지원 
  - StandardScaler : 평균이 0이고, 분산이 1인 정규 분포 형태로 변환 
  - MinMaxScaler : 데이터값을 0과 1사이의 범위 값으로 변환(음수 값이 있으면 -1에서 1값으로 변환)
 
## 사이킷런으로 수행하는 타이타닉 생존자 예측 
# 타이타닉 생존자 ML 예측 구현 
  - 데이터 전처리
    - Null 처리 
    - 불필요한 속성 제거 
    - 인코딩 수행 
  - 모델 학습 및 검증/예측/평가 
    - 결정트리, 랜덤포레스트, 로지스틱 회귀 학습 비교 
    - K폴드 교차 검증 
    - cross_val_score() 와 GridSearchCV() 수행 	

## 2장 Summary 
# 머신러닝 지도학습 프로세스 
  - 데이터 전처리
    - 데이터 클린징 
    - 결손값 처리(Null/NaN 처리) 
    - 데이터 인코딩(레이블, 원-핫 인코딩)
    - 데이터 스케일링 
    - 이상치 제거 
    - Feature 선택, 추출 및 가공 	
  - 데이터 세트 분리 
    - 학습 데이터/테스트 데이터 분리 
  - 모델학습 및 검증 평가 - 교차 검증(K 폴드), cross_val_score(), GridSearchCV
    - 알고리즘 학습 
  - 예측 수행 
    - 테스트 데이터로 예측 수행 
  - 평가   
    - 예측 평가 

3. 평가(Evaluation) 
## 분류(Classification) 성능 평가지표 개요와 정확도(Accuracy) 소개 
# 분류(Classification) 성능 평가 지표 
  - 정확도(Accuracy) 
  - 오차행렬(Confusion Matrix)
  - 정밀도(Precision)
  - 재현율(Recall) 
  - F1 스코어 
  - ROC AUC - 이진분류시 많이 사용 

# 정확도(Accuracy)
  - 정확도(Accuracy) = 예측 결과가 동일한 데이터 건수 / 전체 예측 데이터 건수 
  - 정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표. 하지만 이진 분류의 경우 데이터의 구성에 따라
    ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하지 않음.
  - 특히 정확도는 불균형한(imbalanced) 레이블 값 분포에서 ML 모델의 성능을 판단할 경우, 적합한 평가 지표가 아님.

# 정확도의 문제점 
  - 타이타닉 생존자 예측에서 여성은 모두 생존으로 판별 
    - if sex = '여성' 생존   
  - MNIST 데이터셋을 multi classification에서 binary classification으로 변경 

# 오차 행렬(Confusion Matrix)
  - 오차 행렬은 이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표 

                                             예측 클래스(Predicted Class)
											 
                                       | Negative(0)        | Positive(1)
	                       ------------|--------------------|--------------------			
                           Negative(0) | TN(True Negative)  | FP(False Positive)
    실제 클래스(Actual Class) ------------|--------------------|--------------------	
	                       Positive(1) | FN(False Negative) | TP(True Positive) 
	                                   |                    |

# 정밀도(Precision)과 재현율(Recall) - 이진분류에서 많이 사용
  - 정밀도 = TP / (FP + TP)
    - 정밀도는 예측을 Positive로 한 대상 중에 예측과 실제값이 Positive로 일치한 데이터의 비율을 말함. 
  - 재현율 = TP / (FN + TP) 
    - 재현율은 실제 값이 Positive인 대상 중에 예측과 실제값이 Positive로 일치한 데이터의 비율을 말함.
	
# 사이킷런의 정밀도, 재현율 
  - 정밀도는 precision_sore()
  - 재현율은 recall_score()  
